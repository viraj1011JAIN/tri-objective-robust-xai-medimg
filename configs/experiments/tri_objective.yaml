# Tri-Objective Training Configuration for ISIC2018
# Phase 7.5 - Production-Grade Multi-Objective Optimization
# =============================================================================
# This configuration implements the complete tri-objective framework combining:
#   1. Task Loss (L_task): Calibrated cross-entropy with temperature scaling
#   2. Robustness Loss (L_rob): TRADES adversarial robustness via KL divergence
#   3. Explanation Loss (L_expl): Stability (SSIM) + semantic alignment (TCAV)
#
# Mathematical Formulation:
#   L_total = L_task + λ_rob * L_rob + λ_expl * L_expl
#
# Where:
#   - L_task = CE(f(x), y) / T  (T: learnable temperature)
#   - L_rob = β * KL(f(x) || f(x_adv))  (β = 6.0, TRADES regularization)
#   - L_expl = L_SSIM + γ * L_TCAV  (γ = 0.5, TCAV weight)
#   - x_adv: PGD-7 adversarial examples (ε = 8/255)
#
# Target Performance (A1+ Grade Criteria):
#   - Clean Accuracy: ≥ 82% (ISIC 2018 baseline)
#   - Robust Accuracy (PGD-20): ≥ 65%
#   - Explanation Stability (SSIM): ≥ 0.70
#   - TCAV Medical Alignment: ≥ 0.60
#   - Calibration ECE: ≤ 0.10
#
# Author: Viraj Pankaj Jain
# Institution: University of Glasgow, School of Computing Science
# Date: November 27, 2025
# Version: 1.0.0 (Phase 7.5)
# =============================================================================

# Experiment metadata
experiment:
  name: "tri_objective_isic2018"
  description: "Tri-objective optimization: task accuracy + robustness + explainability"
  task: "tri_objective_classification"
  project_name: "tri-objective-robust-xai-medimg"
  tags:
    phase: "7.5"
    method: "tri-objective"
    dataset: "isic2018"
    model: "resnet50"
    objective: "multi"
    grade_target: "A1+"
  author: "Viraj Pankaj Jain"
  notes: |
    Phase 7.5 tri-objective training with production-grade implementation.
    Integrates Phase 7.1 (explanation loss), Phase 7.2 (tri-objective loss),
    and Phase 7.3 (TRADES robustness) for complete optimization framework.

# Model configuration
model:
  name: "resnet50"
  num_classes: 7  # ISIC 2018: MEL, NV, BCC, AKIEC, BKL, DF, VASC
  pretrained: true  # ImageNet initialization for faster convergence
  in_channels: 3
  dropout: 0.3  # Dropout rate for regularization
  global_pool: "avg"  # Global average pooling
  freeze_backbone: false  # Fine-tune entire network

  # Architecture-specific settings
  architecture:
    replace_stride_with_dilation: [false, false, false]
    groups: 1
    width_per_group: 64

# Dataset configuration
dataset:
  name: "isic2018"
  root: "/content/drive/MyDrive/data/processed/isic2018"
  batch_size: 32  # Optimized for 4GB GPU (RTX 3050)
  num_workers: 0  # Disabled for Windows compatibility
  pin_memory: true
  prefetch_factor: 2

  # Image preprocessing
  image_size: 224  # Standard ResNet-50 input size
  mean: [0.485, 0.456, 0.406]  # ImageNet normalization
  std: [0.229, 0.224, 0.225]

  # Data splits
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15

  # Class names (for logging and analysis)
  class_names:
    - "MEL"    # Melanoma
    - "NV"     # Melanocytic nevus
    - "BCC"    # Basal cell carcinoma
    - "AKIEC"  # Actinic keratosis
    - "BKL"    # Benign keratosis
    - "DF"     # Dermatofibroma
    - "VASC"   # Vascular lesion

  # Class weights (from baseline training - handle severe imbalance)
  # Computed as: weight_c = N_total / (N_classes * N_c)
  # Where N_c is number of samples in class c
  class_weights: [4.356, 0.145, 6.452, 5.678, 2.345, 10.234, 8.123]
  use_class_weights: true

  # Data augmentation (training only)
  augmentation:
    horizontal_flip: true
    vertical_flip: true
    rotation: 20  # Degrees
    brightness: 0.2
    contrast: 0.2
    saturation: 0.2
    hue: 0.1
    random_affine:
      degrees: 15
      translate: [0.1, 0.1]
      scale: [0.9, 1.1]
    random_erasing:
      probability: 0.3
      scale: [0.02, 0.33]

# Training configuration
training:
  max_epochs: 60  # Extended for tri-objective convergence
  learning_rate: 0.0001  # Lower LR for stable multi-objective optimization
  weight_decay: 0.0001  # L2 regularization
  optimizer: "adamw"  # Adam with weight decay decoupling

  # Learning rate scheduler
  scheduler: "cosine"
  scheduler_params:
    T_max: 60  # Full cosine cycle over all epochs
    eta_min: 0.00001  # Minimum learning rate
  warmup_epochs: 5  # Linear warmup from 0 to lr

  # Gradient management
  grad_clip_max_norm: 1.0  # Gradient clipping for stability
  gradient_accumulation_steps: 1  # Effective batch size = 32 * 1

  # Mixed precision training
  mixed_precision: true  # Enable AMP for RTX 3050

  # Early stopping
  early_stopping_patience: 15  # More patience for tri-objective convergence
  early_stopping_metric: "val_total_loss"
  early_stopping_mode: "min"

  # Checkpointing
  save_best_only: true
  save_last: true
  checkpoint_dir: "checkpoints/tri_objective"

  # Device settings
  device: "cuda"
  dataparallel: false  # Single GPU training

# Tri-objective loss configuration
loss:
  type: "tri_objective"

  # Loss component weights (from dissertation blueprint)
  lambda_rob: 0.3   # Robustness weight (λ_rob)
  lambda_expl: 0.1  # Explanation weight (λ_expl) - reduced from 0.2 for stability

  # Task loss (calibrated cross-entropy)
  task_loss:
    type: "cross_entropy"
    label_smoothing: 0.1  # Smoothing factor
    temperature: 1.5      # Initial temperature for calibration
    learnable_temperature: true  # Allow temperature to be learned

  # Robustness loss (TRADES)
  robustness_loss:
    type: "trades"
    beta: 6.0  # TRADES regularization parameter (β)
    distance_metric: "kl"  # KL divergence

    # PGD adversarial generation
    attack:
      type: "pgd"
      epsilon: 0.031372549  # 8/255 in [0, 1] range
      num_steps: 7  # PGD-7 for training efficiency
      step_size: 0.007843137  # 2/255, step_size = ε / 4
      random_start: true
      norm: "linf"  # L∞ norm
      clip_min: 0.0
      clip_max: 1.0

  # Explanation loss (SSIM + TCAV)
  explanation_loss:
    type: "combined"
    gamma: 0.5  # TCAV weight in explanation loss

    # SSIM stability loss
    ssim:
      window_size: 11
      kernel_type: "gaussian"  # gaussian or uniform
      channel: 3  # RGB channels
      spatial: true  # Apply spatial averaging
      data_range: 1.0  # [0, 1] normalized images
      k1: 0.01
      k2: 0.03

    # TCAV semantic alignment
    tcav:
      num_random_concepts: 50  # Random concepts for statistical testing
      concept_layer: "layer4"  # Last ResNet block
      alpha: 0.05  # Significance level for hypothesis testing

      # Concept categories
      medical_concepts:
        - "pigment_network"
        - "blue_white_veil"
        - "milia_like_cysts"
        - "negative_network"
        - "irregular_pigmentation"
        - "regression_structures"
        - "dots_and_globules"
        - "streaks"
        - "asymmetry"
        - "irregular_border"

      artifact_concepts:
        - "hair_occlusion"
        - "ruler_marks"
        - "gel_bubbles"
        - "ink_marks"
        - "vignetting"
        - "color_calibration"
        - "jpeg_artifacts"
        - "blur"
        - "dark_corners"
        - "flash_reflection"

# Validation and evaluation
validation:
  # Validation frequency
  val_interval: 1  # Validate every epoch

  # Adversarial evaluation during validation
  adversarial_eval:
    enabled: true
    attack:
      type: "pgd"
      epsilon: 0.031372549  # 8/255
      num_steps: 20  # PGD-20 for stronger evaluation
      step_size: 0.003921569  # ε / 8
      random_start: true
      norm: "linf"

  # Explanation quality evaluation
  explanation_eval:
    enabled: true
    num_samples: 100  # Evaluate on subset for speed
    methods: ["gradcam"]  # GradCAM explanations
    target_layer: "layer4"  # Last ResNet block

  # Calibration evaluation
  calibration_eval:
    enabled: true
    num_bins: 15  # ECE histogram bins

# Logging configuration
logging:
  # Console logging
  log_interval: 10  # Log every 10 batches

  # MLflow tracking
  use_mlflow: true
  mlflow_tracking_uri: "file:./mlruns"
  mlflow_experiment_name: "tri-objective-isic2018"

  # TensorBoard (optional)
  use_tensorboard: false

  # Metrics to log
  metrics:
    train:
      - "loss_total"
      - "loss_task"
      - "loss_robustness"
      - "loss_explanation"
      - "loss_ssim"
      - "loss_tcav"
      - "accuracy"
      - "learning_rate"
      - "temperature"
    val:
      - "loss_total"
      - "loss_task"
      - "loss_robustness"
      - "loss_explanation"
      - "accuracy_clean"
      - "accuracy_robust"
      - "ssim_mean"
      - "tcav_medical_score"
      - "tcav_artifact_score"
      - "ece"
      - "auroc_macro"

  # Checkpoint logging
  log_model: true
  log_best_model: true

  # Artifact logging
  log_confusion_matrix: true
  log_sample_predictions: true
  log_sample_explanations: true
  num_samples_to_log: 16

# Reproducibility settings
reproducibility:
  seed: 42  # Will be overridden by --seed argument in multi-seed runs
  deterministic_cudnn: true
  benchmark_cudnn: false  # Disable for reproducibility
  enable_tf32: false
  use_deterministic_algorithms: false  # Some ops don't support determinism
  dataloader_seed_offset: 0

# Performance monitoring
monitoring:
  # Training metrics thresholds (for alerts/early stopping)
  thresholds:
    min_train_accuracy: 0.50  # Flag if accuracy drops below 50%
    max_gradient_norm: 10.0   # Flag if gradients explode
    max_loss: 100.0           # Flag if loss diverges

  # Memory monitoring
  log_memory_usage: true
  memory_interval: 50  # Log GPU memory every 50 batches

  # Time monitoring
  log_epoch_time: true
  log_batch_time: false  # Too verbose

# Multi-seed experimental setup
multi_seed:
  seeds: [42, 123, 456]  # Three random seeds for statistical significance
  aggregate_results: true
  confidence_level: 0.95

# Expected results (A1+ grade criteria)
expected_results:
  clean_accuracy: 0.82  # ≥ 82%
  robust_accuracy_pgd20: 0.65  # ≥ 65%
  ssim_stability: 0.70  # ≥ 0.70
  tcav_medical_alignment: 0.60  # ≥ 0.60
  calibration_ece: 0.10  # ≤ 0.10
  training_time_per_epoch: "25-30 minutes (RTX 3050)"
  total_training_time: "25-30 hours (60 epochs)"
  convergence_epoch: "~35-40 epochs"

# Notes for dissertation
dissertation_notes:
  chapter: "5.2 - Tri-Objective Training"
  contributions:
    - "First end-to-end tri-objective framework for medical imaging"
    - "Balanced optimization of task, robustness, and explainability"
    - "Production-grade implementation with comprehensive evaluation"
    - "Multi-seed experimental design for statistical significance"
  limitations:
    - "Computational cost: 3x longer than baseline training"
    - "Hyperparameter sensitivity: λ_rob and λ_expl require tuning"
    - "Memory overhead: storing adversarial examples and explanations"
  future_work:
    - "Adaptive loss weighting (e.g., uncertainty-based scheduling)"
    - "Multi-GPU distributed training for faster convergence"
    - "Extension to other medical imaging modalities (CT, MRI, X-ray)"
