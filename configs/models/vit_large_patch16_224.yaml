# Vision Transformer Large (ViT-L/16) Configuration
# Large transformer model for high-capacity tasks

model:
  name: vit_large_patch16_224
  architecture: vit_large_patch16_224
  pretrained: true
  num_classes: 7
  
  # ViT-specific settings
  img_size: 224
  patch_size: 16
  embed_dim: 1024
  depth: 24
  num_heads: 16
  mlp_ratio: 4.0
  
  # Dropout
  drop_rate: 0.1
  attn_drop_rate: 0.0
  drop_path_rate: 0.2  # Higher stochastic depth
  
  representation_size: null
  pos_embed_type: learned

training:
  optimizer:
    type: adamw
    lr: 0.00005  # Lower LR for large model
    weight_decay: 0.05
    betas: [0.9, 0.999]
    
  scheduler:
    type: cosine
    T_max: 100
    eta_min: 0.000001
    warmup_epochs: 10
    
  loss:
    type: cross_entropy
    label_smoothing: 0.1
    
  batch_size: 16  # Smaller batch for large model
  num_epochs: 100
  gradient_clip: 1.0
  use_amp: true

augmentation:
  train:
    - RandomResizedCrop:
        size: 224
        scale: [0.8, 1.0]
        interpolation: bicubic
    - RandomHorizontalFlip:
        p: 0.5
    - RandAugment:
        num_ops: 2
        magnitude: 9
    - Normalize:
        mean: [0.5, 0.5, 0.5]
        std: [0.5, 0.5, 0.5]
        
  val:
    - Resize:
        size: 256
        interpolation: bicubic
    - CenterCrop:
        size: 224
    - Normalize:
        mean: [0.5, 0.5, 0.5]
        std: [0.5, 0.5, 0.5]

capacity:
  parameters: 304326632  # ~304.3M parameters
  flops: 61555538432  # ~61.6 GFLOPs
  num_patches: 196

tasks:
  dermoscopy:
    num_classes: 7
    batch_size: 16
    image_size: 224
    
  chest_xray:
    num_classes: 14
    batch_size: 8
    image_size: 224
    loss:
      type: bce_with_logits
      
  retinopathy:
    num_classes: 5
    batch_size: 16
    image_size: 224
