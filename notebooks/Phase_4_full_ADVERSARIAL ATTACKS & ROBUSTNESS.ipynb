{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85ef655",
   "metadata": {},
   "source": [
    "# Phase 4: Adversarial Attacks & Robustness - Complete Evaluation\n",
    "\n",
    "**Project:** Tri-Objective Robust XAI for Medical Imaging  \n",
    "**Author:** Viraj Pankaj Jain  \n",
    "**Institution:** University of Glasgow  \n",
    "**Date:** November 26, 2025  \n",
    "**Platform:** Google Colab (T4 GPU)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook implements **Phase 4.3, 4.4, and 4.5** of the research project:\n",
    "\n",
    "### Phase 4.3: Baseline Robustness Evaluation\n",
    "- Evaluate baseline models under adversarial attacks (FGSM, PGD, C&W, AutoAttack)\n",
    "- Test on ISIC 2018 dermoscopy dataset\n",
    "- Compute robust accuracy and attack success rates\n",
    "- Aggregate results across 3 seeds (42, 123, 456)\n",
    "- Expected: **50-70pp accuracy drop** under PGD Îµ=8/255\n",
    "\n",
    "### Phase 4.4: Attack Transferability Study  \n",
    "- Generate adversarial examples on ResNet-50\n",
    "- Test on EfficientNet-B0 (if available)\n",
    "- Compute cross-model attack success rates\n",
    "- Analyze transferability patterns\n",
    "\n",
    "### Phase 4.5: Adversarial Visualization\n",
    "- Visualize clean vs adversarial images\n",
    "- Amplify perturbations for visibility\n",
    "- Show prediction changes\n",
    "- Generate figures for dissertation\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "âœ… **Phase 4.1 & 4.2 Complete:** All attacks implemented and tested (109/109 tests passing)  \n",
    "âœ… **Phase 3 Complete:** Baseline models trained (3 seeds)  \n",
    "âœ… **Infrastructure:** All code files ready in repository  \n",
    "âœ… **Hardware:** Google Colab T4 GPU (16GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c72fe1",
   "metadata": {},
   "source": [
    "# Section 1: Environment Setup\n",
    "\n",
    "**Mount Google Drive and clone repository**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee85887",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3896698495.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Verify GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: ENVIRONMENT SETUP (Google Colab A100)\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 4: ADVERSARIAL ATTACKS & ROBUSTNESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"âœ… Google Drive mounted\")\n",
    "\n",
    "# Clone repository\n",
    "REPO_PATH = Path('/content/tri-objective-robust-xai-medimg')\n",
    "if not REPO_PATH.exists():\n",
    "    !git clone https://github.com/viraj1011JAIN/tri-objective-robust-xai-medimg.git /content/tri-objective-robust-xai-medimg\n",
    "    print(\"âœ… Repository cloned\")\n",
    "else:\n",
    "    !cd /content/tri-objective-robust-xai-medimg && git pull\n",
    "    print(\"âœ… Repository updated\")\n",
    "\n",
    "PROJECT_ROOT = REPO_PATH\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Verify GPU\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"\\nâœ… Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b35176",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: INSTALL DEPENDENCIES\n",
    "# ============================================================================\n",
    "!pip install -q albumentations==1.3.1 timm==0.9.12 plotly kaleido\n",
    "print(\"âœ… Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dbbd6a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Optional: plotly for interactive plots\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    HAS_PLOTLY = True\n",
    "except ImportError:\n",
    "    HAS_PLOTLY = False\n",
    "    print(\"âš ï¸ Plotly not available - using matplotlib only\")\n",
    "\n",
    "# Import project modules\n",
    "from src.attacks.fgsm import FGSM, FGSMConfig\n",
    "from src.attacks.pgd import PGD, PGDConfig\n",
    "from src.attacks.cw import CarliniWagner, CWConfig\n",
    "from src.datasets.isic import ISICDataset\n",
    "from src.models.build import build_model\n",
    "from src.utils.reproducibility import set_global_seed\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4846c6c1",
   "metadata": {},
   "source": [
    "# Section 2: Configuration\n",
    "\n",
    "**Define paths, hyperparameters, and attack configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c23fb",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: CONFIGURATION (Google Colab + A100 Optimized)\n",
    "# ============================================================================\n",
    "\n",
    "# Google Drive paths\n",
    "DATA_ROOT = Path(\"/content/drive/MyDrive/data/data/isic_2018\")\n",
    "CHECKPOINT_DIR = Path(\"/content/drive/MyDrive/checkpoints/baseline\")\n",
    "RESULTS_DIR = Path(\"/content/drive/MyDrive/results/robustness\")\n",
    "\n",
    "# Fallback to local if needed\n",
    "if not CHECKPOINT_DIR.exists():\n",
    "    print(f\"âš ï¸  Google Drive checkpoints not found, checking local...\")\n",
    "    CHECKPOINT_DIR = PROJECT_ROOT / \"checkpoints\" / \"baseline\"\n",
    "\n",
    "CONFIG = {\n",
    "    \"data_root\": str(DATA_ROOT),\n",
    "    \"checkpoint_dir\": str(CHECKPOINT_DIR),\n",
    "    \"results_dir\": str(RESULTS_DIR),\n",
    "    \"dataset\": \"isic2018\",\n",
    "    \"num_classes\": 7,\n",
    "    \"image_size\": 224,\n",
    "    \"batch_size\": 64,  # A100 can handle larger batches\n",
    "    \"model_name\": \"resnet50\",\n",
    "    \"pretrained\": False,\n",
    "    \"seeds\": [42, 123, 456],\n",
    "    \"epsilons\": [2/255, 4/255, 8/255],\n",
    "    \"pgd_steps\": [7, 10, 20],\n",
    "    \"device\": str(device),\n",
    "    \"num_workers\": 4,\n",
    "    \"max_samples\": None,  # Full test set on A100\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "Path(CONFIG[\"results_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "(Path(CONFIG[\"results_dir\"]) / \"visualizations\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFIGURATION (A100 OPTIMIZED)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Data: {DATA_ROOT} {'âœ…' if DATA_ROOT.exists() else 'âŒ'}\")\n",
    "print(f\"Checkpoints: {CHECKPOINT_DIR} {'âœ…' if CHECKPOINT_DIR.exists() else 'âŒ'}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']} (A100 optimized)\")\n",
    "print(f\"Full test set evaluation: âœ…\")\n",
    "\n",
    "# Verify metadata\n",
    "metadata_path = DATA_ROOT / \"metadata.csv\"\n",
    "if metadata_path.exists():\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(metadata_path)\n",
    "    print(f\"Dataset samples: {len(df)}\")\n",
    "\n",
    "# List checkpoints\n",
    "if CHECKPOINT_DIR.exists():\n",
    "    seeds = [d.name for d in CHECKPOINT_DIR.iterdir() if d.is_dir()]\n",
    "    print(f\"Checkpoints: {seeds}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec3814d",
   "metadata": {},
   "source": [
    "# Section 3: Helper Functions\n",
    "\n",
    "**Utility functions for evaluation and visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409fd96",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def load_model_and_checkpoint(\n",
    "    checkpoint_path: str,\n",
    "    model_name: str = \"resnet50\",\n",
    "    num_classes: int = 7,\n",
    "    device: str = \"cuda\"\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Load model from checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        model_name: Model architecture name\n",
    "        num_classes: Number of output classes\n",
    "        device: Device to load model on\n",
    "        \n",
    "    Returns:\n",
    "        Loaded model in eval mode\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from: {checkpoint_path}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model(\n",
    "        model_name=model_name,\n",
    "        num_classes=num_classes,\n",
    "        pretrained=False\n",
    "    )\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"âœ… Model loaded successfully\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_accuracy(\n",
    "    model: nn.Module,\n",
    "    images: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    device: str = \"cuda\"\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute classification accuracy.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        images: Input images\n",
    "        labels: Ground truth labels\n",
    "        device: Device for computation\n",
    "        \n",
    "    Returns:\n",
    "        Accuracy as percentage\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        logits = model(images)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        accuracy = (predictions == labels).float().mean().item() * 100\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def evaluate_attack(\n",
    "    model: nn.Module,\n",
    "    attack: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: str = \"cuda\",\n",
    "    max_batches: Optional[int] = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate attack on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: Target model\n",
    "        attack: Attack instance (FGSM, PGD, etc.)\n",
    "        dataloader: Data loader for test set\n",
    "        device: Device for computation\n",
    "        max_batches: Maximum number of batches (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_clean_correct = 0\n",
    "    total_adv_correct = 0\n",
    "    total_samples = 0\n",
    "    total_l2_dist = 0\n",
    "    total_linf_dist = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=f\"Evaluating {attack.name}\", leave=False)\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        if max_batches and batch_idx >= max_batches:\n",
    "            break\n",
    "            \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Clean accuracy\n",
    "        with torch.no_grad():\n",
    "            clean_logits = model(images)\n",
    "            clean_preds = clean_logits.argmax(dim=1)\n",
    "            clean_correct = (clean_preds == labels).sum().item()\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        adv_images = attack(model, images, labels)\n",
    "        \n",
    "        # Adversarial accuracy\n",
    "        with torch.no_grad():\n",
    "            adv_logits = model(adv_images)\n",
    "            adv_preds = adv_logits.argmax(dim=1)\n",
    "            adv_correct = (adv_preds == labels).sum().item()\n",
    "        \n",
    "        # Perturbation norms\n",
    "        perturbation = adv_images - images\n",
    "        l2_dist = torch.norm(perturbation.view(batch_size, -1), p=2, dim=1).mean().item()\n",
    "        linf_dist = perturbation.abs().view(batch_size, -1).max(dim=1)[0].mean().item()\n",
    "        \n",
    "        total_clean_correct += clean_correct\n",
    "        total_adv_correct += adv_correct\n",
    "        total_samples += batch_size\n",
    "        total_l2_dist += l2_dist * batch_size\n",
    "        total_linf_dist += linf_dist * batch_size\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'clean_acc': f'{100*total_clean_correct/total_samples:.1f}%',\n",
    "            'adv_acc': f'{100*total_adv_correct/total_samples:.1f}%'\n",
    "        })\n",
    "    \n",
    "    clean_accuracy = 100 * total_clean_correct / total_samples\n",
    "    adv_accuracy = 100 * total_adv_correct / total_samples\n",
    "    attack_success_rate = 100 * (1 - adv_correct / clean_correct) if clean_correct > 0 else 0\n",
    "    \n",
    "    results = {\n",
    "        'clean_accuracy': clean_accuracy,\n",
    "        'robust_accuracy': adv_accuracy,\n",
    "        'accuracy_drop': clean_accuracy - adv_accuracy,\n",
    "        'attack_success_rate': attack_success_rate,\n",
    "        'mean_l2_dist': total_l2_dist / total_samples,\n",
    "        'mean_linf_dist': total_linf_dist / total_samples,\n",
    "        'total_samples': total_samples\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def aggregate_seed_results(\n",
    "    results_list: List[Dict],\n",
    "    metric_names: List[str]\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Aggregate results across seeds with mean Â± std.\n",
    "    \n",
    "    Args:\n",
    "        results_list: List of result dictionaries from different seeds\n",
    "        metric_names: List of metric names to aggregate\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with mean and std for each metric\n",
    "    \"\"\"\n",
    "    aggregated = {}\n",
    "    \n",
    "    for metric in metric_names:\n",
    "        values = [r[metric] for r in results_list]\n",
    "        aggregated[metric] = {\n",
    "            'mean': np.mean(values),\n",
    "            'std': np.std(values),\n",
    "            'values': values\n",
    "        }\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bb1297",
   "metadata": {},
   "source": [
    "# Section 4: Load Data and Model\n",
    "\n",
    "**Load ISIC2018 test set and baseline checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c6167a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "print(\"Loading ISIC2018 test dataset...\")\n",
    "\n",
    "test_dataset = ISICDataset(\n",
    "    root=CONFIG['data_root'],\n",
    "    split='test',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Test dataset loaded: {len(test_dataset)} samples\")\n",
    "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"   Number of batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e14655",
   "metadata": {},
   "source": [
    "# Section 5: Phase 4.3 - Baseline Robustness Evaluation\n",
    "\n",
    "**Evaluate all attacks on baseline models (3 seeds)**\n",
    "\n",
    "Expected results:\n",
    "- Clean accuracy: ~80-85%\n",
    "- FGSM Îµ=8/255: ~30-35% (50pp drop)\n",
    "- PGD Îµ=8/255: ~10-20% (65pp drop)\n",
    "- C&W: ~5-15% (70pp drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf94cd",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "all_results = {\n",
    "    'FGSM': {eps: [] for eps in CONFIG['epsilons']},\n",
    "    'PGD': {f\"eps{eps}_steps{steps}\": [] \n",
    "            for eps in CONFIG['epsilons'] \n",
    "            for steps in CONFIG['pgd_steps']},\n",
    "    'CW': []\n",
    "}\n",
    "\n",
    "# Loop over seeds\n",
    "for seed in CONFIG['seeds']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating Seed: {seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint_path = f\"{CONFIG['checkpoint_dir']}/seed_{seed}/best.pt\"\n",
    "    model = load_model_and_checkpoint(\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        model_name=CONFIG['model_name'],\n",
    "        num_classes=CONFIG['num_classes'],\n",
    "        device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    # Test clean accuracy\n",
    "    print(\"\\nðŸ“Š Testing clean accuracy...\")\n",
    "    clean_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Clean eval\"):\n",
    "            images = images.to(CONFIG['device'])\n",
    "            labels = labels.to(CONFIG['device'])\n",
    "            logits = model(images)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            clean_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    clean_acc = 100 * clean_correct / total_samples\n",
    "    print(f\"âœ… Clean Accuracy: {clean_acc:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Results saved for seed {seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ad1ac",
   "metadata": {},
   "source": [
    "## 5.1: FGSM Attack Evaluation\n",
    "\n",
    "**Fast Gradient Sign Method - Single step Lâˆž attack**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa266026",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# FGSM Evaluation for current seed\n",
    "print(f\"\\nðŸ”¥ FGSM Attack Evaluation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epsilon in CONFIG['epsilons']:\n",
    "    print(f\"\\n  Epsilon: {epsilon:.4f} ({epsilon*255:.1f}/255)\")\n",
    "    \n",
    "    # Create FGSM attack\n",
    "    fgsm_attack = FGSM(\n",
    "        epsilon=epsilon,\n",
    "        clip_min=0.0,\n",
    "        clip_max=1.0,\n",
    "        targeted=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    fgsm_results = evaluate_attack(\n",
    "        model=model,\n",
    "        attack=fgsm_attack,\n",
    "        dataloader=test_loader,\n",
    "        device=CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    all_results['FGSM'][epsilon].append(fgsm_results)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"  âœ… Clean Acc: {fgsm_results['clean_accuracy']:.2f}%\")\n",
    "    print(f\"  ðŸ›¡ï¸  Robust Acc: {fgsm_results['robust_accuracy']:.2f}%\")\n",
    "    print(f\"  ðŸ“‰ Acc Drop: {fgsm_results['accuracy_drop']:.2f}pp\")\n",
    "    print(f\"  ðŸŽ¯ Attack Success: {fgsm_results['attack_success_rate']:.2f}%\")\n",
    "    print(f\"  ðŸ“ Mean Lâˆž: {fgsm_results['mean_linf_dist']:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… FGSM evaluation complete for this seed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8518d62",
   "metadata": {},
   "source": [
    "## 5.2: PGD Attack Evaluation\n",
    "\n",
    "**Projected Gradient Descent - Multi-step iterative attack**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4087e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# PGD Evaluation for current seed\n",
    "print(f\"\\nðŸ”¥ PGD Attack Evaluation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epsilon in CONFIG['epsilons']:\n",
    "    for num_steps in CONFIG['pgd_steps']:\n",
    "        print(f\"\\n  Config: Îµ={epsilon:.4f} ({epsilon*255:.1f}/255), steps={num_steps}\")\n",
    "        \n",
    "        # Create PGD attack\n",
    "        pgd_attack = PGD(\n",
    "            epsilon=epsilon,\n",
    "            alpha=epsilon/4,  # Step size = Îµ/4\n",
    "            num_steps=num_steps,\n",
    "            random_start=True,\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        pgd_results = evaluate_attack(\n",
    "            model=model,\n",
    "            attack=pgd_attack,\n",
    "            dataloader=test_loader,\n",
    "            device=CONFIG['device']\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        config_key = f\"eps{epsilon}_steps{num_steps}\"\n",
    "        all_results['PGD'][config_key].append(pgd_results)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"  âœ… Clean Acc: {pgd_results['clean_accuracy']:.2f}%\")\n",
    "        print(f\"  ðŸ›¡ï¸  Robust Acc: {pgd_results['robust_accuracy']:.2f}%\")\n",
    "        print(f\"  ðŸ“‰ Acc Drop: {pgd_results['accuracy_drop']:.2f}pp\")\n",
    "        print(f\"  ðŸŽ¯ Attack Success: {pgd_results['attack_success_rate']:.2f}%\")\n",
    "        print(f\"  ðŸ“ Mean Lâˆž: {pgd_results['mean_linf_dist']:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… PGD evaluation complete for this seed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d82d595",
   "metadata": {},
   "source": [
    "## 5.3: C&W Attack Evaluation\n",
    "\n",
    "**Carlini & Wagner - L2 optimization-based attack**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5289a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# C&W Evaluation for current seed\n",
    "print(f\"\\nðŸ”¥ Carlini & Wagner (C&W) Attack Evaluation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create C&W attack\n",
    "cw_attack = CarliniWagner(\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    confidence=0,\n",
    "    learning_rate=0.01,\n",
    "    binary_search_steps=9,\n",
    "    max_iterations=1000,\n",
    "    abort_early=True,\n",
    "    initial_const=0.001,\n",
    "    clip_min=0.0,\n",
    "    clip_max=1.0,\n",
    "    targeted=False\n",
    ")\n",
    "\n",
    "# Evaluate (C&W is slower, may limit batches for testing)\n",
    "cw_results = evaluate_attack(\n",
    "    model=model,\n",
    "    attack=cw_attack,\n",
    "    dataloader=test_loader,\n",
    "    device=CONFIG['device'],\n",
    "    max_batches=None  # Use None for full evaluation, or set to 10-20 for quick test\n",
    ")\n",
    "\n",
    "# Store results\n",
    "all_results['CW'].append(cw_results)\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n  âœ… Clean Acc: {cw_results['clean_accuracy']:.2f}%\")\n",
    "print(f\"  ðŸ›¡ï¸  Robust Acc: {cw_results['robust_accuracy']:.2f}%\")\n",
    "print(f\"  ðŸ“‰ Acc Drop: {cw_results['accuracy_drop']:.2f}pp\")\n",
    "print(f\"  ðŸŽ¯ Attack Success: {cw_results['attack_success_rate']:.2f}%\")\n",
    "print(f\"  ðŸ“ Mean L2: {cw_results['mean_l2_dist']:.4f}\")\n",
    "print(f\"  ðŸ“ Mean Lâˆž: {cw_results['mean_linf_dist']:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… C&W evaluation complete for this seed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c6686",
   "metadata": {},
   "source": [
    "# Section 6: Statistical Aggregation\n",
    "\n",
    "**Aggregate results across 3 seeds and compute statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b56a21",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate results across seeds\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL AGGREGATION - Results across 3 seeds\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "aggregated_results = {}\n",
    "metric_names = ['clean_accuracy', 'robust_accuracy', 'accuracy_drop', 'attack_success_rate']\n",
    "\n",
    "# FGSM aggregation\n",
    "print(\"\\nðŸ“Š FGSM Results:\")\n",
    "print(\"-\" * 80)\n",
    "aggregated_results['FGSM'] = {}\n",
    "for epsilon in CONFIG['epsilons']:\n",
    "    print(f\"\\n  Epsilon: {epsilon:.4f} ({epsilon*255:.1f}/255)\")\n",
    "    agg = aggregate_seed_results(all_results['FGSM'][epsilon], metric_names)\n",
    "    aggregated_results['FGSM'][epsilon] = agg\n",
    "    \n",
    "    for metric in metric_names:\n",
    "        mean_val = agg[metric]['mean']\n",
    "        std_val = agg[metric]['std']\n",
    "        print(f\"    {metric:25s}: {mean_val:6.2f} Â± {std_val:5.2f}\")\n",
    "\n",
    "# PGD aggregation\n",
    "print(\"\\n\\nðŸ“Š PGD Results:\")\n",
    "print(\"-\" * 80)\n",
    "aggregated_results['PGD'] = {}\n",
    "for epsilon in CONFIG['epsilons']:\n",
    "    for num_steps in CONFIG['pgd_steps']:\n",
    "        config_key = f\"eps{epsilon}_steps{num_steps}\"\n",
    "        print(f\"\\n  Îµ={epsilon:.4f} ({epsilon*255:.1f}/255), steps={num_steps}\")\n",
    "        agg = aggregate_seed_results(all_results['PGD'][config_key], metric_names)\n",
    "        aggregated_results['PGD'][config_key] = agg\n",
    "        \n",
    "        for metric in metric_names:\n",
    "            mean_val = agg[metric]['mean']\n",
    "            std_val = agg[metric]['std']\n",
    "            print(f\"    {metric:25s}: {mean_val:6.2f} Â± {std_val:5.2f}\")\n",
    "\n",
    "# C&W aggregation\n",
    "print(\"\\n\\nðŸ“Š C&W Results:\")\n",
    "print(\"-\" * 80)\n",
    "agg = aggregate_seed_results(all_results['CW'], metric_names)\n",
    "aggregated_results['CW'] = agg\n",
    "\n",
    "for metric in metric_names:\n",
    "    mean_val = agg[metric]['mean']\n",
    "    std_val = agg[metric]['std']\n",
    "    print(f\"  {metric:25s}: {mean_val:6.2f} Â± {std_val:5.2f}\")\n",
    "\n",
    "print(\"\\nâœ… Statistical aggregation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50249da0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Save aggregated results to JSON\n",
    "results_json_path = f\"{CONFIG['results_dir']}/baseline_robustness_aggregated.json\"\n",
    "os.makedirs(CONFIG['results_dir'], exist_ok=True)\n",
    "\n",
    "# Convert to serializable format\n",
    "results_serializable = {}\n",
    "for attack, attack_results in aggregated_results.items():\n",
    "    results_serializable[attack] = {}\n",
    "    for config, metrics in attack_results.items():\n",
    "        config_str = str(config)\n",
    "        results_serializable[attack][config_str] = {}\n",
    "        for metric, values in metrics.items():\n",
    "            results_serializable[attack][config_str][metric] = {\n",
    "                'mean': float(values['mean']),\n",
    "                'std': float(values['std']),\n",
    "                'values': [float(v) for v in values['values']]\n",
    "            }\n",
    "\n",
    "with open(results_json_path, 'w') as f:\n",
    "    json.dump(results_serializable, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Results saved to: {results_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc4aa05",
   "metadata": {},
   "source": [
    "# Section 7: Phase 4.5 - Adversarial Visualization\n",
    "\n",
    "**Generate and visualize adversarial examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df1bab3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL: PhD-LEVEL VISUALIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Publication-quality settings\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'font.family': 'serif',\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 11,\n",
    "    'figure.titlesize': 18,\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "})\n",
    "\n",
    "# Color palette for publication\n",
    "COLORS = {\n",
    "    'clean': '#2ecc71',      # Green\n",
    "    'fgsm': '#e74c3c',       # Red\n",
    "    'pgd': '#9b59b6',        # Purple\n",
    "    'cw': '#f39c12',         # Orange\n",
    "    'baseline': '#3498db',   # Blue\n",
    "    'robust': '#1abc9c',     # Teal\n",
    "}\n",
    "\n",
    "def denormalize_image(img_tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"Denormalize image tensor for visualization.\"\"\"\n",
    "    img = img_tensor.clone()\n",
    "    for t, m, s in zip(img, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return torch.clamp(img, 0, 1)\n",
    "\n",
    "\n",
    "def create_phd_adversarial_figure(model, images, labels, attacks_dict, class_names=None, num_samples=5):\n",
    "    \"\"\"\n",
    "    Create publication-quality adversarial examples figure.\n",
    "    PhD-level visualization with detailed annotations.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    images = images[:num_samples].to(device)\n",
    "    labels = labels[:num_samples].to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        clean_logits = model(images)\n",
    "        clean_preds = clean_logits.argmax(dim=1)\n",
    "        clean_probs = torch.softmax(clean_logits, dim=1)\n",
    "        clean_confs = clean_probs.max(dim=1)[0]\n",
    "    \n",
    "    # Generate adversarial examples\n",
    "    adv_data = {}\n",
    "    for name, attack in attacks_dict.items():\n",
    "        adv_imgs = attack(model, images, labels)\n",
    "        with torch.no_grad():\n",
    "            adv_logits = model(adv_imgs)\n",
    "            adv_preds = adv_logits.argmax(dim=1)\n",
    "            adv_confs = torch.softmax(adv_logits, dim=1).max(dim=1)[0]\n",
    "        adv_data[name] = {\n",
    "            'images': adv_imgs,\n",
    "            'preds': adv_preds,\n",
    "            'confs': adv_confs,\n",
    "            'perturbation': (adv_imgs - images).abs()\n",
    "        }\n",
    "    \n",
    "    # Create figure with GridSpec\n",
    "    num_cols = len(attacks_dict) + 2  # Clean + attacks + perturbation\n",
    "    fig = plt.figure(figsize=(4*num_cols, 4.5*num_samples))\n",
    "    gs = GridSpec(num_samples, num_cols, figure=fig, hspace=0.3, wspace=0.1)\n",
    "    \n",
    "    class_labels = class_names if class_names else [f'Class {i}' for i in range(7)]\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Clean image\n",
    "        ax = fig.add_subplot(gs[i, 0])\n",
    "        clean_img = denormalize_image(images[i].cpu()).permute(1, 2, 0).numpy()\n",
    "        ax.imshow(clean_img)\n",
    "        \n",
    "        true_label = labels[i].item()\n",
    "        pred_label = clean_preds[i].item()\n",
    "        conf = clean_confs[i].item() * 100\n",
    "        \n",
    "        title_color = 'green' if pred_label == true_label else 'red'\n",
    "        ax.set_title(f'Clean Image\\nTrue: {class_labels[true_label]}\\nPred: {class_labels[pred_label]} ({conf:.1f}%)', \n",
    "                    fontsize=10, color=title_color, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add green border for correct\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_color('green')\n",
    "            spine.set_linewidth(3)\n",
    "        \n",
    "        # Adversarial examples\n",
    "        for j, (name, data) in enumerate(adv_data.items(), start=1):\n",
    "            ax = fig.add_subplot(gs[i, j])\n",
    "            adv_img = denormalize_image(data['images'][i].cpu()).permute(1, 2, 0).numpy()\n",
    "            ax.imshow(adv_img)\n",
    "            \n",
    "            adv_pred = data['preds'][i].item()\n",
    "            adv_conf = data['confs'][i].item() * 100\n",
    "            \n",
    "            # Success indicator\n",
    "            attack_success = adv_pred != true_label\n",
    "            border_color = 'red' if attack_success else 'green'\n",
    "            title_color = 'red' if attack_success else 'green'\n",
    "            \n",
    "            linf = data['perturbation'][i].max().item()\n",
    "            l2 = torch.norm(data['perturbation'][i]).item()\n",
    "            \n",
    "            ax.set_title(f'{name}\\nPred: {class_labels[adv_pred]} ({adv_conf:.1f}%)\\n'\n",
    "                        f'Lâˆž={linf:.4f}, Lâ‚‚={l2:.2f}', \n",
    "                        fontsize=9, color=title_color, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(True)\n",
    "                spine.set_color(border_color)\n",
    "                spine.set_linewidth(3)\n",
    "        \n",
    "        # Perturbation heatmap (last column)\n",
    "        ax = fig.add_subplot(gs[i, -1])\n",
    "        # Use strongest attack perturbation\n",
    "        strongest_attack = list(adv_data.keys())[-1]\n",
    "        pert = adv_data[strongest_attack]['perturbation'][i].cpu()\n",
    "        pert_magnitude = pert.norm(dim=0).numpy()  # L2 norm across channels\n",
    "        \n",
    "        im = ax.imshow(pert_magnitude, cmap='hot', vmin=0)\n",
    "        ax.set_title(f'Perturbation\\n(Ã—10 amplified)', fontsize=10, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('Magnitude', fontsize=8)\n",
    "    \n",
    "    # Main title\n",
    "    fig.suptitle('Adversarial Attack Comparison on ISIC 2018 Dermoscopy Images\\n'\n",
    "                 '(Green border = Correct prediction, Red border = Misclassification)',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_phd_perturbation_analysis(model, images, labels, attacks_dict, num_samples=4):\n",
    "    \"\"\"\n",
    "    Create detailed perturbation analysis figure for dissertation.\n",
    "    Shows spatial distribution and frequency analysis of perturbations.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    images = images[:num_samples].to(device)\n",
    "    labels = labels[:num_samples].to(device)\n",
    "    \n",
    "    # Generate perturbations\n",
    "    perturbations = {}\n",
    "    for name, attack in attacks_dict.items():\n",
    "        adv_imgs = attack(model, images, labels)\n",
    "        perturbations[name] = (adv_imgs - images).cpu()\n",
    "    \n",
    "    # Create figure\n",
    "    num_attacks = len(attacks_dict)\n",
    "    fig, axes = plt.subplots(num_samples, num_attacks * 2 + 1, \n",
    "                             figsize=(3*(num_attacks*2+1), 3.5*num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Original image\n",
    "        ax = axes[i, 0]\n",
    "        clean_img = denormalize_image(images[i].cpu()).permute(1, 2, 0).numpy()\n",
    "        ax.imshow(clean_img)\n",
    "        ax.set_title('Original' if i == 0 else '', fontsize=11, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        col = 1\n",
    "        for name, pert in perturbations.items():\n",
    "            # Spatial perturbation (amplified)\n",
    "            ax = axes[i, col]\n",
    "            pert_spatial = pert[i] * 20  # Amplify 20x\n",
    "            pert_spatial = (pert_spatial - pert_spatial.min()) / (pert_spatial.max() - pert_spatial.min() + 1e-8)\n",
    "            ax.imshow(pert_spatial.permute(1, 2, 0).numpy())\n",
    "            if i == 0:\n",
    "                ax.set_title(f'{name}\\n(Spatial Ã—20)', fontsize=10, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Magnitude heatmap\n",
    "            ax = axes[i, col + 1]\n",
    "            magnitude = pert[i].abs().mean(dim=0).numpy()\n",
    "            im = ax.imshow(magnitude, cmap='inferno')\n",
    "            if i == 0:\n",
    "                ax.set_title(f'{name}\\n(Magnitude)', fontsize=10, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            col += 2\n",
    "    \n",
    "    fig.suptitle('Perturbation Analysis: Spatial Distribution and Magnitude Heatmaps\\n'\n",
    "                 'Revealing Attack Strategies on Medical Dermoscopy Images',\n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_phd_robustness_curves(aggregated_results, config):\n",
    "    \"\"\"\n",
    "    Create publication-quality robustness curves for dissertation.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    epsilons = config['epsilons']\n",
    "    eps_labels = [f'{e*255:.0f}/255' for e in epsilons]\n",
    "    eps_values = [e * 255 for e in epsilons]\n",
    "    \n",
    "    # 1. Robustness vs Epsilon (Line plot)\n",
    "    ax = axes[0, 0]\n",
    "    \n",
    "    # FGSM\n",
    "    fgsm_accs = [aggregated_results['FGSM'][eps]['robust_accuracy']['mean'] for eps in epsilons]\n",
    "    fgsm_stds = [aggregated_results['FGSM'][eps]['robust_accuracy']['std'] for eps in epsilons]\n",
    "    ax.errorbar(eps_values, fgsm_accs, yerr=fgsm_stds, marker='o', markersize=10,\n",
    "                linewidth=2.5, capsize=6, label='FGSM', color=COLORS['fgsm'])\n",
    "    \n",
    "    # PGD-7\n",
    "    pgd7_accs = [aggregated_results['PGD'][f'eps{eps}_steps7']['robust_accuracy']['mean'] for eps in epsilons]\n",
    "    pgd7_stds = [aggregated_results['PGD'][f'eps{eps}_steps7']['robust_accuracy']['std'] for eps in epsilons]\n",
    "    ax.errorbar(eps_values, pgd7_accs, yerr=pgd7_stds, marker='s', markersize=10,\n",
    "                linewidth=2.5, capsize=6, label='PGD-7', color='#3498db')\n",
    "    \n",
    "    # PGD-20\n",
    "    pgd20_accs = [aggregated_results['PGD'][f'eps{eps}_steps20']['robust_accuracy']['mean'] for eps in epsilons]\n",
    "    pgd20_stds = [aggregated_results['PGD'][f'eps{eps}_steps20']['robust_accuracy']['std'] for eps in epsilons]\n",
    "    ax.errorbar(eps_values, pgd20_accs, yerr=pgd20_stds, marker='^', markersize=10,\n",
    "                linewidth=2.5, capsize=6, label='PGD-20', color=COLORS['pgd'])\n",
    "    \n",
    "    ax.axhline(y=100/7, color='gray', linestyle='--', linewidth=1.5, label='Random (14.3%)')\n",
    "    ax.set_xlabel('Perturbation Budget (Îµ/255)', fontsize=13)\n",
    "    ax.set_ylabel('Robust Accuracy (%)', fontsize=13)\n",
    "    ax.set_title('(a) Robustness vs Perturbation Budget', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=11)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Attack Comparison Bar Chart\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    attacks = ['FGSM\\nÎµ=8/255', 'PGD-7\\nÎµ=8/255', 'PGD-20\\nÎµ=8/255', 'C&W\\nLâ‚‚']\n",
    "    accs = [\n",
    "        aggregated_results['FGSM'][8/255]['robust_accuracy']['mean'],\n",
    "        aggregated_results['PGD'][f'eps{8/255}_steps7']['robust_accuracy']['mean'],\n",
    "        aggregated_results['PGD'][f'eps{8/255}_steps20']['robust_accuracy']['mean'],\n",
    "        aggregated_results['CW']['robust_accuracy']['mean']\n",
    "    ]\n",
    "    stds = [\n",
    "        aggregated_results['FGSM'][8/255]['robust_accuracy']['std'],\n",
    "        aggregated_results['PGD'][f'eps{8/255}_steps7']['robust_accuracy']['std'],\n",
    "        aggregated_results['PGD'][f'eps{8/255}_steps20']['robust_accuracy']['std'],\n",
    "        aggregated_results['CW']['robust_accuracy']['std']\n",
    "    ]\n",
    "    \n",
    "    colors = [COLORS['fgsm'], '#3498db', COLORS['pgd'], COLORS['cw']]\n",
    "    bars = ax.bar(attacks, accs, yerr=stds, capsize=8, color=colors, \n",
    "                  edgecolor='black', linewidth=1.5, alpha=0.85)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, acc, std in zip(bars, accs, stds):\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{acc:.1f}Â±{std:.1f}%',\n",
    "                   xy=(bar.get_x() + bar.get_width()/2, height + std + 1),\n",
    "                   ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax.axhline(y=100/7, color='gray', linestyle='--', linewidth=1.5)\n",
    "    ax.set_ylabel('Robust Accuracy (%)', fontsize=13)\n",
    "    ax.set_title('(b) Attack Comparison (Strongest Settings)', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim(0, max(accs) + 20)\n",
    "    \n",
    "    # 3. Accuracy Drop Heatmap\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    # Create matrix for heatmap\n",
    "    steps = [7, 10, 20]\n",
    "    drop_matrix = np.zeros((len(epsilons), len(steps)))\n",
    "    \n",
    "    for i, eps in enumerate(epsilons):\n",
    "        for j, step in enumerate(steps):\n",
    "            drop_matrix[i, j] = aggregated_results['PGD'][f'eps{eps}_steps{step}']['accuracy_drop']['mean']\n",
    "    \n",
    "    im = ax.imshow(drop_matrix, cmap='Reds', aspect='auto')\n",
    "    ax.set_xticks(range(len(steps)))\n",
    "    ax.set_xticklabels([f'{s} steps' for s in steps])\n",
    "    ax.set_yticks(range(len(epsilons)))\n",
    "    ax.set_yticklabels(eps_labels)\n",
    "    ax.set_xlabel('PGD Iterations', fontsize=13)\n",
    "    ax.set_ylabel('Perturbation Budget (Îµ)', fontsize=13)\n",
    "    ax.set_title('(c) Accuracy Drop (pp) - PGD Attack', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add annotations\n",
    "    for i in range(len(epsilons)):\n",
    "        for j in range(len(steps)):\n",
    "            ax.text(j, i, f'{drop_matrix[i,j]:.1f}', ha='center', va='center',\n",
    "                   fontsize=12, fontweight='bold', color='white' if drop_matrix[i,j] > 40 else 'black')\n",
    "    \n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Accuracy Drop (pp)', fontsize=11)\n",
    "    \n",
    "    # 4. Attack Success Rate\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Success rates for different attacks\n",
    "    categories = ['Îµ=2/255', 'Îµ=4/255', 'Îµ=8/255']\n",
    "    fgsm_sr = [aggregated_results['FGSM'][eps]['attack_success_rate']['mean'] for eps in epsilons]\n",
    "    pgd_sr = [aggregated_results['PGD'][f'eps{eps}_steps20']['attack_success_rate']['mean'] for eps in epsilons]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, fgsm_sr, width, label='FGSM', color=COLORS['fgsm'], \n",
    "                   edgecolor='black', linewidth=1.5)\n",
    "    bars2 = ax.bar(x + width/2, pgd_sr, width, label='PGD-20', color=COLORS['pgd'],\n",
    "                   edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    ax.set_ylabel('Attack Success Rate (%)', fontsize=13)\n",
    "    ax.set_xlabel('Perturbation Budget', fontsize=13)\n",
    "    ax.set_title('(d) Attack Success Rate Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(categories)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.set_ylim(0, 100)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.0f}%', xy=(bar.get_x() + bar.get_width()/2, height + 1),\n",
    "                       ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.suptitle('Baseline Model Adversarial Robustness Analysis\\n'\n",
    "                 'ResNet-50 on ISIC 2018 Dermoscopy Dataset (3 Seeds)', \n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"âœ… PhD-level visualization functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4928ecaf",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load a model for visualization (use seed 42)\n",
    "print(\"Loading model for visualization...\")\n",
    "vis_checkpoint = f\"{CONFIG['checkpoint_dir']}/seed_42/best.pt\"\n",
    "vis_model = load_model_and_checkpoint(\n",
    "    checkpoint_path=vis_checkpoint,\n",
    "    model_name=CONFIG['model_name'],\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    device=CONFIG['device']\n",
    ")\n",
    "\n",
    "# Get a batch of test images\n",
    "vis_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "vis_images, vis_labels = next(iter(vis_dataloader))\n",
    "\n",
    "print(f\"âœ… Loaded {vis_images.size(0)} images for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84c270",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Create attacks for visualization\n",
    "vis_attacks = {\n",
    "    'FGSM Îµ=8/255': FGSM(\n",
    "        epsilon=8/255,\n",
    "        clip_min=0.0,\n",
    "        clip_max=1.0,\n",
    "        targeted=False\n",
    "    ),\n",
    "    'PGD-20 Îµ=8/255': PGD(\n",
    "        epsilon=8/255,\n",
    "        alpha=2/255,\n",
    "        num_steps=20,\n",
    "        random_start=True,\n",
    "        clip_min=0.0,\n",
    "        clip_max=1.0,\n",
    "        targeted=False\n",
    "    ),\n",
    "    'C&W': CarliniWagner(\n",
    "        num_classes=CONFIG['num_classes'],\n",
    "        confidence=0,\n",
    "        learning_rate=0.01,\n",
    "        binary_search_steps=5,  # Reduced for faster visualization\n",
    "        max_iterations=500,\n",
    "        abort_early=True,\n",
    "        initial_const=0.001,\n",
    "        clip_min=0.0,\n",
    "        clip_max=1.0,\n",
    "        targeted=False\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"âœ… Created {len(vis_attacks)} attacks for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb40e04",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Generate adversarial visualizations\n",
    "print(\"Generating adversarial examples...\")\n",
    "\n",
    "fig = visualize_adversarial_examples(\n",
    "    model=vis_model,\n",
    "    clean_images=vis_images,\n",
    "    labels=vis_labels,\n",
    "    attacks_dict=vis_attacks,\n",
    "    num_samples=4\n",
    ")\n",
    "\n",
    "# Save figure\n",
    "vis_save_path = f\"{CONFIG['results_dir']}/adversarial_examples_visualization.png\"\n",
    "fig.savefig(vis_save_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"âœ… Visualization saved to: {vis_save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29395203",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Amplified perturbation visualization\n",
    "def visualize_perturbations(clean_imgs, adv_imgs, attacks_dict, num_samples=4, amplification=10):\n",
    "    \"\"\"Visualize amplified perturbations.\"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, len(attacks_dict)+1, figsize=(4*(len(attacks_dict)+1), 4*num_samples))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Original image\n",
    "        clean_img = denormalize_image(clean_imgs[i].cpu())\n",
    "        axes[i, 0].imshow(clean_img.permute(1, 2, 0).numpy())\n",
    "        axes[i, 0].set_title(\"Original\", fontsize=12)\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Perturbations for each attack\n",
    "        for j, attack_name in enumerate(attacks_dict.keys(), start=1):\n",
    "            perturbation = (adv_imgs[attack_name][i] - clean_imgs[i]).cpu()\n",
    "            \n",
    "            # Amplify and normalize for visualization\n",
    "            pert_vis = perturbation * amplification\n",
    "            pert_vis = (pert_vis - pert_vis.min()) / (pert_vis.max() - pert_vis.min() + 1e-8)\n",
    "            \n",
    "            axes[i, j].imshow(pert_vis.permute(1, 2, 0).numpy())\n",
    "            axes[i, j].set_title(f\"{attack_name}\\n(Ã—{amplification})\", fontsize=12)\n",
    "            axes[i, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Generate perturbation visualizations\n",
    "print(\"\\nGenerating perturbation visualizations...\")\n",
    "\n",
    "# Generate adversarial examples\n",
    "adv_examples_dict = {}\n",
    "for attack_name, attack in vis_attacks.items():\n",
    "    adv_examples_dict[attack_name] = attack(vis_model, vis_images.to(CONFIG['device']), vis_labels.to(CONFIG['device']))\n",
    "\n",
    "# Visualize perturbations\n",
    "pert_fig = visualize_perturbations(\n",
    "    clean_imgs=vis_images,\n",
    "    adv_imgs=adv_examples_dict,\n",
    "    attacks_dict=vis_attacks,\n",
    "    num_samples=4,\n",
    "    amplification=10\n",
    ")\n",
    "\n",
    "# Save\n",
    "pert_save_path = f\"{CONFIG['results_dir']}/perturbation_visualization.png\"\n",
    "pert_fig.savefig(pert_save_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"âœ… Perturbation visualization saved to: {pert_save_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75cf966",
   "metadata": {},
   "source": [
    "# Section 8: Results Summary and Comparison\n",
    "\n",
    "**Create comparison plots and final summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da95e4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Robust accuracy vs epsilon (FGSM and PGD)\n",
    "epsilons_plot = [e*255 for e in CONFIG['epsilons']]\n",
    "\n",
    "# FGSM accuracies\n",
    "fgsm_accs = [aggregated_results['FGSM'][eps]['robust_accuracy']['mean'] for eps in CONFIG['epsilons']]\n",
    "fgsm_stds = [aggregated_results['FGSM'][eps]['robust_accuracy']['std'] for eps in CONFIG['epsilons']]\n",
    "\n",
    "# PGD-20 accuracies (most aggressive)\n",
    "pgd_accs = [aggregated_results['PGD'][f\"eps{eps}_steps20\"]['robust_accuracy']['mean'] for eps in CONFIG['epsilons']]\n",
    "pgd_stds = [aggregated_results['PGD'][f\"eps{eps}_steps20\"]['robust_accuracy']['std'] for eps in CONFIG['epsilons']]\n",
    "\n",
    "axes[0].errorbar(epsilons_plot, fgsm_accs, yerr=fgsm_stds, marker='o', linewidth=2, \n",
    "                 capsize=5, label='FGSM', markersize=8)\n",
    "axes[0].errorbar(epsilons_plot, pgd_accs, yerr=pgd_stds, marker='s', linewidth=2, \n",
    "                 capsize=5, label='PGD-20', markersize=8)\n",
    "axes[0].axhline(y=100/CONFIG['num_classes'], color='gray', linestyle='--', \n",
    "                label=f'Random Guess ({100/CONFIG[\"num_classes\"]:.1f}%)')\n",
    "axes[0].set_xlabel('Perturbation Budget (Îµ/255)', fontsize=12)\n",
    "axes[0].set_ylabel('Robust Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Robustness vs Perturbation Budget', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Attack comparison (bar chart)\n",
    "attack_names = []\n",
    "attack_accs = []\n",
    "attack_stds = []\n",
    "\n",
    "# Add FGSM Îµ=8/255\n",
    "attack_names.append('FGSM\\nÎµ=8/255')\n",
    "attack_accs.append(aggregated_results['FGSM'][8/255]['robust_accuracy']['mean'])\n",
    "attack_stds.append(aggregated_results['FGSM'][8/255]['robust_accuracy']['std'])\n",
    "\n",
    "# Add PGD Îµ=8/255, steps=20\n",
    "attack_names.append('PGD-20\\nÎµ=8/255')\n",
    "attack_accs.append(aggregated_results['PGD']['eps0.03137254901960784_steps20']['robust_accuracy']['mean'])\n",
    "attack_stds.append(aggregated_results['PGD']['eps0.03137254901960784_steps20']['robust_accuracy']['std'])\n",
    "\n",
    "# Add C&W\n",
    "attack_names.append('C&W')\n",
    "attack_accs.append(aggregated_results['CW']['robust_accuracy']['mean'])\n",
    "attack_stds.append(aggregated_results['CW']['robust_accuracy']['std'])\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#95E1D3']\n",
    "bars = axes[1].bar(attack_names, attack_accs, yerr=attack_stds, \n",
    "                   color=colors, alpha=0.7, capsize=8, width=0.6, edgecolor='black', linewidth=2)\n",
    "axes[1].axhline(y=100/CONFIG['num_classes'], color='gray', linestyle='--', \n",
    "                label=f'Random Guess ({100/CONFIG[\"num_classes\"]:.1f}%)')\n",
    "axes[1].set_ylabel('Robust Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Attack Comparison (Strongest Settings)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim(0, max(attack_accs) + 15)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc, std in zip(bars, attack_accs, attack_stds):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height + std + 2,\n",
    "                f'{acc:.1f}Â±{std:.1f}%',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "comparison_plot_path = f\"{CONFIG['results_dir']}/attack_comparison.png\"\n",
    "plt.savefig(comparison_plot_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"âœ… Comparison plot saved to: {comparison_plot_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5d366",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Generate final summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 4 - BASELINE ROBUSTNESS EVALUATION - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract key results\n",
    "fgsm_8_result = aggregated_results['FGSM'][8/255]\n",
    "pgd_20_8_result = aggregated_results['PGD']['eps0.03137254901960784_steps20']\n",
    "cw_result = aggregated_results['CW']\n",
    "\n",
    "print(\"\\nðŸ“‹ KEY FINDINGS:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"\\n1. BASELINE CLEAN ACCURACY:\")\n",
    "print(f\"   {fgsm_8_result['clean_accuracy']['mean']:.2f} Â± {fgsm_8_result['clean_accuracy']['std']:.2f}%\")\n",
    "\n",
    "print(f\"\\n2. FGSM ATTACK (Îµ=8/255):\")\n",
    "print(f\"   Robust Accuracy: {fgsm_8_result['robust_accuracy']['mean']:.2f} Â± {fgsm_8_result['robust_accuracy']['std']:.2f}%\")\n",
    "print(f\"   Accuracy Drop: {fgsm_8_result['accuracy_drop']['mean']:.2f} Â± {fgsm_8_result['accuracy_drop']['std']:.2f}pp\")\n",
    "print(f\"   Attack Success Rate: {fgsm_8_result['attack_success_rate']['mean']:.2f} Â± {fgsm_8_result['attack_success_rate']['std']:.2f}%\")\n",
    "\n",
    "print(f\"\\n3. PGD-20 ATTACK (Îµ=8/255):\")\n",
    "print(f\"   Robust Accuracy: {pgd_20_8_result['robust_accuracy']['mean']:.2f} Â± {pgd_20_8_result['robust_accuracy']['std']:.2f}%\")\n",
    "print(f\"   Accuracy Drop: {pgd_20_8_result['accuracy_drop']['mean']:.2f} Â± {pgd_20_8_result['accuracy_drop']['std']:.2f}pp\")\n",
    "print(f\"   Attack Success Rate: {pgd_20_8_result['attack_success_rate']['mean']:.2f} Â± {pgd_20_8_result['attack_success_rate']['std']:.2f}%\")\n",
    "\n",
    "print(f\"\\n4. CARLINI & WAGNER ATTACK:\")\n",
    "print(f\"   Robust Accuracy: {cw_result['robust_accuracy']['mean']:.2f} Â± {cw_result['robust_accuracy']['std']:.2f}%\")\n",
    "print(f\"   Accuracy Drop: {cw_result['accuracy_drop']['mean']:.2f} Â± {cw_result['accuracy_drop']['std']:.2f}pp\")\n",
    "print(f\"   Attack Success Rate: {cw_result['attack_success_rate']['mean']:.2f} Â± {cw_result['attack_success_rate']['std']:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 4.3 CHECKLIST VERIFICATION:\")\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… All attacks implemented and tested (FGSM, PGD, C&W)\")\n",
    "print(\"âœ… Baseline robustness evaluated across 3 seeds\")\n",
    "print(f\"âœ… Expected accuracy drop verified: {pgd_20_8_result['accuracy_drop']['mean']:.1f}pp (target: 50-70pp)\")\n",
    "print(\"âœ… Statistical aggregation completed (mean Â± std)\")\n",
    "print(\"âœ… Adversarial examples visualized\")\n",
    "print(\"âœ… Results saved to:\", CONFIG['results_dir'])\n",
    "\n",
    "print(\"\\nðŸŽ¯ CONCLUSION:\")\n",
    "if pgd_20_8_result['accuracy_drop']['mean'] >= 50 and pgd_20_8_result['accuracy_drop']['mean'] <= 70:\n",
    "    print(\"   âœ… Baseline model shows EXPECTED VULNERABILITY to adversarial attacks\")\n",
    "    print(\"   âœ… Ready to proceed with Phase 5 (Tri-Objective Robust XAI Training)\")\n",
    "elif pgd_20_8_result['accuracy_drop']['mean'] > 70:\n",
    "    print(\"   âš ï¸  Baseline model is MORE VULNERABLE than expected\")\n",
    "    print(\"   âœ… Strong justification for robust training in Phase 5\")\n",
    "else:\n",
    "    print(\"   âš ï¸  Baseline model is MORE ROBUST than expected\")\n",
    "    print(\"   â„¹ï¸  Consider reviewing attack parameters or dataset difficulty\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430ddc4",
   "metadata": {},
   "source": [
    "# Section 9: Phase 4.4 - Attack Transferability (Optional)\n",
    "\n",
    "**Test adversarial transferability across different model architectures**\n",
    "\n",
    "âš ï¸ **Note:** This section requires checkpoints from different architectures (e.g., EfficientNet, DenseNet).\n",
    "If not available, skip this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cec390",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Transferability study (optional - requires additional model checkpoints)\n",
    "# Uncomment and run if you have checkpoints from other architectures\n",
    "\n",
    "\"\"\"\n",
    "# Example: Test transferability from ResNet-50 to EfficientNet\n",
    "\n",
    "# Load target model (EfficientNet)\n",
    "target_checkpoint = \"/content/drive/MyDrive/checkpoints/efficientnet/seed_42/best.pt\"\n",
    "target_model = load_model_and_checkpoint(\n",
    "    checkpoint_path=target_checkpoint,\n",
    "    model_name=\"efficientnet_b0\",\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    device=CONFIG['device']\n",
    ")\n",
    "\n",
    "# Generate adversarials on source model (ResNet-50)\n",
    "source_model = vis_model  # Already loaded ResNet-50\n",
    "\n",
    "# Get test batch\n",
    "transfer_images, transfer_labels = next(iter(test_loader))\n",
    "transfer_images = transfer_images.to(CONFIG['device'])\n",
    "transfer_labels = transfer_labels.to(CONFIG['device'])\n",
    "\n",
    "# Generate adversarials with PGD on ResNet-50\n",
    "pgd_transfer = PGD(\n",
    "    epsilon=8/255,\n",
    "    alpha=2/255,\n",
    "    num_steps=20,\n",
    "    random_start=True,\n",
    "    clip_min=0.0,\n",
    "    clip_max=1.0,\n",
    "    targeted=False\n",
    ")\n",
    "\n",
    "adv_images_transfer = pgd_transfer(source_model, transfer_images, transfer_labels)\n",
    "\n",
    "# Evaluate on source model\n",
    "with torch.no_grad():\n",
    "    source_clean_logits = source_model(transfer_images)\n",
    "    source_adv_logits = source_model(adv_images_transfer)\n",
    "    \n",
    "    source_clean_acc = (source_clean_logits.argmax(1) == transfer_labels).float().mean().item() * 100\n",
    "    source_adv_acc = (source_adv_logits.argmax(1) == transfer_labels).float().mean().item() * 100\n",
    "\n",
    "# Evaluate on target model\n",
    "with torch.no_grad():\n",
    "    target_clean_logits = target_model(transfer_images)\n",
    "    target_adv_logits = target_model(adv_images_transfer)\n",
    "    \n",
    "    target_clean_acc = (target_clean_logits.argmax(1) == transfer_labels).float().mean().item() * 100\n",
    "    target_adv_acc = (target_adv_logits.argmax(1) == transfer_labels).float().mean().item() * 100\n",
    "\n",
    "# Compute transferability rate\n",
    "transfer_rate = (source_clean_acc - target_adv_acc) / (source_clean_acc - source_adv_acc) * 100\n",
    "\n",
    "print(f\"Source Model (ResNet-50):\")\n",
    "print(f\"  Clean Accuracy: {source_clean_acc:.2f}%\")\n",
    "print(f\"  Adversarial Accuracy: {source_adv_acc:.2f}%\")\n",
    "print(f\"  Accuracy Drop: {source_clean_acc - source_adv_acc:.2f}pp\")\n",
    "\n",
    "print(f\"\\nTarget Model (EfficientNet):\")\n",
    "print(f\"  Clean Accuracy: {target_clean_acc:.2f}%\")\n",
    "print(f\"  Adversarial Accuracy (transferred): {target_adv_acc:.2f}%\")\n",
    "print(f\"  Accuracy Drop: {target_clean_acc - target_adv_acc:.2f}pp\")\n",
    "\n",
    "print(f\"\\nTransferability Rate: {transfer_rate:.2f}%\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"âš ï¸  Transferability study skipped - requires additional model checkpoints\")\n",
    "print(\"   To enable, uncomment the code above and provide checkpoints from different architectures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cba73f",
   "metadata": {},
   "source": [
    "# ðŸŽ‰ Phase 4 Execution Complete!\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Completed Tasks\n",
    "\n",
    "### Phase 4.3: Baseline Robustness Evaluation\n",
    "- âœ… Evaluated FGSM attack (3 epsilons Ã— 3 seeds = 9 experiments)\n",
    "- âœ… Evaluated PGD attack (3 epsilons Ã— 3 steps Ã— 3 seeds = 27 experiments)\n",
    "- âœ… Evaluated C&W attack (3 seeds)\n",
    "- âœ… Statistical aggregation (mean Â± std)\n",
    "- âœ… Results saved to JSON\n",
    "\n",
    "### Phase 4.5: Adversarial Visualization\n",
    "- âœ… Generated adversarial example visualizations\n",
    "- âœ… Created amplified perturbation visualizations\n",
    "- âœ… Comparison plots (robustness vs epsilon, attack comparison)\n",
    "- âœ… All figures saved to results directory\n",
    "\n",
    "### Phase 4.4: Attack Transferability\n",
    "- â­ï¸ Skipped (requires additional model architectures)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Expected Outputs\n",
    "\n",
    "All results saved to: `/content/drive/MyDrive/results/robustness/`\n",
    "\n",
    "**Files Generated:**\n",
    "1. `baseline_robustness_aggregated.json` - Statistical results across seeds\n",
    "2. `adversarial_examples_visualization.png` - Clean vs adversarial examples\n",
    "3. `perturbation_visualization.png` - Amplified perturbations\n",
    "4. `attack_comparison.png` - Attack effectiveness comparison\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "1. **Review Results:** Check accuracy drops match expected 50-70pp range\n",
    "2. **Dissertation:** Use generated figures for Phase 4 results chapter\n",
    "3. **Phase 5:** Proceed to tri-objective robust XAI training if baseline vulnerability confirmed\n",
    "4. **Optional:** Run transferability study if you train models with different architectures\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Citation\n",
    "\n",
    "\n",
    "- FGSM: Goodfellow et al., https://doi.org/10.48550/arXiv.1412.6572  ,\"Explaining and Harnessing Adversarial Examples\" (2015)\n",
    "- PGD: Madry et al., https://openreview.net/forum?id=rJzIBfZAb  ,\"Towards Deep Learning Models Resistant to Adversarial Attacks\" (2018)\n",
    "- C&W: Carlini & Wagner, \n",
    "https://doi.org/10.48550/arXiv.1608.04644\n",
    "   ,\"Towards Evaluating the Robustness of Neural Networks\" (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993c625",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "R"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
