{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca3c4bc",
   "metadata": {},
   "source": [
    "# ðŸ›¡ï¸ Phase 4: Adversarial Robustness Evaluation\n",
    "\n",
    "## Tri-Objective Robust XAI for Medical Imaging\n",
    "\n",
    "**Author:** Viraj Pankaj Jain  \n",
    "**Institution:** University of Glasgow  \n",
    "**Date:** November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Overview\n",
    "\n",
    "This notebook evaluates the **adversarial robustness** of baseline ResNet-50 models trained on ISIC 2018 dermoscopy images. We systematically assess vulnerability to:\n",
    "\n",
    "| Attack | Type | Strength | Use Case |\n",
    "|--------|------|----------|----------|\n",
    "| **FGSM** | Gradient-based | Fast, single-step | Real-time threat assessment |\n",
    "| **PGD** | Iterative | Strong, multi-step | Reliable robustness benchmark |\n",
    "| **C&W** | Optimization | Strongest, minimal perturbation | Worst-case security analysis |\n",
    "\n",
    "## ðŸŽ¯ Research Questions Addressed\n",
    "\n",
    "- **RQ1:** How vulnerable are standard CNNs to adversarial attacks in medical imaging?\n",
    "- **RQ2:** How does attack strength (Îµ) affect model accuracy degradation?\n",
    "- **RQ3:** Which skin lesion classes are most vulnerable to adversarial perturbations?\n",
    "\n",
    "## ðŸ“Š Evaluation Protocol\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ADVERSARIAL EVALUATION PIPELINE                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. Load trained baseline models (Seeds: 42, 123, 456)          â”‚\n",
    "â”‚  2. Evaluate clean accuracy (sanity check)                      â”‚\n",
    "â”‚  3. Generate adversarial examples at Îµ âˆˆ {2/255, 4/255, 8/255}  â”‚\n",
    "â”‚  4. Measure robust accuracy under each attack                   â”‚\n",
    "â”‚  5. Analyze per-class vulnerability                             â”‚\n",
    "â”‚  6. Visualize perturbations and decision boundaries             â”‚\n",
    "â”‚  7. Statistical analysis across seeds                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## âš¡ Hardware Requirements\n",
    "\n",
    "- **Recommended:** NVIDIA A100 (40GB) - Full evaluation ~15 minutes\n",
    "- **Minimum:** NVIDIA T4 (16GB) - Full evaluation ~45 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 4: ADVERSARIAL ROBUSTNESS EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Mount Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… Google Colab detected, Drive mounted\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"âœ… Local environment detected\")\n",
    "\n",
    "# Clone/update repository\n",
    "if IN_COLAB:\n",
    "    REPO_PATH = Path('/content/tri-objective-robust-xai-medimg')\n",
    "    if not REPO_PATH.exists():\n",
    "        !git clone https://github.com/viraj1011JAIN/tri-objective-robust-xai-medimg.git {REPO_PATH}\n",
    "        print(\"âœ… Repository cloned\")\n",
    "    else:\n",
    "        os.chdir(REPO_PATH)\n",
    "        !git pull origin main\n",
    "        print(\"âœ… Repository updated\")\n",
    "\n",
    "    os.chdir(REPO_PATH)\n",
    "    sys.path.insert(0, str(REPO_PATH))\n",
    "    PROJECT_ROOT = REPO_PATH\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"ðŸ“ Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: INSTALL DEPENDENCIES\n",
    "# ============================================================================\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q timm albumentations scikit-learn pandas matplotlib seaborn tqdm mlflow\n",
    "!pip install -q plotly kaleido scipy statsmodels\n",
    "print(\"âœ… Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a790d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: IMPORTS\n",
    "# ============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Albumentations for transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score\n",
    ")\n",
    "from scipy import stats\n",
    "\n",
    "# Project imports - Attacks\n",
    "from src.attacks.fgsm import FGSM, FGSMConfig\n",
    "from src.attacks.pgd import PGD, PGDConfig\n",
    "from src.attacks.cw import CarliniWagner, CWConfig\n",
    "\n",
    "# Project imports - Data & Model\n",
    "from src.datasets.isic import ISICDataset\n",
    "from src.models.build import build_model\n",
    "from src.utils.reproducibility import set_global_seed\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    # Enable TF32 for A100\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: CONFIGURATION\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths (Google Drive)\n",
    "    'data_root': Path('/content/drive/MyDrive/data/data/isic_2018'),\n",
    "    'checkpoint_dir': Path('/content/drive/MyDrive/checkpoints/baseline'),\n",
    "    'results_dir': Path('/content/drive/MyDrive/results/phase4'),\n",
    "\n",
    "    # Model\n",
    "    'model_name': 'resnet50',\n",
    "    'num_classes': 7,\n",
    "\n",
    "    # Evaluation settings\n",
    "    'batch_size': 64,\n",
    "    'num_workers': 4,\n",
    "    'image_size': 224,\n",
    "\n",
    "    # Seeds to evaluate\n",
    "    'seeds': [42, 123, 456],\n",
    "\n",
    "    # Attack configurations\n",
    "    'epsilons': [2/255, 4/255, 8/255],\n",
    "    'pgd_steps': 40,\n",
    "    'cw_iterations': 100,\n",
    "\n",
    "    # Class names\n",
    "    'class_names': ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'],\n",
    "}\n",
    "\n",
    "# Class descriptions for visualization labels\n",
    "CLASS_DESCRIPTIONS = {\n",
    "    'AKIEC': 'Actinic Keratoses (pre-cancerous)',\n",
    "    'BCC': 'Basal Cell Carcinoma (cancerous)',\n",
    "    'BKL': 'Benign Keratosis (benign)',\n",
    "    'DF': 'Dermatofibroma (benign)',\n",
    "    'MEL': 'Melanoma (malignant)',\n",
    "    'NV': 'Melanocytic Nevi (moles)',\n",
    "    'VASC': 'Vascular Lesions (blood vessel)',\n",
    "}\n",
    "\n",
    "# ImageNet normalization\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Create output directories\n",
    "CONFIG['results_dir'].mkdir(parents=True, exist_ok=True)\n",
    "(CONFIG['results_dir'] / 'figures').mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“Š Model: {CONFIG['model_name']}\")\n",
    "print(f\"ðŸ“Š Seeds: {CONFIG['seeds']}\")\n",
    "print(f\"ðŸ“Š Epsilons: {[f'{int(e*255)}/255' for e in CONFIG['epsilons']]}\")\n",
    "print(f\"ðŸ“Š Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"ðŸ“ Data: {CONFIG['data_root']}\")\n",
    "print(f\"ðŸ“ Checkpoints: {CONFIG['checkpoint_dir']}\")\n",
    "print(f\"ðŸ“ Results: {CONFIG['results_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: DATA PREPARATION\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load and fix metadata\n",
    "metadata_path = CONFIG['data_root'] / 'metadata.csv'\n",
    "print(f\"ðŸ“„ Loading metadata: {metadata_path}\")\n",
    "\n",
    "df = pd.read_csv(metadata_path)\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "\n",
    "# Fix path separators\n",
    "if 'image_path' in df.columns:\n",
    "    df['image_path'] = df['image_path'].str.replace('\\\\', '/', regex=False)\n",
    "    print(\"   âœ… Fixed path separators\")\n",
    "\n",
    "# Save fixed metadata\n",
    "fixed_path = CONFIG['data_root'] / 'metadata_fixed.csv'\n",
    "df.to_csv(fixed_path, index=False)\n",
    "\n",
    "# Show test split info\n",
    "test_df = df[df['split'] == 'test']\n",
    "print(f\"\\nðŸ“Š Test samples: {len(test_df)}\")\n",
    "print(f\"ðŸ“Š Test class distribution:\")\n",
    "print(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: CREATE TEST DATASET\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"CREATING TEST DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test transforms - NO normalization (attacks need [0,1] range)\n",
    "test_transforms = A.Compose([\n",
    "    A.Resize(CONFIG['image_size'], CONFIG['image_size']),\n",
    "    A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),  # Keep in [0, 1]\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = ISICDataset(\n",
    "    root=str(CONFIG['data_root']),\n",
    "    split='test',\n",
    "    transforms=test_transforms,\n",
    "    csv_path=str(fixed_path),\n",
    "    image_column='image_path',\n",
    "    label_column='label'\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Test samples: {len(test_dataset)}\")\n",
    "print(f\"âœ… Batches: {len(test_loader)}\")\n",
    "print(f\"âœ… Classes: {CONFIG['class_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd9df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_normalizer(device):\n",
    "    \"\"\"Create ImageNet normalization function.\"\"\"\n",
    "    mean = torch.tensor(IMAGENET_MEAN).view(1, 3, 1, 1).to(device)\n",
    "    std = torch.tensor(IMAGENET_STD).view(1, 3, 1, 1).to(device)\n",
    "    \n",
    "    def normalize(x):\n",
    "        return (x - mean) / std\n",
    "    return normalize\n",
    "\n",
    "def evaluate_clean(model, dataloader, device, normalize_fn):\n",
    "    \"\"\"Evaluate model on clean data.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Clean Eval', leave=False):\n",
    "            if len(batch) == 3:\n",
    "                images, labels, _ = batch\n",
    "            else:\n",
    "                images, labels = batch\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Normalize and predict\n",
    "            outputs = model(normalize_fn(images))\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds) * 100,\n",
    "        'balanced_accuracy': balanced_accuracy_score(all_labels, all_preds) * 100,\n",
    "        'f1_macro': f1_score(all_labels, all_preds, average='macro') * 100,\n",
    "        'auroc': roc_auc_score(all_labels, all_probs, multi_class='ovr') * 100,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'probs': all_probs\n",
    "    }\n",
    "\n",
    "def evaluate_attack(model, dataloader, device, normalize_fn, attack_fn, desc='Attack'):\n",
    "    \"\"\"Evaluate model under adversarial attack.\"\"\"\n",
    "    model.eval()\n",
    "    all_clean_preds, all_adv_preds, all_labels = [], [], []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=desc, leave=False):\n",
    "        if len(batch) == 3:\n",
    "            images, labels, _ = batch\n",
    "        else:\n",
    "            images, labels = batch\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Clean predictions\n",
    "        with torch.no_grad():\n",
    "            clean_preds = model(normalize_fn(images)).argmax(dim=1)\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        x_adv = attack_fn(images, labels)\n",
    "        \n",
    "        # Adversarial predictions\n",
    "        with torch.no_grad():\n",
    "            adv_preds = model(normalize_fn(x_adv)).argmax(dim=1)\n",
    "        \n",
    "        all_clean_preds.extend(clean_preds.cpu().numpy())\n",
    "        all_adv_preds.extend(adv_preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_clean_preds = np.array(all_clean_preds)\n",
    "    all_adv_preds = np.array(all_adv_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    clean_acc = accuracy_score(all_labels, all_clean_preds) * 100\n",
    "    robust_acc = accuracy_score(all_labels, all_adv_preds) * 100\n",
    "    \n",
    "    # Per-class robust accuracy\n",
    "    cm = confusion_matrix(all_labels, all_adv_preds)\n",
    "    per_class_acc = (cm.diagonal() / cm.sum(axis=1)) * 100\n",
    "    \n",
    "    return {\n",
    "        'clean_accuracy': clean_acc,\n",
    "        'robust_accuracy': robust_acc,\n",
    "        'accuracy_drop': clean_acc - robust_acc,\n",
    "        'per_class_robust_acc': dict(zip(CONFIG['class_names'], per_class_acc)),\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3148d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: LOAD MODELS AND VERIFY CLEAN ACCURACY\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MODELS & VERIFYING CLEAN ACCURACY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "normalize = get_normalizer(device)\n",
    "models = {}\n",
    "clean_results = {}\n",
    "\n",
    "for seed in CONFIG['seeds']:\n",
    "    print(f\"\\nðŸ“¥ Loading seed {seed}...\")\n",
    "    \n",
    "    # Find checkpoint\n",
    "    checkpoint_path = CONFIG['checkpoint_dir'] / f'seed_{seed}' / 'best.pt'\n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"   âŒ Checkpoint not found: {checkpoint_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Load model\n",
    "    model = build_model(\n",
    "        architecture=CONFIG['model_name'],\n",
    "        num_classes=CONFIG['num_classes'],\n",
    "        pretrained=False\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    models[seed] = model\n",
    "    \n",
    "    # Verify clean accuracy\n",
    "    result = evaluate_clean(model, test_loader, device, normalize)\n",
    "    clean_results[seed] = result\n",
    "    \n",
    "    print(f\"   âœ… Clean Accuracy: {result['accuracy']:.2f}%\")\n",
    "    print(f\"   âœ… AUROC: {result['auroc']:.2f}%\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ðŸ“Š CLEAN ACCURACY SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "accs = [clean_results[s]['accuracy'] for s in clean_results]\n",
    "print(f\"Mean: {np.mean(accs):.2f}% Â± {np.std(accs):.2f}%\")\n",
    "for seed in clean_results:\n",
    "    print(f\"   Seed {seed}: {clean_results[seed]['accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6691f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: RUN ADVERSARIAL EVALUATION - ALL ATTACKS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"ADVERSARIAL EVALUATION - ALL ATTACKS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"â±ï¸  Start time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for seed in CONFIG['seeds']:\n",
    "    if seed not in models:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SEED {seed}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model = models[seed]\n",
    "    seed_results = {'clean': clean_results[seed]}\n",
    "    \n",
    "    # ==================== FGSM ====================\n",
    "    print(\"\\nðŸ”¥ FGSM Attacks:\")\n",
    "    for eps in CONFIG['epsilons']:\n",
    "        eps_str = f\"{int(eps*255)}/255\"\n",
    "        \n",
    "        # Create FGSM attack\n",
    "        fgsm_config = FGSMConfig(\n",
    "            epsilon=eps,\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False\n",
    "        )\n",
    "        fgsm = FGSM(fgsm_config)\n",
    "        \n",
    "        def fgsm_attack_fn(x, y):\n",
    "            return fgsm.generate(model, x, y, loss_fn=nn.CrossEntropyLoss(), normalize=normalize)\n",
    "        \n",
    "        result = evaluate_attack(model, test_loader, device, normalize, fgsm_attack_fn, f\"FGSM Îµ={eps_str}\")\n",
    "        seed_results[f'fgsm_{eps_str}'] = result\n",
    "        print(f\"   Îµ={eps_str}: {result['robust_accuracy']:.2f}% (drop: {result['accuracy_drop']:.2f}%)\")\n",
    "    \n",
    "    # ==================== PGD ====================\n",
    "    print(\"\\nðŸ”¥ PGD Attacks:\")\n",
    "    for eps in CONFIG['epsilons']:\n",
    "        eps_str = f\"{int(eps*255)}/255\"\n",
    "        step_size = eps / 4\n",
    "        \n",
    "        # Create PGD attack\n",
    "        pgd_config = PGDConfig(\n",
    "            epsilon=eps,\n",
    "            num_steps=CONFIG['pgd_steps'],\n",
    "            step_size=step_size,\n",
    "            random_start=True,\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False\n",
    "        )\n",
    "        pgd = PGD(pgd_config)\n",
    "        \n",
    "        def pgd_attack_fn(x, y):\n",
    "            return pgd.generate(model, x, y, loss_fn=nn.CrossEntropyLoss(), normalize=normalize)\n",
    "        \n",
    "        result = evaluate_attack(model, test_loader, device, normalize, pgd_attack_fn, f\"PGD Îµ={eps_str}\")\n",
    "        seed_results[f'pgd_{eps_str}'] = result\n",
    "        print(f\"   Îµ={eps_str}: {result['robust_accuracy']:.2f}% (drop: {result['accuracy_drop']:.2f}%)\")\n",
    "    \n",
    "    # ==================== C&W ====================\n",
    "    print(\"\\nðŸ”¥ Carlini-Wagner Attack:\")\n",
    "    cw_config = CWConfig(\n",
    "        confidence=0.0,\n",
    "        learning_rate=0.01,\n",
    "        max_iterations=CONFIG['cw_iterations'],\n",
    "        binary_search_steps=5,\n",
    "        clip_min=0.0,\n",
    "        clip_max=1.0,\n",
    "        targeted=False\n",
    "    )\n",
    "    cw = CarliniWagner(cw_config)\n",
    "    \n",
    "    def cw_attack_fn(x, y):\n",
    "        return cw.generate(model, x, y, normalize=normalize)\n",
    "    \n",
    "    result = evaluate_attack(model, test_loader, device, normalize, cw_attack_fn, \"C&W L2\")\n",
    "    seed_results['cw'] = result\n",
    "    print(f\"   C&W: {result['robust_accuracy']:.2f}% (drop: {result['accuracy_drop']:.2f}%)\")\n",
    "    \n",
    "    all_results[seed] = seed_results\n",
    "\n",
    "print(f\"\\nâ±ï¸  End time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(\"âœ… Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: ADVANCED RESULTS TABLE WITH STYLED OUTPUT\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ“Š COMPREHENSIVE ADVERSARIAL EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===================== BUILD COMPREHENSIVE RESULTS TABLE =====================\n",
    "rows = []\n",
    "attacks = ['clean'] + [f'fgsm_{int(e*255)}/255' for e in CONFIG['epsilons']] + \\\n",
    "          [f'pgd_{int(e*255)}/255' for e in CONFIG['epsilons']] + ['cw']\n",
    "\n",
    "for attack in attacks:\n",
    "    if attack == 'clean':\n",
    "        accs = [all_results[s]['clean']['accuracy'] for s in all_results]\n",
    "        attack_name = 'ðŸŸ¢ Clean (No Attack)'\n",
    "        attack_type = 'Baseline'\n",
    "        severity = 'â€”'\n",
    "    elif attack == 'cw':\n",
    "        accs = [all_results[s]['cw']['robust_accuracy'] for s in all_results]\n",
    "        attack_name = 'ðŸ”´ Carlini-Wagner L2'\n",
    "        attack_type = 'Optimization'\n",
    "        severity = 'Maximum'\n",
    "    elif 'fgsm' in attack:\n",
    "        accs = [all_results[s][attack]['robust_accuracy'] for s in all_results]\n",
    "        eps = int(attack.split('_')[1].split('/')[0])\n",
    "        attack_name = f'ðŸ”µ FGSM (Îµ={eps}/255)'\n",
    "        attack_type = 'Gradient (1-step)'\n",
    "        severity = 'Weak' if eps == 2 else 'Medium' if eps == 4 else 'Strong'\n",
    "    else:  # pgd\n",
    "        accs = [all_results[s][attack]['robust_accuracy'] for s in all_results]\n",
    "        eps = int(attack.split('_')[1].split('/')[0])\n",
    "        attack_name = f'ðŸŸ  PGD-40 (Îµ={eps}/255)'\n",
    "        attack_type = 'Iterative (40-step)'\n",
    "        severity = 'Weak' if eps == 2 else 'Medium' if eps == 4 else 'Strong'\n",
    "    \n",
    "    mean_acc = np.mean(accs)\n",
    "    clean_acc = np.mean([all_results[s]['clean']['accuracy'] for s in all_results])\n",
    "    drop = clean_acc - mean_acc if attack != 'clean' else 0\n",
    "    \n",
    "    rows.append({\n",
    "        'Attack': attack_name,\n",
    "        'Type': attack_type,\n",
    "        'Severity': severity,\n",
    "        'Mean Acc (%)': mean_acc,\n",
    "        'Std (%)': np.std(accs),\n",
    "        'Drop (%)': drop,\n",
    "        'Seed 42': accs[0] if len(accs) > 0 else np.nan,\n",
    "        'Seed 123': accs[1] if len(accs) > 1 else np.nan,\n",
    "        'Seed 456': accs[2] if len(accs) > 2 else np.nan,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "\n",
    "# ===================== STYLED TABLE DISPLAY =====================\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def color_accuracy(val):\n",
    "    \"\"\"Color code accuracy values.\"\"\"\n",
    "    if pd.isna(val): return ''\n",
    "    if val >= 70: return 'background-color: #a8e6cf; color: black'\n",
    "    elif val >= 40: return 'background-color: #ffd3b6; color: black'\n",
    "    else: return 'background-color: #ffaaa5; color: black'\n",
    "\n",
    "def color_drop(val):\n",
    "    \"\"\"Color code accuracy drop.\"\"\"\n",
    "    if pd.isna(val) or val == 0: return ''\n",
    "    if val <= 20: return 'background-color: #dcedc1; color: black'\n",
    "    elif val <= 50: return 'background-color: #ffeead; color: black'\n",
    "    else: return 'background-color: #ff6f69; color: white'\n",
    "\n",
    "# Format for display\n",
    "display_df = results_df.copy()\n",
    "display_df['Mean Acc (%)'] = display_df['Mean Acc (%)'].apply(lambda x: f'{x:.2f}')\n",
    "display_df['Std (%)'] = display_df['Std (%)'].apply(lambda x: f'{x:.2f}')\n",
    "display_df['Drop (%)'] = display_df['Drop (%)'].apply(lambda x: f'{x:.2f}' if x > 0 else 'â€”')\n",
    "display_df['Seed 42'] = display_df['Seed 42'].apply(lambda x: f'{x:.2f}')\n",
    "display_df['Seed 123'] = display_df['Seed 123'].apply(lambda x: f'{x:.2f}')\n",
    "display_df['Seed 456'] = display_df['Seed 456'].apply(lambda x: f'{x:.2f}')\n",
    "\n",
    "print(\"\\nðŸ“‹ DETAILED RESULTS TABLE:\")\n",
    "print(display_df.to_string(index=False))\n",
    "\n",
    "# ===================== EXECUTIVE SUMMARY STATISTICS =====================\n",
    "clean_mean = np.mean([all_results[s]['clean']['accuracy'] for s in all_results])\n",
    "fgsm8_mean = np.mean([all_results[s]['fgsm_8/255']['robust_accuracy'] for s in all_results])\n",
    "pgd8_mean = np.mean([all_results[s]['pgd_8/255']['robust_accuracy'] for s in all_results])\n",
    "cw_mean = np.mean([all_results[s]['cw']['robust_accuracy'] for s in all_results])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ“ˆ EXECUTIVE SUMMARY - KEY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "summary_table = f\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  BASELINE MODEL VULNERABILITY ASSESSMENT                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Clean Accuracy (Baseline):     {clean_mean:>6.2f}%                         â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  FGSM Îµ=8/255:                  {fgsm8_mean:>6.2f}%  (â†“ {clean_mean-fgsm8_mean:>5.2f}% drop)         â”‚\n",
    "â”‚  PGD-40 Îµ=8/255:                {pgd8_mean:>6.2f}%  (â†“ {clean_mean-pgd8_mean:>5.2f}% drop)         â”‚\n",
    "â”‚  Carlini-Wagner L2:             {cw_mean:>6.2f}%  (â†“ {clean_mean-cw_mean:>5.2f}% drop)         â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\n",
    "â”‚  Average Robustness Degradation: {np.mean([clean_mean-fgsm8_mean, clean_mean-pgd8_mean, clean_mean-cw_mean]):>5.2f}% under strong attacks  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "print(summary_table)\n",
    "\n",
    "# Save to CSV with full precision\n",
    "results_df.to_csv(CONFIG['results_dir'] / 'adversarial_results.csv', index=False, float_format='%.4f')\n",
    "print(f\"âœ… Results saved to: {CONFIG['results_dir'] / 'adversarial_results.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: ADVANCED VISUALIZATION - PUBLICATION-QUALITY ROBUSTNESS ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ“Š GENERATING ADVANCED VISUALIZATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===================== STYLE CONFIGURATION =====================\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.labelweight': 'bold',\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'legend.framealpha': 0.9,\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "})\n",
    "\n",
    "# Custom color palette - professional & accessible\n",
    "COLORS = {\n",
    "    'clean': '#2E8B57',      # Sea green\n",
    "    'fgsm': '#4169E1',       # Royal blue\n",
    "    'pgd': '#DC143C',        # Crimson\n",
    "    'cw': '#9400D3',         # Dark violet\n",
    "    'seeds': ['#FF6B6B', '#4ECDC4', '#45B7D1'],  # Coral, Teal, Sky blue\n",
    "    'gradient': ['#00C853', '#FFD600', '#FF6D00', '#D50000']  # Green to red\n",
    "}\n",
    "\n",
    "# ===================== FIGURE 1: COMPREHENSIVE ROBUSTNESS DASHBOARD =====================\n",
    "fig = plt.figure(figsize=(18, 14))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3, height_ratios=[1.2, 1, 1])\n",
    "\n",
    "# ------ Panel A: Robustness Degradation Curves (FGSM vs PGD) ------\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "\n",
    "eps_vals = [0] + [e*255 for e in CONFIG['epsilons']]\n",
    "markers = ['o', 's', '^']\n",
    "linestyles = ['-', '--', ':']\n",
    "\n",
    "# Plot for each seed\n",
    "for i, seed in enumerate(CONFIG['seeds']):\n",
    "    if seed not in all_results:\n",
    "        continue\n",
    "    \n",
    "    # FGSM curve\n",
    "    fgsm_accs = [all_results[seed]['clean']['accuracy']]\n",
    "    for eps in CONFIG['epsilons']:\n",
    "        fgsm_accs.append(all_results[seed][f'fgsm_{int(eps*255)}/255']['robust_accuracy'])\n",
    "    ax1.plot(eps_vals, fgsm_accs, marker=markers[i], linestyle='-', \n",
    "             color=COLORS['fgsm'], linewidth=2.5, markersize=10, \n",
    "             label=f'FGSM (Seed {seed})', alpha=0.7 + 0.1*i)\n",
    "    \n",
    "    # PGD curve\n",
    "    pgd_accs = [all_results[seed]['clean']['accuracy']]\n",
    "    for eps in CONFIG['epsilons']:\n",
    "        pgd_accs.append(all_results[seed][f'pgd_{int(eps*255)}/255']['robust_accuracy'])\n",
    "    ax1.plot(eps_vals, pgd_accs, marker=markers[i], linestyle='--',\n",
    "             color=COLORS['pgd'], linewidth=2.5, markersize=10,\n",
    "             label=f'PGD-40 (Seed {seed})', alpha=0.7 + 0.1*i)\n",
    "\n",
    "# Add mean curves with confidence band\n",
    "fgsm_means = [np.mean([all_results[s]['clean']['accuracy'] for s in all_results])]\n",
    "fgsm_stds = [np.std([all_results[s]['clean']['accuracy'] for s in all_results])]\n",
    "pgd_means = [np.mean([all_results[s]['clean']['accuracy'] for s in all_results])]\n",
    "pgd_stds = [np.std([all_results[s]['clean']['accuracy'] for s in all_results])]\n",
    "\n",
    "for eps in CONFIG['epsilons']:\n",
    "    fgsm_vals = [all_results[s][f'fgsm_{int(eps*255)}/255']['robust_accuracy'] for s in all_results]\n",
    "    fgsm_means.append(np.mean(fgsm_vals))\n",
    "    fgsm_stds.append(np.std(fgsm_vals))\n",
    "    pgd_vals = [all_results[s][f'pgd_{int(eps*255)}/255']['robust_accuracy'] for s in all_results]\n",
    "    pgd_means.append(np.mean(pgd_vals))\n",
    "    pgd_stds.append(np.std(pgd_vals))\n",
    "\n",
    "# Shade confidence bands\n",
    "ax1.fill_between(eps_vals, np.array(fgsm_means)-np.array(fgsm_stds), \n",
    "                 np.array(fgsm_means)+np.array(fgsm_stds), alpha=0.15, color=COLORS['fgsm'])\n",
    "ax1.fill_between(eps_vals, np.array(pgd_means)-np.array(pgd_stds),\n",
    "                 np.array(pgd_means)+np.array(pgd_stds), alpha=0.15, color=COLORS['pgd'])\n",
    "\n",
    "# Styling\n",
    "ax1.set_xlabel('Perturbation Budget Îµ (Ã—255)', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('A) Adversarial Robustness Degradation: FGSM vs PGD-40', fontsize=15, fontweight='bold', pad=15)\n",
    "ax1.set_xticks(eps_vals)\n",
    "ax1.set_xticklabels(['0\\n(Clean)', '2', '4', '8'])\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.axhline(y=50, color='gray', linestyle=':', alpha=0.5, label='Random Guess (50%)')\n",
    "ax1.legend(loc='upper right', ncol=2, frameon=True, fancybox=True, shadow=True)\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add annotations for key drops\n",
    "clean_mean = np.mean([all_results[s]['clean']['accuracy'] for s in all_results])\n",
    "pgd8_mean = np.mean([all_results[s]['pgd_8/255']['robust_accuracy'] for s in all_results])\n",
    "ax1.annotate(f'â†“{clean_mean - pgd8_mean:.1f}%', xy=(8, pgd8_mean), xytext=(8.5, pgd8_mean + 15),\n",
    "             fontsize=12, fontweight='bold', color=COLORS['pgd'],\n",
    "             arrowprops=dict(arrowstyle='->', color=COLORS['pgd'], lw=2))\n",
    "\n",
    "# ------ Panel B: Attack Severity Radar Chart ------\n",
    "ax2 = fig.add_subplot(gs[0, 2], projection='polar')\n",
    "\n",
    "# Radar data for strongest attacks (Îµ=8/255)\n",
    "categories = ['FGSM\\nÎµ=2/255', 'FGSM\\nÎµ=8/255', 'PGD\\nÎµ=2/255', 'PGD\\nÎµ=8/255', 'C&W\\nL2']\n",
    "attack_keys = ['fgsm_2/255', 'fgsm_8/255', 'pgd_2/255', 'pgd_8/255', 'cw']\n",
    "\n",
    "values = []\n",
    "for key in attack_keys:\n",
    "    accs = [all_results[s][key]['robust_accuracy'] for s in all_results]\n",
    "    values.append(np.mean(accs))\n",
    "\n",
    "# Normalize to 0-100 and invert (higher = more vulnerable)\n",
    "vulnerability = [100 - v for v in values]\n",
    "vulnerability.append(vulnerability[0])  # Close the polygon\n",
    "\n",
    "angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()\n",
    "angles.append(angles[0])\n",
    "\n",
    "ax2.plot(angles, vulnerability, 'o-', linewidth=2.5, color='#E74C3C', markersize=8)\n",
    "ax2.fill(angles, vulnerability, alpha=0.25, color='#E74C3C')\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels(categories, size=9)\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.set_title('B) Vulnerability Profile\\n(Higher = More Vulnerable)', fontsize=13, fontweight='bold', pad=20)\n",
    "\n",
    "# ------ Panel C: Per-Seed Comparison ------\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "seed_data = []\n",
    "for seed in CONFIG['seeds']:\n",
    "    seed_data.append({\n",
    "        'Seed': str(seed),\n",
    "        'Clean': all_results[seed]['clean']['accuracy'],\n",
    "        'FGSM-8': all_results[seed]['fgsm_8/255']['robust_accuracy'],\n",
    "        'PGD-8': all_results[seed]['pgd_8/255']['robust_accuracy'],\n",
    "        'C&W': all_results[seed]['cw']['robust_accuracy']\n",
    "    })\n",
    "\n",
    "seed_df = pd.DataFrame(seed_data)\n",
    "x = np.arange(len(CONFIG['seeds']))\n",
    "width = 0.2\n",
    "\n",
    "bars1 = ax3.bar(x - 1.5*width, seed_df['Clean'], width, label='Clean', color=COLORS['clean'], edgecolor='white', linewidth=1.5)\n",
    "bars2 = ax3.bar(x - 0.5*width, seed_df['FGSM-8'], width, label='FGSM Îµ=8', color=COLORS['fgsm'], edgecolor='white', linewidth=1.5)\n",
    "bars3 = ax3.bar(x + 0.5*width, seed_df['PGD-8'], width, label='PGD Îµ=8', color=COLORS['pgd'], edgecolor='white', linewidth=1.5)\n",
    "bars4 = ax3.bar(x + 1.5*width, seed_df['C&W'], width, label='C&W', color=COLORS['cw'], edgecolor='white', linewidth=1.5)\n",
    "\n",
    "ax3.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "ax3.set_xlabel('Random Seed', fontweight='bold')\n",
    "ax3.set_title('C) Cross-Seed Consistency', fontsize=13, fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels([f'Seed {s}' for s in CONFIG['seeds']])\n",
    "ax3.legend(loc='upper right', fontsize=9)\n",
    "ax3.set_ylim(0, 100)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2, bars3, bars4]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.annotate(f'{height:.0f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                     xytext=(0, 3), textcoords='offset points', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "# ------ Panel D: Accuracy Drop Waterfall ------\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "attacks_order = ['FGSM\\nÎµ=2', 'FGSM\\nÎµ=4', 'FGSM\\nÎµ=8', 'PGD\\nÎµ=2', 'PGD\\nÎµ=4', 'PGD\\nÎµ=8', 'C&W']\n",
    "attack_keys_order = ['fgsm_2/255', 'fgsm_4/255', 'fgsm_8/255', 'pgd_2/255', 'pgd_4/255', 'pgd_8/255', 'cw']\n",
    "\n",
    "drops = []\n",
    "for key in attack_keys_order:\n",
    "    accs = [all_results[s][key]['robust_accuracy'] for s in all_results]\n",
    "    drop = clean_mean - np.mean(accs)\n",
    "    drops.append(drop)\n",
    "\n",
    "colors_drop = [COLORS['fgsm']]*3 + [COLORS['pgd']]*3 + [COLORS['cw']]\n",
    "bars = ax4.barh(attacks_order, drops, color=colors_drop, edgecolor='white', linewidth=1.5, alpha=0.85)\n",
    "\n",
    "ax4.set_xlabel('Accuracy Drop (%)', fontweight='bold')\n",
    "ax4.set_title('D) Robustness Degradation by Attack', fontsize=13, fontweight='bold')\n",
    "ax4.axvline(x=np.mean(drops), color='red', linestyle='--', linewidth=2, label=f'Mean Drop: {np.mean(drops):.1f}%')\n",
    "ax4.legend(loc='lower right')\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for bar, drop in zip(bars, drops):\n",
    "    ax4.annotate(f'{drop:.1f}%', xy=(bar.get_width(), bar.get_y() + bar.get_height()/2),\n",
    "                 xytext=(5, 0), textcoords='offset points', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ------ Panel E: Statistical Significance ------\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "\n",
    "# Calculate p-values (t-test: clean vs each attack)\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "p_values = []\n",
    "attack_labels = ['FGSM-8', 'PGD-8', 'C&W']\n",
    "for key in ['fgsm_8/255', 'pgd_8/255', 'cw']:\n",
    "    clean_accs = [all_results[s]['clean']['accuracy'] for s in all_results]\n",
    "    attack_accs = [all_results[s][key]['robust_accuracy'] for s in all_results]\n",
    "    _, p = scipy_stats.ttest_rel(clean_accs, attack_accs)\n",
    "    p_values.append(p)\n",
    "\n",
    "colors_p = ['green' if p < 0.05 else 'orange' for p in p_values]\n",
    "bars = ax5.barh(attack_labels, [-np.log10(p) for p in p_values], color=colors_p, edgecolor='white', linewidth=1.5)\n",
    "ax5.axvline(x=-np.log10(0.05), color='red', linestyle='--', linewidth=2, label='p=0.05 threshold')\n",
    "ax5.set_xlabel('-logâ‚â‚€(p-value)', fontweight='bold')\n",
    "ax5.set_title('E) Statistical Significance\\n(Paired t-test vs Clean)', fontsize=13, fontweight='bold')\n",
    "ax5.legend(loc='lower right')\n",
    "\n",
    "for bar, p in zip(bars, p_values):\n",
    "    ax5.annotate(f'p={p:.4f}', xy=(bar.get_width(), bar.get_y() + bar.get_height()/2),\n",
    "                 xytext=(5, 0), textcoords='offset points', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ------ Panel F: Key Findings Summary ------\n",
    "ax6 = fig.add_subplot(gs[2, :])\n",
    "ax6.axis('off')\n",
    "\n",
    "findings_text = f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                                    ðŸ”¬ KEY FINDINGS: ADVERSARIAL ROBUSTNESS ANALYSIS                                  â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                                                                      â•‘\n",
    "â•‘  ðŸ“Š BASELINE MODEL PERFORMANCE                           â”‚  âš ï¸  VULNERABILITY ASSESSMENT                            â•‘\n",
    "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â•‘\n",
    "â•‘  â€¢ Clean Accuracy: {clean_mean:.2f}% (Mean Â± {np.std([all_results[s]['clean']['accuracy'] for s in all_results]):.2f}%)         â”‚  â€¢ Single-step FGSM reduces accuracy by {clean_mean - fgsm8_mean:.1f}% at Îµ=8/255      â•‘\n",
    "â•‘  â€¢ Model: ResNet-50 (ImageNet pretrained)                â”‚  â€¢ Iterative PGD-40 causes {clean_mean - pgd8_mean:.1f}% drop at Îµ=8/255          â•‘\n",
    "â•‘  â€¢ Dataset: ISIC 2018 (7 dermoscopy classes)             â”‚  â€¢ Optimization-based C&W achieves {clean_mean - cw_mean:.1f}% degradation    â•‘\n",
    "â•‘  â€¢ Seeds evaluated: {', '.join(map(str, CONFIG['seeds']))}                        â”‚  â€¢ Maximum observed drop: {max([clean_mean - fgsm8_mean, clean_mean - pgd8_mean, clean_mean - cw_mean]):.1f}%                           â•‘\n",
    "â•‘                                                          â”‚                                                           â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                                                                      â•‘\n",
    "â•‘  ðŸ’¡ IMPLICATIONS FOR MEDICAL IMAGING                                                                                 â•‘\n",
    "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘\n",
    "â•‘  1. Standard CNNs are HIGHLY VULNERABLE to adversarial perturbations - poses serious risk in clinical deployment     â•‘\n",
    "â•‘  2. Even small perturbations (Îµ=2/255) cause measurable accuracy degradation - imperceptible to human eye            â•‘\n",
    "â•‘  3. Cross-seed consistency shows vulnerability is SYSTEMATIC, not random - fundamental model weakness                â•‘\n",
    "â•‘  4. ADVERSARIAL TRAINING (Phase 5) is ESSENTIAL before clinical deployment                                          â•‘\n",
    "â•‘                                                                                                                      â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "\n",
    "ax6.text(0.5, 0.5, findings_text, transform=ax6.transAxes, fontsize=10, \n",
    "         fontfamily='monospace', verticalalignment='center', horizontalalignment='center',\n",
    "         bbox=dict(boxstyle='round,pad=0.5', facecolor='#f8f9fa', edgecolor='#dee2e6', linewidth=2))\n",
    "\n",
    "plt.suptitle('Phase 4: Adversarial Robustness Evaluation â€” ResNet-50 Baseline on ISIC 2018', \n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "plt.savefig(CONFIG['results_dir'] / 'figures' / 'robustness_dashboard.png', dpi=300, bbox_inches='tight', \n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.show()\n",
    "print(\"âœ… Saved: robustness_dashboard.png (300 DPI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: ADVANCED PER-CLASS VULNERABILITY ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ“Š PER-CLASS VULNERABILITY HEATMAP & ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===================== COMPREHENSIVE HEATMAP DATA =====================\n",
    "attacks_to_show = [\n",
    "    ('clean', 'Clean\\n(No Attack)'),\n",
    "    ('fgsm_2/255', 'FGSM\\nÎµ=2/255'),\n",
    "    ('fgsm_4/255', 'FGSM\\nÎµ=4/255'),\n",
    "    ('fgsm_8/255', 'FGSM\\nÎµ=8/255'),\n",
    "    ('pgd_2/255', 'PGD\\nÎµ=2/255'),\n",
    "    ('pgd_4/255', 'PGD\\nÎµ=4/255'),\n",
    "    ('pgd_8/255', 'PGD\\nÎµ=8/255'),\n",
    "    ('cw', 'C&W\\nL2'),\n",
    "]\n",
    "\n",
    "# Build comprehensive heatmap matrix\n",
    "heatmap_matrix = []\n",
    "for attack_key, attack_label in attacks_to_show:\n",
    "    row = []\n",
    "    for cls in CONFIG['class_names']:\n",
    "        if attack_key == 'clean':\n",
    "            # For clean, use predictions vs labels to compute per-class accuracy\n",
    "            all_preds = np.concatenate([all_results[s]['clean']['predictions'] for s in all_results])\n",
    "            all_labels = np.concatenate([all_results[s]['clean']['labels'] for s in all_results])\n",
    "            cls_idx = CONFIG['class_names'].index(cls)\n",
    "            cls_mask = all_labels == cls_idx\n",
    "            if cls_mask.sum() > 0:\n",
    "                cls_acc = (all_preds[cls_mask] == all_labels[cls_mask]).mean() * 100\n",
    "            else:\n",
    "                cls_acc = 0\n",
    "            row.append(cls_acc)\n",
    "        else:\n",
    "            accs = [all_results[s][attack_key]['per_class_robust_acc'][cls] for s in all_results]\n",
    "            row.append(np.mean(accs))\n",
    "    heatmap_matrix.append(row)\n",
    "\n",
    "heatmap_array = np.array(heatmap_matrix)\n",
    "\n",
    "# ===================== FIGURE: MULTI-PANEL VULNERABILITY ANALYSIS =====================\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.25, height_ratios=[1.3, 1])\n",
    "\n",
    "# ------ Panel A: Full Vulnerability Heatmap ------\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "\n",
    "# Create custom colormap (green = robust, red = vulnerable)\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors_cmap = ['#D32F2F', '#FF5722', '#FF9800', '#FFC107', '#CDDC39', '#8BC34A', '#4CAF50', '#2E7D32']\n",
    "cmap = LinearSegmentedColormap.from_list('vulnerability', colors_cmap)\n",
    "\n",
    "im = ax1.imshow(heatmap_array, cmap=cmap, aspect='auto', vmin=0, vmax=100)\n",
    "\n",
    "# Styling\n",
    "ax1.set_xticks(range(len(CONFIG['class_names'])))\n",
    "ax1.set_xticklabels([f'{name}\\n({desc.split(\"(\")[0].strip()[:15]})' \n",
    "                      for name, desc in zip(CONFIG['class_names'], \n",
    "                      [CLASS_DESCRIPTIONS.get(c, c) if 'CLASS_DESCRIPTIONS' in dir() else c for c in CONFIG['class_names']])],\n",
    "                    fontsize=10, fontweight='bold')\n",
    "ax1.set_yticks(range(len(attacks_to_show)))\n",
    "ax1.set_yticklabels([label for _, label in attacks_to_show], fontsize=10)\n",
    "\n",
    "# Add text annotations with adaptive coloring\n",
    "for i in range(len(attacks_to_show)):\n",
    "    for j in range(len(CONFIG['class_names'])):\n",
    "        val = heatmap_array[i, j]\n",
    "        text_color = 'white' if val < 40 else 'black'\n",
    "        ax1.text(j, i, f'{val:.1f}%', ha='center', va='center', \n",
    "                 color=text_color, fontsize=9, fontweight='bold')\n",
    "\n",
    "ax1.set_title('A) Per-Class Robustness Under All Attack Conditions\\n(Mean across 3 seeds)', \n",
    "              fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "# Colorbar with labels\n",
    "cbar = plt.colorbar(im, ax=ax1, shrink=0.8, pad=0.02)\n",
    "cbar.set_label('Accuracy (%)', fontsize=11, fontweight='bold')\n",
    "cbar.ax.set_yticks([0, 25, 50, 75, 100])\n",
    "cbar.ax.set_yticklabels(['0%\\n(Failed)', '25%', '50%', '75%', '100%\\n(Robust)'])\n",
    "\n",
    "# Add class vulnerability ranking annotation\n",
    "most_vulnerable_idx = np.argmin(heatmap_array[-2, :])  # PGD Îµ=8/255\n",
    "most_robust_idx = np.argmax(heatmap_array[-2, :])\n",
    "ax1.annotate('Most Vulnerable', xy=(most_vulnerable_idx, 6), xytext=(most_vulnerable_idx, 8),\n",
    "             fontsize=10, color='red', fontweight='bold', ha='center',\n",
    "             arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "# ------ Panel B: Class Vulnerability Ranking ------\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "# Calculate mean robustness across all attacks for each class\n",
    "class_robustness = []\n",
    "for j, cls in enumerate(CONFIG['class_names']):\n",
    "    mean_rob = np.mean(heatmap_array[1:, j])  # Exclude clean\n",
    "    class_robustness.append((cls, mean_rob))\n",
    "\n",
    "class_robustness.sort(key=lambda x: x[1])  # Sort by robustness (ascending = most vulnerable first)\n",
    "\n",
    "cls_names = [c[0] for c in class_robustness]\n",
    "cls_values = [c[1] for c in class_robustness]\n",
    "\n",
    "# Color bars by vulnerability\n",
    "colors_bars = [plt.cm.RdYlGn(v/100) for v in cls_values]\n",
    "bars = ax2.barh(cls_names, cls_values, color=colors_bars, edgecolor='white', linewidth=1.5)\n",
    "\n",
    "ax2.set_xlabel('Mean Robust Accuracy (%)', fontweight='bold')\n",
    "ax2.set_title('B) Class Vulnerability Ranking\\n(Lower = More Vulnerable)', fontsize=13, fontweight='bold')\n",
    "ax2.axvline(x=np.mean(cls_values), color='navy', linestyle='--', linewidth=2, label=f'Mean: {np.mean(cls_values):.1f}%')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.set_xlim(0, 100)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, val in zip(bars, cls_values):\n",
    "    ax2.annotate(f'{val:.1f}%', xy=(val, bar.get_y() + bar.get_height()/2),\n",
    "                 xytext=(5, 0), textcoords='offset points', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ------ Panel C: Attack Strength Impact by Class ------\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "# Line plot: accuracy degradation by epsilon for each class\n",
    "x_positions = [0, 2, 4, 8]  # Epsilon values * 255\n",
    "colors_class = plt.cm.tab10(np.linspace(0, 1, len(CONFIG['class_names'])))\n",
    "\n",
    "for j, cls in enumerate(CONFIG['class_names']):\n",
    "    class_accs = [heatmap_array[0, j]]  # Clean\n",
    "    class_accs.append(heatmap_array[4, j])  # PGD Îµ=2/255\n",
    "    class_accs.append(heatmap_array[5, j])  # PGD Îµ=4/255\n",
    "    class_accs.append(heatmap_array[6, j])  # PGD Îµ=8/255\n",
    "    ax3.plot(x_positions, class_accs, 'o-', color=colors_class[j], \n",
    "             label=cls, linewidth=2, markersize=8, alpha=0.8)\n",
    "\n",
    "ax3.set_xlabel('Perturbation Îµ (Ã—255)', fontweight='bold')\n",
    "ax3.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "ax3.set_title('C) PGD Attack Impact by Class\\n(Accuracy vs Epsilon)', fontsize=13, fontweight='bold')\n",
    "ax3.set_xticks(x_positions)\n",
    "ax3.set_xticklabels(['0\\n(Clean)', '2', '4', '8'])\n",
    "ax3.legend(loc='upper right', ncol=2, fontsize=9)\n",
    "ax3.set_ylim(0, 100)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Per-Class Adversarial Vulnerability Analysis â€” ISIC 2018 Dermoscopy', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.savefig(CONFIG['results_dir'] / 'figures' / 'class_vulnerability_analysis.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.show()\n",
    "print(\"âœ… Saved: class_vulnerability_analysis.png (300 DPI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fadd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: INTERACTIVE PLOTLY VISUALIZATIONS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ“Š GENERATING INTERACTIVE PLOTLY VISUALIZATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ===================== FIGURE 1: INTERACTIVE ATTACK COMPARISON =====================\n",
    "# Prepare data for Plotly\n",
    "attack_data = []\n",
    "attack_keys = ['clean', 'fgsm_2/255', 'fgsm_4/255', 'fgsm_8/255', \n",
    "               'pgd_2/255', 'pgd_4/255', 'pgd_8/255', 'cw']\n",
    "attack_labels = ['Clean', 'FGSM Îµ=2/255', 'FGSM Îµ=4/255', 'FGSM Îµ=8/255',\n",
    "                 'PGD Îµ=2/255', 'PGD Îµ=4/255', 'PGD Îµ=8/255', 'C&W L2']\n",
    "attack_types = ['Baseline', 'FGSM', 'FGSM', 'FGSM', 'PGD', 'PGD', 'PGD', 'C&W']\n",
    "attack_colors = ['#2E8B57', '#4169E1', '#4169E1', '#4169E1', '#DC143C', '#DC143C', '#DC143C', '#9400D3']\n",
    "\n",
    "for seed in all_results:\n",
    "    for key, label, atype, color in zip(attack_keys, attack_labels, attack_types, attack_colors):\n",
    "        if key == 'clean':\n",
    "            acc = all_results[seed]['clean']['accuracy']\n",
    "        else:\n",
    "            acc = all_results[seed][key]['robust_accuracy']\n",
    "        attack_data.append({\n",
    "            'Seed': f'Seed {seed}',\n",
    "            'Attack': label,\n",
    "            'Attack Type': atype,\n",
    "            'Accuracy': acc,\n",
    "            'Color': color\n",
    "        })\n",
    "\n",
    "attack_df = pd.DataFrame(attack_data)\n",
    "\n",
    "# Create grouped bar chart\n",
    "fig1 = px.bar(\n",
    "    attack_df, \n",
    "    x='Attack', \n",
    "    y='Accuracy', \n",
    "    color='Seed',\n",
    "    barmode='group',\n",
    "    title='<b>Interactive Attack Comparison: Baseline Model Robustness</b><br><sup>Click legend to toggle seeds | Hover for details</sup>',\n",
    "    labels={'Accuracy': 'Accuracy (%)', 'Attack': 'Attack Configuration'},\n",
    "    color_discrete_sequence=['#FF6B6B', '#4ECDC4', '#45B7D1'],\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig1.update_layout(\n",
    "    font=dict(family='Arial', size=12),\n",
    "    title_font_size=18,\n",
    "    title_x=0.5,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
    "    xaxis_tickangle=-45,\n",
    "    yaxis=dict(range=[0, 100], title_font_size=14),\n",
    "    xaxis=dict(title_font_size=14),\n",
    "    hovermode='x unified',\n",
    "    bargap=0.15,\n",
    "    bargroupgap=0.1\n",
    ")\n",
    "\n",
    "# Add mean line\n",
    "mean_clean = attack_df[attack_df['Attack'] == 'Clean']['Accuracy'].mean()\n",
    "fig1.add_hline(y=mean_clean, line_dash='dash', line_color='gray', \n",
    "               annotation_text=f'Clean Baseline: {mean_clean:.1f}%', annotation_position='top right')\n",
    "\n",
    "fig1.show()\n",
    "fig1.write_html(str(CONFIG['results_dir'] / 'figures' / 'attack_comparison_interactive.html'))\n",
    "print(\"âœ… Saved: attack_comparison_interactive.html\")\n",
    "\n",
    "# ===================== FIGURE 2: ROBUSTNESS DEGRADATION SURFACE =====================\n",
    "# Create 3D surface for PGD attack analysis\n",
    "fig2 = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{'type': 'scatter3d'}, {'type': 'heatmap'}]],\n",
    "    subplot_titles=('3D Robustness Surface', 'Attack Intensity Heatmap'),\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# Prepare 3D data\n",
    "epsilons = [0, 2, 4, 8]\n",
    "seeds_list = list(all_results.keys())\n",
    "\n",
    "X, Y = np.meshgrid(epsilons, range(len(seeds_list)))\n",
    "Z_fgsm = np.zeros_like(X, dtype=float)\n",
    "Z_pgd = np.zeros_like(X, dtype=float)\n",
    "\n",
    "for i, seed in enumerate(seeds_list):\n",
    "    Z_fgsm[i, 0] = all_results[seed]['clean']['accuracy']\n",
    "    Z_pgd[i, 0] = all_results[seed]['clean']['accuracy']\n",
    "    for j, eps in enumerate([2, 4, 8]):\n",
    "        Z_fgsm[i, j+1] = all_results[seed][f'fgsm_{eps}/255']['robust_accuracy']\n",
    "        Z_pgd[i, j+1] = all_results[seed][f'pgd_{eps}/255']['robust_accuracy']\n",
    "\n",
    "# Add 3D surface for PGD\n",
    "fig2.add_trace(\n",
    "    go.Surface(\n",
    "        x=X, y=Y, z=Z_pgd,\n",
    "        colorscale='RdYlGn',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title='Accuracy (%)', x=0.45),\n",
    "        name='PGD Robustness',\n",
    "        hovertemplate='Îµ=%{x}/255<br>Seed=%{y}<br>Accuracy=%{z:.1f}%<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add heatmap for both attacks\n",
    "combined_heatmap = np.vstack([Z_fgsm, Z_pgd])\n",
    "heatmap_labels = [f'FGSM-S{s}' for s in seeds_list] + [f'PGD-S{s}' for s in seeds_list]\n",
    "\n",
    "fig2.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=combined_heatmap,\n",
    "        x=['Îµ=0', 'Îµ=2/255', 'Îµ=4/255', 'Îµ=8/255'],\n",
    "        y=heatmap_labels,\n",
    "        colorscale='RdYlGn',\n",
    "        showscale=False,\n",
    "        text=np.round(combined_heatmap, 1),\n",
    "        texttemplate='%{text}%',\n",
    "        textfont=dict(size=10, color='black'),\n",
    "        hovertemplate='%{y}<br>%{x}<br>Accuracy: %{z:.1f}%<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig2.update_layout(\n",
    "    title='<b>3D Robustness Analysis: PGD Attack Surface</b><br><sup>Drag to rotate | Scroll to zoom</sup>',\n",
    "    title_font_size=16,\n",
    "    title_x=0.5,\n",
    "    font=dict(family='Arial', size=11),\n",
    "    template='plotly_white',\n",
    "    scene=dict(\n",
    "        xaxis_title='Epsilon (Ã—255)',\n",
    "        yaxis_title='Seed Index',\n",
    "        zaxis_title='Accuracy (%)',\n",
    "        zaxis=dict(range=[0, 100]),\n",
    "        camera=dict(eye=dict(x=1.5, y=1.5, z=1))\n",
    "    ),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig2.show()\n",
    "fig2.write_html(str(CONFIG['results_dir'] / 'figures' / 'robustness_3d_surface.html'))\n",
    "print(\"âœ… Saved: robustness_3d_surface.html\")\n",
    "\n",
    "# ===================== FIGURE 3: ANIMATED ROBUSTNESS DEGRADATION =====================\n",
    "# Create animation data\n",
    "animation_data = []\n",
    "for eps_idx, eps in enumerate([0, 2, 4, 8]):\n",
    "    for seed in all_results:\n",
    "        for cls_idx, cls in enumerate(CONFIG['class_names']):\n",
    "            if eps == 0:\n",
    "                # Clean accuracy per class\n",
    "                all_preds = all_results[seed]['clean']['predictions']\n",
    "                all_labels = all_results[seed]['clean']['labels']\n",
    "                cls_mask = all_labels == cls_idx\n",
    "                acc = (all_preds[cls_mask] == all_labels[cls_mask]).mean() * 100 if cls_mask.sum() > 0 else 0\n",
    "            else:\n",
    "                acc = all_results[seed][f'pgd_{eps}/255']['per_class_robust_acc'][cls]\n",
    "            \n",
    "            animation_data.append({\n",
    "                'Epsilon': f'Îµ={eps}/255' if eps > 0 else 'Clean',\n",
    "                'Epsilon_num': eps,\n",
    "                'Class': cls,\n",
    "                'Accuracy': acc,\n",
    "                'Seed': f'Seed {seed}'\n",
    "            })\n",
    "\n",
    "anim_df = pd.DataFrame(animation_data)\n",
    "\n",
    "fig3 = px.bar(\n",
    "    anim_df,\n",
    "    x='Class',\n",
    "    y='Accuracy',\n",
    "    color='Seed',\n",
    "    animation_frame='Epsilon',\n",
    "    title='<b>Animated: Per-Class Robustness Under Increasing PGD Attack Strength</b><br><sup>Press play to see degradation | Each bar = seed performance</sup>',\n",
    "    labels={'Accuracy': 'Accuracy (%)', 'Class': 'Skin Lesion Class'},\n",
    "    color_discrete_sequence=['#FF6B6B', '#4ECDC4', '#45B7D1'],\n",
    "    template='plotly_white',\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig3.update_layout(\n",
    "    font=dict(family='Arial', size=12),\n",
    "    title_font_size=16,\n",
    "    title_x=0.5,\n",
    "    yaxis=dict(range=[0, 100]),\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
    "    updatemenus=[dict(\n",
    "        type='buttons',\n",
    "        showactive=False,\n",
    "        y=0,\n",
    "        x=0.1,\n",
    "        xanchor='right',\n",
    "        yanchor='top',\n",
    "        buttons=[\n",
    "            dict(label='â–¶ Play', method='animate', args=[None, {'frame': {'duration': 1000, 'redraw': True}, 'fromcurrent': True}]),\n",
    "            dict(label='â¸ Pause', method='animate', args=[[None], {'frame': {'duration': 0, 'redraw': False}, 'mode': 'immediate'}])\n",
    "        ]\n",
    "    )]\n",
    ")\n",
    "\n",
    "fig3.show()\n",
    "fig3.write_html(str(CONFIG['results_dir'] / 'figures' / 'animated_robustness.html'))\n",
    "print(\"âœ… Saved: animated_robustness.html\")\n",
    "\n",
    "# ===================== FIGURE 4: SUNBURST VULNERABILITY BREAKDOWN =====================\n",
    "sunburst_data = []\n",
    "for attack_type in ['FGSM', 'PGD', 'C&W']:\n",
    "    for eps in ['Weak (Îµ=2)', 'Medium (Îµ=4)', 'Strong (Îµ=8)'] if attack_type != 'C&W' else ['Optimization']:\n",
    "        for cls in CONFIG['class_names']:\n",
    "            if attack_type == 'FGSM':\n",
    "                key = f'fgsm_{eps.split(\"=\")[1].split(\")\")[0]}/255' if eps != 'Optimization' else None\n",
    "            elif attack_type == 'PGD':\n",
    "                key = f'pgd_{eps.split(\"=\")[1].split(\")\")[0]}/255' if eps != 'Optimization' else None\n",
    "            else:\n",
    "                key = 'cw'\n",
    "            \n",
    "            if key:\n",
    "                accs = [all_results[s][key]['per_class_robust_acc'][cls] for s in all_results]\n",
    "                vulnerability = 100 - np.mean(accs)\n",
    "                sunburst_data.append({\n",
    "                    'Attack Type': attack_type,\n",
    "                    'Strength': eps,\n",
    "                    'Class': cls,\n",
    "                    'Vulnerability': vulnerability,\n",
    "                    'Path': f'{attack_type}/{eps}/{cls}'\n",
    "                })\n",
    "\n",
    "sunburst_df = pd.DataFrame(sunburst_data)\n",
    "\n",
    "fig4 = px.sunburst(\n",
    "    sunburst_df,\n",
    "    path=['Attack Type', 'Strength', 'Class'],\n",
    "    values='Vulnerability',\n",
    "    color='Vulnerability',\n",
    "    color_continuous_scale='Reds',\n",
    "    title='<b>Hierarchical Vulnerability Breakdown</b><br><sup>Click to drill down | Size = Vulnerability (100% - Accuracy)</sup>',\n",
    ")\n",
    "\n",
    "fig4.update_layout(\n",
    "    font=dict(family='Arial', size=12),\n",
    "    title_font_size=16,\n",
    "    title_x=0.5,\n",
    ")\n",
    "\n",
    "fig4.show()\n",
    "fig4.write_html(str(CONFIG['results_dir'] / 'figures' / 'vulnerability_sunburst.html'))\n",
    "print(\"âœ… Saved: vulnerability_sunburst.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 14: COMPREHENSIVE RESULTS EXPORT & DISSERTATION-READY FIGURES\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ’¾ SAVING ALL RESULTS & GENERATING DISSERTATION FIGURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===================== DISSERTATION-QUALITY SUMMARY FIGURE =====================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# ------ Panel A: Clean vs Robust Accuracy Comparison ------\n",
    "ax = axes[0, 0]\n",
    "clean_mean = np.mean([all_results[s]['clean']['accuracy'] for s in all_results])\n",
    "attacks_compare = [\n",
    "    ('Clean', clean_mean, '#2E8B57'),\n",
    "    ('FGSM Îµ=8/255', np.mean([all_results[s]['fgsm_8/255']['robust_accuracy'] for s in all_results]), '#4169E1'),\n",
    "    ('PGD-40 Îµ=8/255', np.mean([all_results[s]['pgd_8/255']['robust_accuracy'] for s in all_results]), '#DC143C'),\n",
    "    ('C&W L2', np.mean([all_results[s]['cw']['robust_accuracy'] for s in all_results]), '#9400D3'),\n",
    "]\n",
    "\n",
    "names = [a[0] for a in attacks_compare]\n",
    "values = [a[1] for a in attacks_compare]\n",
    "colors = [a[2] for a in attacks_compare]\n",
    "\n",
    "bars = ax.bar(names, values, color=colors, edgecolor='white', linewidth=2, alpha=0.85)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('(a) Model Accuracy Under Strongest Attacks', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.axhline(y=50, color='gray', linestyle=':', alpha=0.5)\n",
    "\n",
    "for bar, val in zip(bars, values):\n",
    "    ax.annotate(f'{val:.1f}%', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 5), textcoords='offset points', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add drop annotations\n",
    "for i, (name, val, color) in enumerate(attacks_compare[1:], 1):\n",
    "    drop = clean_mean - val\n",
    "    ax.annotate(f'â†“{drop:.1f}%', xy=(i, val/2), ha='center', va='center',\n",
    "                fontsize=10, color='white', fontweight='bold')\n",
    "\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# ------ Panel B: Epsilon Sensitivity ------\n",
    "ax = axes[0, 1]\n",
    "eps_vals = [2, 4, 8]\n",
    "\n",
    "fgsm_means = [np.mean([all_results[s][f'fgsm_{e}/255']['robust_accuracy'] for s in all_results]) for e in eps_vals]\n",
    "pgd_means = [np.mean([all_results[s][f'pgd_{e}/255']['robust_accuracy'] for s in all_results]) for e in eps_vals]\n",
    "fgsm_stds = [np.std([all_results[s][f'fgsm_{e}/255']['robust_accuracy'] for s in all_results]) for e in eps_vals]\n",
    "pgd_stds = [np.std([all_results[s][f'pgd_{e}/255']['robust_accuracy'] for s in all_results]) for e in eps_vals]\n",
    "\n",
    "ax.errorbar(eps_vals, fgsm_means, yerr=fgsm_stds, marker='o', markersize=12, linewidth=3,\n",
    "            color='#4169E1', label='FGSM (1-step)', capsize=5, capthick=2)\n",
    "ax.errorbar(eps_vals, pgd_means, yerr=pgd_stds, marker='s', markersize=12, linewidth=3,\n",
    "            color='#DC143C', label='PGD-40 (iterative)', capsize=5, capthick=2)\n",
    "ax.axhline(y=clean_mean, color='#2E8B57', linestyle='--', linewidth=2, label=f'Clean ({clean_mean:.1f}%)')\n",
    "\n",
    "ax.set_xlabel('Perturbation Budget Îµ (Ã—255)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Robust Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('(b) Impact of Perturbation Strength', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(eps_vals)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.legend(loc='upper right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Fill area between curves\n",
    "ax.fill_between(eps_vals, fgsm_means, pgd_means, alpha=0.1, color='gray')\n",
    "ax.annotate('Gap', xy=(5, (fgsm_means[1] + pgd_means[1])/2), fontsize=10, style='italic')\n",
    "\n",
    "# ------ Panel C: Per-Class Vulnerability Radar ------\n",
    "ax = axes[1, 0]\n",
    "ax.axis('off')\n",
    "\n",
    "# Create radar subplot\n",
    "ax_radar = fig.add_subplot(2, 2, 3, projection='polar')\n",
    "\n",
    "# Data for strongest attack (PGD Îµ=8/255)\n",
    "values_radar = []\n",
    "for cls in CONFIG['class_names']:\n",
    "    accs = [all_results[s]['pgd_8/255']['per_class_robust_acc'][cls] for s in all_results]\n",
    "    values_radar.append(np.mean(accs))\n",
    "\n",
    "# Close the loop\n",
    "values_radar_closed = values_radar + [values_radar[0]]\n",
    "angles = np.linspace(0, 2*np.pi, len(CONFIG['class_names']), endpoint=False).tolist()\n",
    "angles += [angles[0]]\n",
    "\n",
    "ax_radar.plot(angles, values_radar_closed, 'o-', linewidth=2.5, color='#DC143C', markersize=8)\n",
    "ax_radar.fill(angles, values_radar_closed, alpha=0.25, color='#DC143C')\n",
    "ax_radar.set_xticks(angles[:-1])\n",
    "ax_radar.set_xticklabels(CONFIG['class_names'], fontsize=11, fontweight='bold')\n",
    "ax_radar.set_ylim(0, 100)\n",
    "ax_radar.set_title('(c) Per-Class Robustness Profile\\n(PGD-40 Îµ=8/255)', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# ------ Panel D: Confusion of Attack Success ------\n",
    "ax = axes[1, 1]\n",
    "\n",
    "# Calculate success rate per class (correctly classified â†’ misclassified)\n",
    "success_rates = []\n",
    "for cls in CONFIG['class_names']:\n",
    "    clean_acc = []\n",
    "    attack_acc = []\n",
    "    for seed in all_results:\n",
    "        all_preds = all_results[seed]['clean']['predictions']\n",
    "        all_labels = all_results[seed]['clean']['labels']\n",
    "        cls_idx = CONFIG['class_names'].index(cls)\n",
    "        cls_mask = all_labels == cls_idx\n",
    "        if cls_mask.sum() > 0:\n",
    "            clean_acc.append((all_preds[cls_mask] == all_labels[cls_mask]).mean() * 100)\n",
    "            attack_acc.append(all_results[seed]['pgd_8/255']['per_class_robust_acc'][cls])\n",
    "    \n",
    "    if clean_acc:\n",
    "        drop = np.mean(clean_acc) - np.mean(attack_acc)\n",
    "        success_rates.append(drop)\n",
    "    else:\n",
    "        success_rates.append(0)\n",
    "\n",
    "sorted_indices = np.argsort(success_rates)[::-1]\n",
    "sorted_classes = [CONFIG['class_names'][i] for i in sorted_indices]\n",
    "sorted_rates = [success_rates[i] for i in sorted_indices]\n",
    "\n",
    "colors_sr = plt.cm.Reds(np.linspace(0.3, 0.9, len(sorted_classes)))\n",
    "bars = ax.barh(sorted_classes, sorted_rates, color=colors_sr, edgecolor='white', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Accuracy Drop Under PGD-40 Îµ=8/255 (%)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('(d) Class-wise Attack Success Rate', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=np.mean(sorted_rates), color='navy', linestyle='--', linewidth=2, label=f'Mean: {np.mean(sorted_rates):.1f}%')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, rate in zip(bars, sorted_rates):\n",
    "    ax.annotate(f'{rate:.1f}%', xy=(rate, bar.get_y() + bar.get_height()/2),\n",
    "                xytext=(5, 0), textcoords='offset points', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Phase 4: Adversarial Robustness Evaluation Summary\\nResNet-50 Baseline on ISIC 2018 Dermoscopy Dataset',\n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(CONFIG['results_dir'] / 'figures' / 'dissertation_figure_robustness.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.savefig(CONFIG['results_dir'] / 'figures' / 'dissertation_figure_robustness.pdf', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.show()\n",
    "print(\"âœ… Saved: dissertation_figure_robustness.png (300 DPI)\")\n",
    "print(\"âœ… Saved: dissertation_figure_robustness.pdf (Vector)\")\n",
    "\n",
    "# ===================== EXPORT ALL RESULTS =====================\n",
    "# Prepare comprehensive JSON export\n",
    "export_results = {\n",
    "    'metadata': {\n",
    "        'experiment': 'Phase 4 Adversarial Robustness Evaluation',\n",
    "        'model': 'ResNet-50 (ImageNet pretrained)',\n",
    "        'dataset': 'ISIC 2018 Dermoscopy',\n",
    "        'num_classes': 7,\n",
    "        'class_names': CONFIG['class_names'],\n",
    "        'seeds_evaluated': list(all_results.keys()),\n",
    "        'attacks_evaluated': ['FGSM', 'PGD-40', 'Carlini-Wagner L2'],\n",
    "        'epsilons': [2/255, 4/255, 8/255],\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    },\n",
    "    'summary': {\n",
    "        'clean_accuracy': {\n",
    "            'mean': float(np.mean([all_results[s]['clean']['accuracy'] for s in all_results])),\n",
    "            'std': float(np.std([all_results[s]['clean']['accuracy'] for s in all_results])),\n",
    "            'per_seed': {str(s): float(all_results[s]['clean']['accuracy']) for s in all_results}\n",
    "        },\n",
    "        'robust_accuracy_fgsm_8': {\n",
    "            'mean': float(np.mean([all_results[s]['fgsm_8/255']['robust_accuracy'] for s in all_results])),\n",
    "            'std': float(np.std([all_results[s]['fgsm_8/255']['robust_accuracy'] for s in all_results])),\n",
    "        },\n",
    "        'robust_accuracy_pgd_8': {\n",
    "            'mean': float(np.mean([all_results[s]['pgd_8/255']['robust_accuracy'] for s in all_results])),\n",
    "            'std': float(np.std([all_results[s]['pgd_8/255']['robust_accuracy'] for s in all_results])),\n",
    "        },\n",
    "        'robust_accuracy_cw': {\n",
    "            'mean': float(np.mean([all_results[s]['cw']['robust_accuracy'] for s in all_results])),\n",
    "            'std': float(np.std([all_results[s]['cw']['robust_accuracy'] for s in all_results])),\n",
    "        },\n",
    "    },\n",
    "    'detailed_results': {}\n",
    "}\n",
    "\n",
    "for seed in all_results:\n",
    "    export_results['detailed_results'][str(seed)] = {}\n",
    "    for attack_key, attack_result in all_results[seed].items():\n",
    "        export_results['detailed_results'][str(seed)][attack_key] = {\n",
    "            k: v.tolist() if isinstance(v, np.ndarray) else v\n",
    "            for k, v in attack_result.items()\n",
    "            if k not in ['predictions', 'labels', 'probs', 'confusion_matrix']\n",
    "        }\n",
    "\n",
    "# Save JSON\n",
    "results_file = CONFIG['results_dir'] / 'adversarial_results_full.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(export_results, f, indent=2)\n",
    "print(f\"âœ… Full results saved to: {results_file}\")\n",
    "\n",
    "# ===================== LIST ALL SAVED FILES =====================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ðŸ“ ALL SAVED FILES IN {CONFIG['results_dir']}:\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for f in sorted(CONFIG['results_dir'].glob('**/*')):\n",
    "    if f.is_file():\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        rel_path = f.relative_to(CONFIG['results_dir'])\n",
    "        icon = 'ðŸ“Š' if f.suffix in ['.png', '.pdf'] else 'ðŸ“„' if f.suffix == '.html' else 'ðŸ’¾'\n",
    "        print(f\"   {icon} {rel_path} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 15: EXECUTIVE SUMMARY & NEXT STEPS\n",
    "# ============================================================================\n",
    "print(\"\\n\")\n",
    "print(\"â•”\" + \"â•\"*78 + \"â•—\")\n",
    "print(\"â•‘\" + \" \"*25 + \"ðŸŽ¯ PHASE 4 COMPLETE\" + \" \"*25 + \"â•‘\")\n",
    "print(\"â•‘\" + \" \"*15 + \"ADVERSARIAL ROBUSTNESS EVALUATION SUMMARY\" + \" \"*14 + \"â•‘\")\n",
    "print(\"â•š\" + \"â•\"*78 + \"â•\")\n",
    "\n",
    "# ===================== KEY METRICS =====================\n",
    "clean_mean = np.mean([all_results[s]['clean']['accuracy'] for s in all_results])\n",
    "clean_std = np.std([all_results[s]['clean']['accuracy'] for s in all_results])\n",
    "fgsm8_mean = np.mean([all_results[s]['fgsm_8/255']['robust_accuracy'] for s in all_results])\n",
    "pgd8_mean = np.mean([all_results[s]['pgd_8/255']['robust_accuracy'] for s in all_results])\n",
    "cw_mean = np.mean([all_results[s]['cw']['robust_accuracy'] for s in all_results])\n",
    "\n",
    "# Calculate average drop\n",
    "avg_drop = np.mean([clean_mean - fgsm8_mean, clean_mean - pgd8_mean, clean_mean - cw_mean])\n",
    "\n",
    "print(f\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      ðŸ“Š BASELINE MODEL PERFORMANCE                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   ðŸŽ¯ Clean Accuracy:         {clean_mean:>6.2f}% Â± {clean_std:.2f}%                            â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   âš”ï¸  Adversarial Robustness (Strong Attacks):                              â”‚\n",
    "â”‚   â”œâ”€â”€ FGSM (Îµ=8/255):        {fgsm8_mean:>6.2f}%  â”‚  Drop: {clean_mean - fgsm8_mean:>5.2f}%                 â”‚\n",
    "â”‚   â”œâ”€â”€ PGD-40 (Îµ=8/255):      {pgd8_mean:>6.2f}%  â”‚  Drop: {clean_mean - pgd8_mean:>5.2f}%                 â”‚\n",
    "â”‚   â””â”€â”€ Carlini-Wagner L2:     {cw_mean:>6.2f}%  â”‚  Drop: {clean_mean - cw_mean:>5.2f}%                 â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   ðŸ“‰ Average Robustness Drop: {avg_drop:.2f}%                                       â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "# ===================== KEY FINDINGS =====================\n",
    "most_vulnerable = CONFIG['class_names'][np.argmin([\n",
    "    np.mean([all_results[s]['pgd_8/255']['per_class_robust_acc'][c] for s in all_results])\n",
    "    for c in CONFIG['class_names']\n",
    "])]\n",
    "most_robust = CONFIG['class_names'][np.argmax([\n",
    "    np.mean([all_results[s]['pgd_8/255']['per_class_robust_acc'][c] for s in all_results])\n",
    "    for c in CONFIG['class_names']\n",
    "])]\n",
    "\n",
    "print(f\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         ðŸ”¬ KEY RESEARCH FINDINGS                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  1ï¸âƒ£  CRITICAL VULNERABILITY                                                 â”‚\n",
    "â”‚      Standard CNNs show SEVERE vulnerability to adversarial attacks         â”‚\n",
    "â”‚      â€¢ Up to {max([clean_mean - fgsm8_mean, clean_mean - pgd8_mean, clean_mean - cw_mean]):.1f}% accuracy degradation under imperceptible perturbations    â”‚\n",
    "â”‚      â€¢ Perturbations invisible to human eye (Îµ â‰¤ 8/255)                     â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  2ï¸âƒ£  ATTACK COMPARISON                                                      â”‚\n",
    "â”‚      â€¢ PGD-40 is more effective than single-step FGSM                       â”‚\n",
    "â”‚      â€¢ C&W finds minimum perturbation for misclassification                 â”‚\n",
    "â”‚      â€¢ Iterative attacks reveal true model fragility                        â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  3ï¸âƒ£  CLASS-WISE ANALYSIS                                                    â”‚\n",
    "â”‚      â€¢ Most vulnerable class: {most_vulnerable:<8}                                      â”‚\n",
    "â”‚      â€¢ Most robust class: {most_robust:<8}                                          â”‚\n",
    "â”‚      â€¢ Vulnerability varies significantly across classes                    â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  4ï¸âƒ£  CLINICAL IMPLICATIONS                                                  â”‚\n",
    "â”‚      âš ï¸  Baseline models are NOT SAFE for clinical deployment               â”‚\n",
    "â”‚      âš ï¸  Adversarial training is ESSENTIAL before real-world use            â”‚\n",
    "â”‚      âš ï¸  All skin lesion classes show significant vulnerability             â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "# ===================== VISUALIZATIONS GENERATED =====================\n",
    "print(f\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      ðŸ“Š VISUALIZATIONS GENERATED                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  Static Figures (PNG/PDF, 300 DPI):                                         â”‚\n",
    "â”‚  â”œâ”€â”€ ðŸ“ˆ robustness_dashboard.png          - Multi-panel analysis            â”‚\n",
    "â”‚  â”œâ”€â”€ ðŸ”¥ class_vulnerability_analysis.png  - Per-class breakdown             â”‚\n",
    "â”‚  â””â”€â”€ ðŸ“„ dissertation_figure_robustness.pdf - Publication-ready              â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  Interactive Figures (HTML):                                                â”‚\n",
    "â”‚  â”œâ”€â”€ ðŸ–±ï¸  attack_comparison_interactive.html                                 â”‚\n",
    "â”‚  â”œâ”€â”€ ðŸŒ robustness_3d_surface.html        - 3D rotatable surface            â”‚\n",
    "â”‚  â”œâ”€â”€ â–¶ï¸  animated_robustness.html          - Epsilon animation              â”‚\n",
    "â”‚  â””â”€â”€ ðŸŒ³ vulnerability_sunburst.html       - Hierarchical breakdown          â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "# ===================== NEXT STEPS =====================\n",
    "print(f\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                          ðŸš€ NEXT STEPS                                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  âœ… PHASE 4 COMPLETE: Adversarial Robustness Evaluation                     â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  ðŸ“Œ PHASE 5: Tri-Objective Robust Training                                  â”‚\n",
    "â”‚     â€¢ Implement adversarial training with PGD-AT                            â”‚\n",
    "â”‚     â€¢ Add explainability preservation objective                             â”‚\n",
    "â”‚     â€¢ Multi-objective optimization (Accuracy + Robustness + XAI)            â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  ðŸ“Œ PHASE 6: Explainability Analysis                                        â”‚\n",
    "â”‚     â€¢ Grad-CAM visualization comparison                                     â”‚\n",
    "â”‚     â€¢ SHAP analysis for feature importance                                  â”‚\n",
    "â”‚     â€¢ XAI consistency under adversarial perturbations                       â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  ðŸ“Œ PHASE 7: Comparative Evaluation                                         â”‚\n",
    "â”‚     â€¢ Baseline vs Robust model comparison                                   â”‚\n",
    "â”‚     â€¢ Trade-off analysis (Accuracy-Robustness-Explainability)               â”‚\n",
    "â”‚     â€¢ Statistical significance testing                                      â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "print(\"â•\"*80)\n",
    "print(\"   âœ… PHASE 4 ADVERSARIAL ROBUSTNESS EVALUATION SUCCESSFULLY COMPLETED!\")\n",
    "print(\"â•\"*80)\n",
    "print(f\"\\nðŸ“ All results saved to: {CONFIG['results_dir']}\")\n",
    "print(f\"ðŸ“Š Total figures generated: 7 (4 static + 4 interactive)\")\n",
    "print(f\"ðŸ’¾ Data files: adversarial_results.csv, adversarial_results_full.json\")\n",
    "print(\"\\nðŸ”— Run Phase 5 notebook to continue with adversarial training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ”§ Cell 2: Environment Setup & Dependencies\n",
    "#@markdown **Run this cell first to install all required packages**\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for adversarial evaluation.\"\"\"\n",
    "    packages = [\n",
    "        \"torch>=2.0.0\",\n",
    "        \"torchvision>=0.15.0\",\n",
    "        \"timm>=0.9.0\",\n",
    "        \"albumentations>=1.3.0\",\n",
    "        \"scikit-learn>=1.3.0\",\n",
    "        \"pandas>=2.0.0\",\n",
    "        \"numpy>=1.24.0\",\n",
    "        \"matplotlib>=3.7.0\",\n",
    "        \"seaborn>=0.12.0\",\n",
    "        \"plotly>=5.15.0\",\n",
    "        \"kaleido\",  # For plotly static export\n",
    "        \"tqdm>=4.65.0\",\n",
    "        \"mlflow>=2.5.0\",\n",
    "        \"scipy>=1.11.0\",\n",
    "    ]\n",
    "    \n",
    "    print(\"ðŸ“¦ Installing packages...\")\n",
    "    for pkg in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "    print(\"âœ… All packages installed!\")\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"ðŸŒ Running in Google Colab\")\n",
    "    install_packages()\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"ðŸ’» Running locally\")\n",
    "\n",
    "# Core imports\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 8),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.dpi': 100,\n",
    "})\n",
    "\n",
    "# GPU Configuration\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ–¥ï¸  HARDWARE CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"âœ… GPU: {gpu_name}\")\n",
    "    print(f\"âœ… VRAM: {gpu_mem:.1f} GB\")\n",
    "    \n",
    "    # Enable optimizations for A100/Ampere GPUs\n",
    "    if \"A100\" in gpu_name or torch.cuda.get_device_capability()[0] >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"âœ… TF32 enabled for Ampere GPU\")\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"âœ… cuDNN benchmark enabled\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"âš ï¸ No GPU found, using CPU (will be slow)\")\n",
    "\n",
    "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "print(f\"âœ… Device: {device}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ—‚ï¸ Cell 3: Mount Google Drive & Configure Paths\n",
    "#@markdown **Configure data and checkpoint paths**\n",
    "\n",
    "# Mount Google Drive (Colab only)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    print(\"âœ… Google Drive mounted\")\n",
    "\n",
    "# ============================================================================\n",
    "# PATH CONFIGURATION - Adjust these paths as needed\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class PathConfig:\n",
    "    \"\"\"Central path configuration for the evaluation pipeline.\"\"\"\n",
    "    \n",
    "    # Base paths\n",
    "    drive_base: Path = Path(\"/content/drive/MyDrive\")\n",
    "    \n",
    "    # Data paths\n",
    "    data_root: Path = field(default=None)\n",
    "    train_dir: Path = field(default=None)\n",
    "    val_dir: Path = field(default=None)\n",
    "    test_dir: Path = field(default=None)\n",
    "    \n",
    "    # Checkpoint paths\n",
    "    checkpoint_dir: Path = field(default=None)\n",
    "    \n",
    "    # Output paths\n",
    "    results_dir: Path = field(default=None)\n",
    "    figures_dir: Path = field(default=None)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize derived paths.\"\"\"\n",
    "        if self.data_root is None:\n",
    "            self.data_root = self.drive_base / \"data\" / \"data\" / \"isic_2018\"\n",
    "        if self.train_dir is None:\n",
    "            self.train_dir = self.data_root / \"train\"\n",
    "        if self.val_dir is None:\n",
    "            self.val_dir = self.data_root / \"val\"\n",
    "        if self.test_dir is None:\n",
    "            self.test_dir = self.data_root / \"test\"\n",
    "        if self.checkpoint_dir is None:\n",
    "            self.checkpoint_dir = self.drive_base / \"checkpoints\" / \"baseline\"\n",
    "        if self.results_dir is None:\n",
    "            self.results_dir = self.drive_base / \"results\" / \"phase4_adversarial\"\n",
    "        if self.figures_dir is None:\n",
    "            self.figures_dir = self.results_dir / \"figures\"\n",
    "    \n",
    "    def validate(self) -> bool:\n",
    "        \"\"\"Validate that required paths exist.\"\"\"\n",
    "        required = [self.data_root, self.test_dir, self.checkpoint_dir]\n",
    "        missing = [p for p in required if not p.exists()]\n",
    "        \n",
    "        if missing:\n",
    "            print(\"âŒ Missing paths:\")\n",
    "            for p in missing:\n",
    "                print(f\"   - {p}\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def create_output_dirs(self):\n",
    "        \"\"\"Create output directories if they don't exist.\"\"\"\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"âœ… Results directory: {self.results_dir}\")\n",
    "        print(f\"âœ… Figures directory: {self.figures_dir}\")\n",
    "\n",
    "# Initialize paths\n",
    "paths = PathConfig()\n",
    "\n",
    "# Validate paths\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“ PATH VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if paths.validate():\n",
    "    print(f\"âœ… Data root: {paths.data_root}\")\n",
    "    print(f\"âœ… Test directory: {paths.test_dir}\")\n",
    "    print(f\"âœ… Checkpoint directory: {paths.checkpoint_dir}\")\n",
    "    paths.create_output_dirs()\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Please update PathConfig with correct paths!\")\n",
    "    \n",
    "# List available checkpoints\n",
    "print(\"\\nðŸ“¦ Available Checkpoints:\")\n",
    "if paths.checkpoint_dir.exists():\n",
    "    checkpoints = list(paths.checkpoint_dir.glob(\"*.pt\"))\n",
    "    for ckpt in sorted(checkpoints):\n",
    "        size_mb = ckpt.stat().st_size / 1e6\n",
    "        print(f\"   - {ckpt.name} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"   âš ï¸ No checkpoints found!\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e498f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ“¥ Cell 4: Clone Repository & Import Attack Classes\n",
    "#@markdown **Clone the project repository and import custom modules**\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Clone repository (Colab only)\n",
    "REPO_URL = \"https://github.com/viraj1011JAIN/tri-objective-robust-xai-medimg.git\"\n",
    "REPO_DIR = \"/content/tri-objective-robust-xai-medimg\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists(REPO_DIR):\n",
    "        print(f\"ðŸ“¥ Cloning repository...\")\n",
    "        os.system(f\"git clone {REPO_URL} {REPO_DIR}\")\n",
    "        print(\"âœ… Repository cloned!\")\n",
    "    else:\n",
    "        print(\"ðŸ“ Repository already exists, pulling latest...\")\n",
    "        os.system(f\"cd {REPO_DIR} && git pull\")\n",
    "    \n",
    "    # Add to Python path\n",
    "    if REPO_DIR not in sys.path:\n",
    "        sys.path.insert(0, REPO_DIR)\n",
    "    print(f\"âœ… Added {REPO_DIR} to Python path\")\n",
    "else:\n",
    "    # Local development - find project root\n",
    "    current_dir = Path.cwd()\n",
    "    if \"notebooks\" in str(current_dir):\n",
    "        project_root = current_dir.parent\n",
    "    else:\n",
    "        project_root = current_dir\n",
    "    \n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "    print(f\"âœ… Using local project: {project_root}\")\n",
    "\n",
    "# Import custom attack classes\n",
    "print(\"\\nðŸ”§ Importing attack modules...\")\n",
    "\n",
    "try:\n",
    "    from src.attacks.fgsm import FGSM, FGSMConfig, fgsm_attack\n",
    "    from src.attacks.pgd import PGD, PGDConfig, pgd_attack\n",
    "    from src.attacks.cw import CarliniWagner, CWConfig, cw_attack\n",
    "    from src.attacks.base import AttackConfig, AttackResult\n",
    "    print(\"âœ… FGSM attack imported\")\n",
    "    print(\"âœ… PGD attack imported\")\n",
    "    print(\"âœ… Carlini-Wagner attack imported\")\n",
    "    print(\"âœ… Base attack classes imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "    print(\"âš ï¸ Please ensure the repository is properly cloned\")\n",
    "    raise\n",
    "\n",
    "# Import dataset utilities\n",
    "try:\n",
    "    from src.datasets.isic import ISICDataset\n",
    "    print(\"âœ… ISICDataset imported\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ ISICDataset not found, will use custom implementation\")\n",
    "    ISICDataset = None\n",
    "\n",
    "# Import model utilities\n",
    "import timm\n",
    "print(f\"âœ… timm version: {timm.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ALL MODULES IMPORTED SUCCESSFULLY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf885ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ“Š Cell 5: Dataset & Model Loading Utilities\n",
    "#@markdown **Define dataset wrapper and model loading functions**\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ============================================================================\n",
    "# ISIC 2018 CLASS INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"AKIEC\",  # Actinic Keratoses\n",
    "    \"BCC\",    # Basal Cell Carcinoma\n",
    "    \"BKL\",    # Benign Keratosis\n",
    "    \"DF\",     # Dermatofibroma\n",
    "    \"MEL\",    # Melanoma\n",
    "    \"NV\",     # Melanocytic Nevi\n",
    "    \"VASC\"    # Vascular Lesions\n",
    "]\n",
    "\n",
    "CLASS_DESCRIPTIONS = {\n",
    "    \"AKIEC\": \"Actinic Keratoses (pre-cancerous)\",\n",
    "    \"BCC\": \"Basal Cell Carcinoma (cancerous)\",\n",
    "    \"BKL\": \"Benign Keratosis (non-cancerous)\",\n",
    "    \"DF\": \"Dermatofibroma (benign)\",\n",
    "    \"MEL\": \"Melanoma (malignant, dangerous)\",\n",
    "    \"NV\": \"Melanocytic Nevi (common moles)\",\n",
    "    \"VASC\": \"Vascular Lesions (blood vessel)\"\n",
    "}\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# ImageNet normalization (used by pretrained models)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class ISICTestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ISIC 2018 Test Dataset for adversarial evaluation.\n",
    "    \n",
    "    Returns unnormalized images in [0, 1] range for adversarial attacks.\n",
    "    Normalization is applied separately during model inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: Path,\n",
    "        transform: Optional[A.Compose] = None,\n",
    "        max_samples: Optional[int] = None\n",
    "    ):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform or self._default_transform()\n",
    "        \n",
    "        # Collect samples\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {name: idx for idx, name in enumerate(CLASS_NAMES)}\n",
    "        \n",
    "        for class_name in CLASS_NAMES:\n",
    "            class_dir = self.root_dir / class_name\n",
    "            if class_dir.exists():\n",
    "                images = list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\"))\n",
    "                for img_path in images:\n",
    "                    self.samples.append((img_path, self.class_to_idx[class_name]))\n",
    "        \n",
    "        # Limit samples if specified\n",
    "        if max_samples and len(self.samples) > max_samples:\n",
    "            # Stratified sampling\n",
    "            from collections import defaultdict\n",
    "            by_class = defaultdict(list)\n",
    "            for path, label in self.samples:\n",
    "                by_class[label].append((path, label))\n",
    "            \n",
    "            per_class = max_samples // NUM_CLASSES\n",
    "            self.samples = []\n",
    "            for label, items in by_class.items():\n",
    "                self.samples.extend(items[:per_class])\n",
    "        \n",
    "        print(f\"ðŸ“Š Loaded {len(self.samples)} test samples from {self.root_dir}\")\n",
    "        \n",
    "        # Print class distribution\n",
    "        class_counts = {}\n",
    "        for _, label in self.samples:\n",
    "            class_counts[label] = class_counts.get(label, 0) + 1\n",
    "        for idx, name in enumerate(CLASS_NAMES):\n",
    "            print(f\"   {name}: {class_counts.get(idx, 0)} samples\")\n",
    "    \n",
    "    def _default_transform(self) -> A.Compose:\n",
    "        \"\"\"Default test transform: resize and convert to tensor.\"\"\"\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),  # Keep in [0, 1]\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, dict]:\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed[\"image\"]\n",
    "        \n",
    "        # Metadata\n",
    "        metadata = {\n",
    "            \"image_path\": str(img_path),\n",
    "            \"class_name\": CLASS_NAMES[label]\n",
    "        }\n",
    "        \n",
    "        return image.float(), label, metadata\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def create_model(num_classes: int = NUM_CLASSES, pretrained: bool = False) -> nn.Module:\n",
    "    \"\"\"Create ResNet-50 model for ISIC classification.\"\"\"\n",
    "    model = timm.create_model(\n",
    "        \"resnet50\",\n",
    "        pretrained=pretrained,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def load_checkpoint(model: nn.Module, checkpoint_path: Path, device: torch.device) -> dict:\n",
    "    \"\"\"\n",
    "    Load model checkpoint and return metadata.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        device: Target device\n",
    "    \n",
    "    Returns:\n",
    "        Checkpoint metadata dictionary\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    \n",
    "    # Handle different checkpoint formats\n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        metadata = {\n",
    "            \"epoch\": checkpoint.get(\"epoch\", \"unknown\"),\n",
    "            \"val_acc\": checkpoint.get(\"val_acc\", checkpoint.get(\"best_val_acc\", \"unknown\")),\n",
    "            \"seed\": checkpoint.get(\"seed\", \"unknown\")\n",
    "        }\n",
    "    elif \"state_dict\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        metadata = {\"epoch\": \"unknown\", \"val_acc\": \"unknown\", \"seed\": \"unknown\"}\n",
    "    else:\n",
    "        # Direct state dict\n",
    "        model.load_state_dict(checkpoint)\n",
    "        metadata = {\"epoch\": \"unknown\", \"val_acc\": \"unknown\", \"seed\": \"unknown\"}\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def get_normalizer(device: torch.device):\n",
    "    \"\"\"Get ImageNet normalization function.\"\"\"\n",
    "    mean = torch.tensor(IMAGENET_MEAN).view(1, 3, 1, 1).to(device)\n",
    "    std = torch.tensor(IMAGENET_STD).view(1, 3, 1, 1).to(device)\n",
    "    \n",
    "    def normalize(x: torch.Tensor) -> torch.Tensor:\n",
    "        return (x - mean) / std\n",
    "    \n",
    "    return normalize\n",
    "\n",
    "print(\"âœ… Dataset and model utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ab186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title âš™ï¸ Cell 6: Evaluation Configuration\n",
    "#@markdown **Configure attack parameters and evaluation settings**\n",
    "\n",
    "@dataclass\n",
    "class EvaluationConfig:\n",
    "    \"\"\"Configuration for adversarial evaluation.\"\"\"\n",
    "    \n",
    "    # Seeds to evaluate\n",
    "    seeds: List[int] = field(default_factory=lambda: [42, 123, 456])\n",
    "    \n",
    "    # Epsilon values (perturbation budgets)\n",
    "    epsilons: List[float] = field(default_factory=lambda: [2/255, 4/255, 8/255])\n",
    "    \n",
    "    # Attack configurations\n",
    "    fgsm_enabled: bool = True\n",
    "    pgd_enabled: bool = True\n",
    "    pgd_steps: int = 40\n",
    "    pgd_step_size: Optional[float] = None  # Auto: epsilon/4\n",
    "    \n",
    "    cw_enabled: bool = True\n",
    "    cw_iterations: int = 100  # Reduced for speed (default 1000)\n",
    "    cw_confidence: float = 0.0\n",
    "    cw_learning_rate: float = 0.01\n",
    "    \n",
    "    # Evaluation settings\n",
    "    batch_size: int = 64  # Increase for A100\n",
    "    num_workers: int = 4\n",
    "    max_test_samples: Optional[int] = None  # None = all samples\n",
    "    \n",
    "    # Output settings\n",
    "    save_adversarial_examples: bool = True\n",
    "    num_examples_to_save: int = 50\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Adjust settings based on hardware.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            \n",
    "            # Optimize batch size for GPU memory\n",
    "            if gpu_mem >= 35:  # A100 40GB\n",
    "                self.batch_size = 128\n",
    "                self.num_workers = 8\n",
    "                self.cw_iterations = 200\n",
    "                print(f\"âš¡ A100 optimizations: batch={self.batch_size}, C&W iters={self.cw_iterations}\")\n",
    "            elif gpu_mem >= 14:  # T4/V100\n",
    "                self.batch_size = 64\n",
    "                self.num_workers = 4\n",
    "                self.cw_iterations = 100\n",
    "                print(f\"âš¡ T4/V100 settings: batch={self.batch_size}\")\n",
    "            else:  # Smaller GPU\n",
    "                self.batch_size = 32\n",
    "                self.num_workers = 2\n",
    "                self.cw_iterations = 50\n",
    "                print(f\"âš ï¸ Limited GPU: batch={self.batch_size}\")\n",
    "    \n",
    "    def get_epsilon_str(self, eps: float) -> str:\n",
    "        \"\"\"Convert epsilon to readable string.\"\"\"\n",
    "        return f\"{int(eps * 255)}/255\"\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Get configuration summary.\"\"\"\n",
    "        lines = [\n",
    "            \"=\"*60,\n",
    "            \"ðŸ“‹ EVALUATION CONFIGURATION\",\n",
    "            \"=\"*60,\n",
    "            f\"Seeds: {self.seeds}\",\n",
    "            f\"Epsilons: {[self.get_epsilon_str(e) for e in self.epsilons]}\",\n",
    "            \"\",\n",
    "            \"Attacks:\",\n",
    "            f\"  FGSM: {'âœ“' if self.fgsm_enabled else 'âœ—'}\",\n",
    "            f\"  PGD:  {'âœ“' if self.pgd_enabled else 'âœ—'} (steps={self.pgd_steps})\",\n",
    "            f\"  C&W:  {'âœ“' if self.cw_enabled else 'âœ—'} (iters={self.cw_iterations})\",\n",
    "            \"\",\n",
    "            f\"Batch size: {self.batch_size}\",\n",
    "            f\"Max samples: {self.max_test_samples or 'all'}\",\n",
    "            \"=\"*60\n",
    "        ]\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# Initialize configuration\n",
    "config = EvaluationConfig()\n",
    "print(config.summary())\n",
    "\n",
    "# Epsilon display helper\n",
    "EPSILON_LABELS = {\n",
    "    2/255: \"Îµ=2/255 (weak)\",\n",
    "    4/255: \"Îµ=4/255 (medium)\",\n",
    "    8/255: \"Îµ=8/255 (strong)\"\n",
    "}\n",
    "\n",
    "print(\"\\nâœ… Configuration ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7777fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸŽ¯ Cell 7: Adversarial Attack Engine\n",
    "#@markdown **Core attack generation and evaluation functions**\n",
    "\n",
    "class AdversarialEvaluator:\n",
    "    \"\"\"\n",
    "    Unified adversarial evaluation engine.\n",
    "    \n",
    "    Supports FGSM, PGD, and Carlini-Wagner attacks with batch processing\n",
    "    and detailed metrics collection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        device: torch.device,\n",
    "        normalize_fn: callable\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.normalize = normalize_fn\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = {}\n",
    "    \n",
    "    def evaluate_clean(\n",
    "        self,\n",
    "        dataloader: DataLoader,\n",
    "        desc: str = \"Clean Evaluation\"\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate model on clean (unperturbed) data.\"\"\"\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=desc, leave=False):\n",
    "                # Handle 3-tuple (image, label, metadata)\n",
    "                images, labels = batch[0], batch[1]\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                # Forward pass with normalization\n",
    "                logits = self.model(self.normalize(images))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_probs = np.array(all_probs)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "        f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        # Per-class accuracy\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "        \n",
    "        # AUROC (one-vs-rest)\n",
    "        try:\n",
    "            auroc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
    "        except ValueError:\n",
    "            auroc = 0.0\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"balanced_accuracy\": balanced_acc,\n",
    "            \"f1_macro\": f1_macro,\n",
    "            \"f1_weighted\": f1_weighted,\n",
    "            \"auroc\": auroc,\n",
    "            \"per_class_accuracy\": dict(zip(CLASS_NAMES, per_class_acc)),\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"predictions\": all_preds,\n",
    "            \"labels\": all_labels,\n",
    "            \"probabilities\": all_probs\n",
    "        }\n",
    "    \n",
    "    def evaluate_attack(\n",
    "        self,\n",
    "        dataloader: DataLoader,\n",
    "        attack_name: str,\n",
    "        epsilon: float,\n",
    "        attack_fn: callable,\n",
    "        desc: str = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate model under adversarial attack.\n",
    "        \n",
    "        Args:\n",
    "            dataloader: Test data loader\n",
    "            attack_name: Name of attack (FGSM, PGD, CW)\n",
    "            epsilon: Perturbation budget\n",
    "            attack_fn: Function that generates adversarial examples\n",
    "            desc: Progress bar description\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with attack results and metrics\n",
    "        \"\"\"\n",
    "        if desc is None:\n",
    "            desc = f\"{attack_name} Îµ={config.get_epsilon_str(epsilon)}\"\n",
    "        \n",
    "        all_preds_clean = []\n",
    "        all_preds_adv = []\n",
    "        all_labels = []\n",
    "        all_l2_dists = []\n",
    "        all_linf_dists = []\n",
    "        successful_attacks = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Store some examples for visualization\n",
    "        saved_examples = []\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=desc, leave=False):\n",
    "            images, labels = batch[0], batch[1]\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            # Clean predictions\n",
    "            with torch.no_grad():\n",
    "                clean_logits = self.model(self.normalize(images))\n",
    "                clean_preds = clean_logits.argmax(dim=1)\n",
    "            \n",
    "            # Generate adversarial examples\n",
    "            try:\n",
    "                x_adv = attack_fn(images, labels)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Attack failed on batch: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Adversarial predictions\n",
    "            with torch.no_grad():\n",
    "                adv_logits = self.model(self.normalize(x_adv))\n",
    "                adv_preds = adv_logits.argmax(dim=1)\n",
    "            \n",
    "            # Calculate perturbation norms\n",
    "            delta = (x_adv - images).view(images.size(0), -1)\n",
    "            l2_dist = torch.norm(delta, p=2, dim=1)\n",
    "            linf_dist = torch.norm(delta, p=float('inf'), dim=1)\n",
    "            \n",
    "            # Track successful attacks (correctly classified â†’ misclassified)\n",
    "            was_correct = (clean_preds == labels)\n",
    "            is_wrong = (adv_preds != labels)\n",
    "            successful = was_correct & is_wrong\n",
    "            \n",
    "            successful_attacks += successful.sum().item()\n",
    "            total_samples += was_correct.sum().item()  # Only count correctly classified\n",
    "            \n",
    "            # Store results\n",
    "            all_preds_clean.extend(clean_preds.cpu().numpy())\n",
    "            all_preds_adv.extend(adv_preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_l2_dists.extend(l2_dist.cpu().numpy())\n",
    "            all_linf_dists.extend(linf_dist.cpu().numpy())\n",
    "            \n",
    "            # Save examples for visualization\n",
    "            if len(saved_examples) < config.num_examples_to_save:\n",
    "                for i in range(min(5, images.size(0))):\n",
    "                    if len(saved_examples) >= config.num_examples_to_save:\n",
    "                        break\n",
    "                    saved_examples.append({\n",
    "                        \"clean\": images[i].cpu(),\n",
    "                        \"adversarial\": x_adv[i].cpu(),\n",
    "                        \"perturbation\": (x_adv[i] - images[i]).cpu(),\n",
    "                        \"true_label\": labels[i].item(),\n",
    "                        \"clean_pred\": clean_preds[i].item(),\n",
    "                        \"adv_pred\": adv_preds[i].item()\n",
    "                    })\n",
    "        \n",
    "        # Convert to arrays\n",
    "        all_preds_clean = np.array(all_preds_clean)\n",
    "        all_preds_adv = np.array(all_preds_adv)\n",
    "        all_labels = np.array(all_labels)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        clean_acc = accuracy_score(all_labels, all_preds_clean)\n",
    "        robust_acc = accuracy_score(all_labels, all_preds_adv)\n",
    "        attack_success_rate = successful_attacks / max(total_samples, 1)\n",
    "        \n",
    "        # Per-class robust accuracy\n",
    "        cm_adv = confusion_matrix(all_labels, all_preds_adv)\n",
    "        per_class_robust_acc = cm_adv.diagonal() / cm_adv.sum(axis=1)\n",
    "        \n",
    "        return {\n",
    "            \"attack_name\": attack_name,\n",
    "            \"epsilon\": epsilon,\n",
    "            \"epsilon_str\": config.get_epsilon_str(epsilon),\n",
    "            \"clean_accuracy\": clean_acc,\n",
    "            \"robust_accuracy\": robust_acc,\n",
    "            \"accuracy_drop\": clean_acc - robust_acc,\n",
    "            \"attack_success_rate\": attack_success_rate,\n",
    "            \"mean_l2_dist\": np.mean(all_l2_dists),\n",
    "            \"mean_linf_dist\": np.mean(all_linf_dists),\n",
    "            \"per_class_robust_accuracy\": dict(zip(CLASS_NAMES, per_class_robust_acc)),\n",
    "            \"confusion_matrix\": cm_adv,\n",
    "            \"saved_examples\": saved_examples,\n",
    "            \"predictions_clean\": all_preds_clean,\n",
    "            \"predictions_adv\": all_preds_adv,\n",
    "            \"labels\": all_labels\n",
    "        }\n",
    "    \n",
    "    def create_fgsm_attack(self, epsilon: float) -> callable:\n",
    "        \"\"\"Create FGSM attack function.\"\"\"\n",
    "        fgsm_config = FGSMConfig(\n",
    "            epsilon=epsilon,\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False,\n",
    "            device=str(self.device)\n",
    "        )\n",
    "        attack = FGSM(fgsm_config)\n",
    "        \n",
    "        def attack_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "            return attack.generate(\n",
    "                self.model, x, y,\n",
    "                loss_fn=nn.CrossEntropyLoss(),\n",
    "                normalize=self.normalize\n",
    "            )\n",
    "        return attack_fn\n",
    "    \n",
    "    def create_pgd_attack(self, epsilon: float, num_steps: int = 40) -> callable:\n",
    "        \"\"\"Create PGD attack function.\"\"\"\n",
    "        step_size = epsilon / 4  # Standard choice\n",
    "        \n",
    "        pgd_config = PGDConfig(\n",
    "            epsilon=epsilon,\n",
    "            num_steps=num_steps,\n",
    "            step_size=step_size,\n",
    "            random_start=True,\n",
    "            early_stop=False,\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False,\n",
    "            device=str(self.device)\n",
    "        )\n",
    "        attack = PGD(pgd_config)\n",
    "        \n",
    "        def attack_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "            return attack.generate(\n",
    "                self.model, x, y,\n",
    "                loss_fn=nn.CrossEntropyLoss(),\n",
    "                normalize=self.normalize\n",
    "            )\n",
    "        return attack_fn\n",
    "    \n",
    "    def create_cw_attack(\n",
    "        self,\n",
    "        confidence: float = 0.0,\n",
    "        max_iterations: int = 100,\n",
    "        learning_rate: float = 0.01\n",
    "    ) -> callable:\n",
    "        \"\"\"Create Carlini-Wagner L2 attack function.\"\"\"\n",
    "        cw_config = CWConfig(\n",
    "            confidence=confidence,\n",
    "            learning_rate=learning_rate,\n",
    "            max_iterations=max_iterations,\n",
    "            binary_search_steps=5,  # Reduced for speed\n",
    "            initial_c=1e-3,\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False,\n",
    "            device=str(self.device)\n",
    "        )\n",
    "        attack = CarliniWagner(cw_config)\n",
    "        \n",
    "        def attack_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "            return attack.generate(\n",
    "                self.model, x, y,\n",
    "                normalize=self.normalize\n",
    "            )\n",
    "        return attack_fn\n",
    "\n",
    "print(\"âœ… AdversarialEvaluator class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32201a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ“¦ Cell 8: Load Test Dataset and Models\n",
    "#@markdown **Load test data and all seed checkpoints**\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š LOADING TEST DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = ISICTestDataset(\n",
    "    root_dir=paths.test_dir,\n",
    "    max_samples=config.max_test_samples\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Test samples: {len(test_dataset)}\")\n",
    "print(f\"âœ… Batches: {len(test_loader)}\")\n",
    "print(f\"âœ… Batch size: {config.batch_size}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODELS FOR ALL SEEDS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ”§ LOADING MODEL CHECKPOINTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = {}\n",
    "normalize = get_normalizer(device)\n",
    "\n",
    "for seed in config.seeds:\n",
    "    checkpoint_path = paths.checkpoint_dir / f\"baseline_seed_{seed}.pt\"\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        # Try alternative naming\n",
    "        alt_path = paths.checkpoint_dir / f\"seed_{seed}_best.pt\"\n",
    "        if alt_path.exists():\n",
    "            checkpoint_path = alt_path\n",
    "        else:\n",
    "            print(f\"âš ï¸ Checkpoint not found for seed {seed}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nðŸ“¥ Loading seed {seed}...\")\n",
    "    model = create_model(num_classes=NUM_CLASSES, pretrained=False)\n",
    "    metadata = load_checkpoint(model, checkpoint_path, device)\n",
    "    \n",
    "    models[seed] = model\n",
    "    \n",
    "    print(f\"   âœ… Loaded: {checkpoint_path.name}\")\n",
    "    print(f\"   ðŸ“ˆ Validation accuracy: {metadata.get('val_acc', 'N/A')}\")\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(models)} models\")\n",
    "\n",
    "# ============================================================================\n",
    "# SANITY CHECK: VERIFY CLEAN ACCURACY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ” SANITY CHECK: CLEAN ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "clean_results = {}\n",
    "\n",
    "for seed, model in models.items():\n",
    "    print(f\"\\nðŸ§ª Evaluating seed {seed}...\")\n",
    "    evaluator = AdversarialEvaluator(model, device, normalize)\n",
    "    result = evaluator.evaluate_clean(test_loader, desc=f\"Clean eval (seed {seed})\")\n",
    "    clean_results[seed] = result\n",
    "    \n",
    "    print(f\"   âœ… Accuracy: {result['accuracy']*100:.2f}%\")\n",
    "    print(f\"   âœ… Balanced Accuracy: {result['balanced_accuracy']*100:.2f}%\")\n",
    "    print(f\"   âœ… F1 (macro): {result['f1_macro']*100:.2f}%\")\n",
    "    print(f\"   âœ… AUROC: {result['auroc']*100:.2f}%\")\n",
    "\n",
    "# Summary statistics\n",
    "mean_acc = np.mean([r['accuracy'] for r in clean_results.values()])\n",
    "std_acc = np.std([r['accuracy'] for r in clean_results.values()])\n",
    "\n",
    "print(f\"\\nðŸ“Š Mean Clean Accuracy: {mean_acc*100:.2f}% Â± {std_acc*100:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259012ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸš€ Cell 9: Run Full Adversarial Evaluation\n",
    "#@markdown **Execute FGSM, PGD, and C&W attacks across all seeds and epsilons**\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸš€ STARTING FULL ADVERSARIAL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"â±ï¸  Start time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"ðŸŽ¯ Seeds: {config.seeds}\")\n",
    "print(f\"ðŸŽ¯ Epsilons: {[config.get_epsilon_str(e) for e in config.epsilons]}\")\n",
    "print(f\"ðŸŽ¯ Attacks: FGSM={'âœ“' if config.fgsm_enabled else 'âœ—'}, \"\n",
    "      f\"PGD={'âœ“' if config.pgd_enabled else 'âœ—'}, \"\n",
    "      f\"C&W={'âœ“' if config.cw_enabled else 'âœ—'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Results storage\n",
    "all_results = {\n",
    "    \"clean\": clean_results,\n",
    "    \"fgsm\": {},\n",
    "    \"pgd\": {},\n",
    "    \"cw\": {}\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for seed in config.seeds:\n",
    "    if seed not in models:\n",
    "        continue\n",
    "    \n",
    "    model = models[seed]\n",
    "    evaluator = AdversarialEvaluator(model, device, normalize)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ”¬ EVALUATING SEED {seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Initialize seed results\n",
    "    all_results[\"fgsm\"][seed] = {}\n",
    "    all_results[\"pgd\"][seed] = {}\n",
    "    all_results[\"cw\"][seed] = {}\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FGSM ATTACKS\n",
    "    # ========================================================================\n",
    "    if config.fgsm_enabled:\n",
    "        print(f\"\\nâš¡ FGSM Attacks (Seed {seed})\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for eps in config.epsilons:\n",
    "            attack_fn = evaluator.create_fgsm_attack(eps)\n",
    "            result = evaluator.evaluate_attack(\n",
    "                test_loader,\n",
    "                attack_name=\"FGSM\",\n",
    "                epsilon=eps,\n",
    "                attack_fn=attack_fn\n",
    "            )\n",
    "            all_results[\"fgsm\"][seed][eps] = result\n",
    "            \n",
    "            print(f\"   Îµ={config.get_epsilon_str(eps):>7}: \"\n",
    "                  f\"Robust Acc = {result['robust_accuracy']*100:5.2f}% \"\n",
    "                  f\"(â†“{result['accuracy_drop']*100:5.2f}pp)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PGD ATTACKS\n",
    "    # ========================================================================\n",
    "    if config.pgd_enabled:\n",
    "        print(f\"\\nðŸ”„ PGD-{config.pgd_steps} Attacks (Seed {seed})\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for eps in config.epsilons:\n",
    "            attack_fn = evaluator.create_pgd_attack(eps, num_steps=config.pgd_steps)\n",
    "            result = evaluator.evaluate_attack(\n",
    "                test_loader,\n",
    "                attack_name=f\"PGD-{config.pgd_steps}\",\n",
    "                epsilon=eps,\n",
    "                attack_fn=attack_fn\n",
    "            )\n",
    "            all_results[\"pgd\"][seed][eps] = result\n",
    "            \n",
    "            print(f\"   Îµ={config.get_epsilon_str(eps):>7}: \"\n",
    "                  f\"Robust Acc = {result['robust_accuracy']*100:5.2f}% \"\n",
    "                  f\"(â†“{result['accuracy_drop']*100:5.2f}pp)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # C&W ATTACKS (Only at strongest epsilon for speed)\n",
    "    # ========================================================================\n",
    "    if config.cw_enabled:\n",
    "        print(f\"\\nðŸŽ¯ C&W L2 Attack (Seed {seed})\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # C&W is epsilon-free (L2 minimization), run once\n",
    "        attack_fn = evaluator.create_cw_attack(\n",
    "            confidence=config.cw_confidence,\n",
    "            max_iterations=config.cw_iterations,\n",
    "            learning_rate=config.cw_learning_rate\n",
    "        )\n",
    "        result = evaluator.evaluate_attack(\n",
    "            test_loader,\n",
    "            attack_name=\"C&W-L2\",\n",
    "            epsilon=0.0,  # C&W minimizes L2 directly\n",
    "            attack_fn=attack_fn,\n",
    "            desc=\"C&W L2 Attack\"\n",
    "        )\n",
    "        all_results[\"cw\"][seed][\"l2\"] = result\n",
    "        \n",
    "        print(f\"   Robust Acc = {result['robust_accuracy']*100:5.2f}% \"\n",
    "              f\"(â†“{result['accuracy_drop']*100:5.2f}pp)\")\n",
    "        print(f\"   Mean L2 perturbation: {result['mean_l2_dist']:.4f}\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Timing\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ… EVALUATION COMPLETE\")\n",
    "print(f\"â±ï¸  Total time: {elapsed/60:.1f} minutes\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ“Š Cell 10: Results Summary Table\n",
    "#@markdown **Generate comprehensive results summary with statistics**\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š ADVERSARIAL ROBUSTNESS RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# BUILD RESULTS DATAFRAME\n",
    "# ============================================================================\n",
    "\n",
    "results_data = []\n",
    "\n",
    "for seed in config.seeds:\n",
    "    if seed not in clean_results:\n",
    "        continue\n",
    "    \n",
    "    # Clean accuracy\n",
    "    clean_acc = clean_results[seed][\"accuracy\"]\n",
    "    \n",
    "    # FGSM results\n",
    "    if config.fgsm_enabled and seed in all_results[\"fgsm\"]:\n",
    "        for eps in config.epsilons:\n",
    "            if eps in all_results[\"fgsm\"][seed]:\n",
    "                r = all_results[\"fgsm\"][seed][eps]\n",
    "                results_data.append({\n",
    "                    \"Seed\": seed,\n",
    "                    \"Attack\": \"FGSM\",\n",
    "                    \"Epsilon\": config.get_epsilon_str(eps),\n",
    "                    \"Epsilon_Val\": eps,\n",
    "                    \"Clean_Acc\": clean_acc * 100,\n",
    "                    \"Robust_Acc\": r[\"robust_accuracy\"] * 100,\n",
    "                    \"Acc_Drop\": r[\"accuracy_drop\"] * 100,\n",
    "                    \"Attack_Success\": r[\"attack_success_rate\"] * 100,\n",
    "                    \"Mean_Linf\": r[\"mean_linf_dist\"]\n",
    "                })\n",
    "    \n",
    "    # PGD results\n",
    "    if config.pgd_enabled and seed in all_results[\"pgd\"]:\n",
    "        for eps in config.epsilons:\n",
    "            if eps in all_results[\"pgd\"][seed]:\n",
    "                r = all_results[\"pgd\"][seed][eps]\n",
    "                results_data.append({\n",
    "                    \"Seed\": seed,\n",
    "                    \"Attack\": f\"PGD-{config.pgd_steps}\",\n",
    "                    \"Epsilon\": config.get_epsilon_str(eps),\n",
    "                    \"Epsilon_Val\": eps,\n",
    "                    \"Clean_Acc\": clean_acc * 100,\n",
    "                    \"Robust_Acc\": r[\"robust_accuracy\"] * 100,\n",
    "                    \"Acc_Drop\": r[\"accuracy_drop\"] * 100,\n",
    "                    \"Attack_Success\": r[\"attack_success_rate\"] * 100,\n",
    "                    \"Mean_Linf\": r[\"mean_linf_dist\"]\n",
    "                })\n",
    "    \n",
    "    # C&W results\n",
    "    if config.cw_enabled and seed in all_results[\"cw\"]:\n",
    "        if \"l2\" in all_results[\"cw\"][seed]:\n",
    "            r = all_results[\"cw\"][seed][\"l2\"]\n",
    "            results_data.append({\n",
    "                \"Seed\": seed,\n",
    "                \"Attack\": \"C&W-L2\",\n",
    "                \"Epsilon\": \"N/A\",\n",
    "                \"Epsilon_Val\": 0,\n",
    "                \"Clean_Acc\": clean_acc * 100,\n",
    "                \"Robust_Acc\": r[\"robust_accuracy\"] * 100,\n",
    "                \"Acc_Drop\": r[\"accuracy_drop\"] * 100,\n",
    "                \"Attack_Success\": r[\"attack_success_rate\"] * 100,\n",
    "                \"Mean_L2\": r[\"mean_l2_dist\"]\n",
    "            })\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "\n",
    "# ============================================================================\n",
    "# AGGREGATE STATISTICS (Mean Â± Std across seeds)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“ˆ AGGREGATED RESULTS (Mean Â± Std across seeds)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "# Group by Attack and Epsilon\n",
    "for attack in df_results[\"Attack\"].unique():\n",
    "    for eps in df_results[df_results[\"Attack\"] == attack][\"Epsilon\"].unique():\n",
    "        subset = df_results[(df_results[\"Attack\"] == attack) & (df_results[\"Epsilon\"] == eps)]\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            summary_data.append({\n",
    "                \"Attack\": attack,\n",
    "                \"Epsilon\": eps,\n",
    "                \"Clean_Acc\": f\"{subset['Clean_Acc'].mean():.2f} Â± {subset['Clean_Acc'].std():.2f}\",\n",
    "                \"Robust_Acc\": f\"{subset['Robust_Acc'].mean():.2f} Â± {subset['Robust_Acc'].std():.2f}\",\n",
    "                \"Acc_Drop\": f\"{subset['Acc_Drop'].mean():.2f} Â± {subset['Acc_Drop'].std():.2f}\",\n",
    "                \"Attack_Success\": f\"{subset['Attack_Success'].mean():.2f} Â± {subset['Attack_Success'].std():.2f}\"\n",
    "            })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# DETAILED PER-SEED TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\nðŸ“‹ DETAILED RESULTS (Per Seed)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "display_cols = [\"Seed\", \"Attack\", \"Epsilon\", \"Clean_Acc\", \"Robust_Acc\", \"Acc_Drop\", \"Attack_Success\"]\n",
    "print(df_results[display_cols].to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# KEY FINDINGS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\nðŸ”‘ KEY FINDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Best/worst robust accuracy under PGD-8/255\n",
    "if config.pgd_enabled:\n",
    "    pgd_8 = df_results[(df_results[\"Attack\"] == f\"PGD-{config.pgd_steps}\") & \n",
    "                       (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "    if len(pgd_8) > 0:\n",
    "        mean_robust = pgd_8[\"Robust_Acc\"].mean()\n",
    "        mean_drop = pgd_8[\"Acc_Drop\"].mean()\n",
    "        print(f\"â€¢ PGD-{config.pgd_steps} (Îµ=8/255): {mean_robust:.2f}% robust accuracy \"\n",
    "              f\"(â†“{mean_drop:.2f}pp from clean)\")\n",
    "\n",
    "# FGSM vs PGD comparison\n",
    "if config.fgsm_enabled and config.pgd_enabled:\n",
    "    fgsm_8 = df_results[(df_results[\"Attack\"] == \"FGSM\") & (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "    pgd_8 = df_results[(df_results[\"Attack\"] == f\"PGD-{config.pgd_steps}\") & \n",
    "                       (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "    if len(fgsm_8) > 0 and len(pgd_8) > 0:\n",
    "        fgsm_robust = fgsm_8[\"Robust_Acc\"].mean()\n",
    "        pgd_robust = pgd_8[\"Robust_Acc\"].mean()\n",
    "        print(f\"â€¢ FGSM vs PGD gap at Îµ=8/255: {abs(fgsm_robust - pgd_robust):.2f}pp\")\n",
    "\n",
    "# C&W results\n",
    "if config.cw_enabled:\n",
    "    cw_results = df_results[df_results[\"Attack\"] == \"C&W-L2\"]\n",
    "    if len(cw_results) > 0:\n",
    "        mean_cw_robust = cw_results[\"Robust_Acc\"].mean()\n",
    "        print(f\"â€¢ C&W L2 attack: {mean_cw_robust:.2f}% robust accuracy\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ebcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ“ˆ Cell 11: PhD-Level Visualization - Robustness Degradation Curves\n",
    "#@markdown **Publication-quality robustness vs perturbation strength plots**\n",
    "\n",
    "def create_robustness_curves(df_results: pd.DataFrame, config: EvaluationConfig) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create interactive robustness degradation curves.\n",
    "    \n",
    "    Shows how accuracy degrades with increasing perturbation budget Îµ.\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    attacks = [\"FGSM\", f\"PGD-{config.pgd_steps}\"]\n",
    "    colors = {\"FGSM\": \"#FF6B6B\", f\"PGD-{config.pgd_steps}\": \"#4ECDC4\"}\n",
    "    markers = {\"FGSM\": \"circle\", f\"PGD-{config.pgd_steps}\": \"square\"}\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\n",
    "            \"<b>Robustness Degradation by Attack Type</b>\",\n",
    "            \"<b>Accuracy Drop Severity</b>\"\n",
    "        ),\n",
    "        horizontal_spacing=0.12\n",
    "    )\n",
    "    \n",
    "    # Add clean accuracy reference line\n",
    "    clean_acc = df_results[\"Clean_Acc\"].mean()\n",
    "    \n",
    "    for attack in attacks:\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack].copy()\n",
    "        if len(attack_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Group by epsilon\n",
    "        grouped = attack_data.groupby(\"Epsilon_Val\").agg({\n",
    "            \"Robust_Acc\": [\"mean\", \"std\"],\n",
    "            \"Acc_Drop\": [\"mean\", \"std\"]\n",
    "        }).reset_index()\n",
    "        grouped.columns = [\"Epsilon\", \"Robust_Mean\", \"Robust_Std\", \"Drop_Mean\", \"Drop_Std\"]\n",
    "        grouped = grouped.sort_values(\"Epsilon\")\n",
    "        \n",
    "        # Convert epsilon to string labels for x-axis\n",
    "        epsilon_labels = [f\"{int(e*255)}/255\" for e in grouped[\"Epsilon\"]]\n",
    "        \n",
    "        # Plot 1: Robustness curves with confidence bands\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epsilon_labels,\n",
    "                y=grouped[\"Robust_Mean\"],\n",
    "                mode=\"lines+markers\",\n",
    "                name=attack,\n",
    "                line=dict(color=colors[attack], width=3),\n",
    "                marker=dict(size=12, symbol=markers[attack]),\n",
    "                error_y=dict(\n",
    "                    type=\"data\",\n",
    "                    array=grouped[\"Robust_Std\"],\n",
    "                    visible=True,\n",
    "                    color=colors[attack],\n",
    "                    thickness=2\n",
    "                ),\n",
    "                legendgroup=attack,\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Plot 2: Accuracy drop bars\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=epsilon_labels,\n",
    "                y=grouped[\"Drop_Mean\"],\n",
    "                name=attack,\n",
    "                marker_color=colors[attack],\n",
    "                error_y=dict(\n",
    "                    type=\"data\",\n",
    "                    array=grouped[\"Drop_Std\"],\n",
    "                    visible=True\n",
    "                ),\n",
    "                legendgroup=attack,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Add clean accuracy reference\n",
    "    fig.add_hline(\n",
    "        y=clean_acc, \n",
    "        line_dash=\"dash\", \n",
    "        line_color=\"gray\",\n",
    "        annotation_text=f\"Clean: {clean_acc:.1f}%\",\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=500,\n",
    "        width=1100,\n",
    "        title=dict(\n",
    "            text=\"<b>Adversarial Robustness Analysis: Baseline ResNet-50 on ISIC 2018</b>\",\n",
    "            font=dict(size=18),\n",
    "            x=0.5\n",
    "        ),\n",
    "        font=dict(family=\"Arial\", size=12),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.08,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        ),\n",
    "        paper_bgcolor=\"white\",\n",
    "        plot_bgcolor=\"white\"\n",
    "    )\n",
    "    \n",
    "    # Axis labels\n",
    "    fig.update_xaxes(title_text=\"Perturbation Budget (Îµ)\", row=1, col=1, gridcolor=\"lightgray\")\n",
    "    fig.update_xaxes(title_text=\"Perturbation Budget (Îµ)\", row=1, col=2, gridcolor=\"lightgray\")\n",
    "    fig.update_yaxes(title_text=\"Robust Accuracy (%)\", row=1, col=1, gridcolor=\"lightgray\", range=[0, 100])\n",
    "    fig.update_yaxes(title_text=\"Accuracy Drop (pp)\", row=1, col=2, gridcolor=\"lightgray\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display\n",
    "fig_robustness = create_robustness_curves(df_results, config)\n",
    "fig_robustness.show()\n",
    "\n",
    "# Save figure\n",
    "if paths.figures_dir.exists():\n",
    "    fig_robustness.write_html(paths.figures_dir / \"robustness_curves.html\")\n",
    "    fig_robustness.write_image(paths.figures_dir / \"robustness_curves.png\", scale=2)\n",
    "    print(f\"âœ… Saved to {paths.figures_dir / 'robustness_curves.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ”¥ Cell 12: PhD-Level Visualization - Per-Class Vulnerability Heatmap\n",
    "#@markdown **Detailed heatmap showing which skin lesion classes are most vulnerable**\n",
    "\n",
    "def create_vulnerability_heatmap(all_results: dict, clean_results: dict, config: EvaluationConfig) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create per-class vulnerability heatmap across attacks and epsilons.\n",
    "    \"\"\"\n",
    "    # Build vulnerability matrix\n",
    "    attack_configs = []\n",
    "    \n",
    "    # Add FGSM configs\n",
    "    if config.fgsm_enabled:\n",
    "        for eps in config.epsilons:\n",
    "            attack_configs.append((\"FGSM\", eps, f\"FGSM Îµ={config.get_epsilon_str(eps)}\"))\n",
    "    \n",
    "    # Add PGD configs\n",
    "    if config.pgd_enabled:\n",
    "        for eps in config.epsilons:\n",
    "            attack_configs.append((f\"PGD-{config.pgd_steps}\", eps, f\"PGD-{config.pgd_steps} Îµ={config.get_epsilon_str(eps)}\"))\n",
    "    \n",
    "    # Add C&W\n",
    "    if config.cw_enabled:\n",
    "        attack_configs.append((\"C&W-L2\", \"l2\", \"C&W L2\"))\n",
    "    \n",
    "    # Calculate mean per-class robust accuracy across seeds\n",
    "    vulnerability_matrix = []\n",
    "    \n",
    "    for class_name in CLASS_NAMES:\n",
    "        row = []\n",
    "        for attack_type, eps_key, label in attack_configs:\n",
    "            # Get attack dict key\n",
    "            attack_key = \"fgsm\" if attack_type == \"FGSM\" else (\"pgd\" if \"PGD\" in attack_type else \"cw\")\n",
    "            \n",
    "            accuracies = []\n",
    "            for seed in config.seeds:\n",
    "                if seed not in all_results[attack_key]:\n",
    "                    continue\n",
    "                if eps_key not in all_results[attack_key][seed]:\n",
    "                    continue\n",
    "                \n",
    "                result = all_results[attack_key][seed][eps_key]\n",
    "                if class_name in result.get(\"per_class_robust_accuracy\", {}):\n",
    "                    accuracies.append(result[\"per_class_robust_accuracy\"][class_name] * 100)\n",
    "            \n",
    "            if accuracies:\n",
    "                row.append(np.mean(accuracies))\n",
    "            else:\n",
    "                row.append(np.nan)\n",
    "        \n",
    "        vulnerability_matrix.append(row)\n",
    "    \n",
    "    vulnerability_matrix = np.array(vulnerability_matrix)\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=vulnerability_matrix,\n",
    "        x=[c[2] for c in attack_configs],\n",
    "        y=[f\"{name}\\n({CLASS_DESCRIPTIONS[name].split(' ')[0]})\" for name in CLASS_NAMES],\n",
    "        colorscale=[\n",
    "            [0, \"#d73027\"],      # Red - low accuracy (vulnerable)\n",
    "            [0.25, \"#fc8d59\"],   # Orange\n",
    "            [0.5, \"#fee08b\"],    # Yellow\n",
    "            [0.75, \"#91cf60\"],   # Light green\n",
    "            [1, \"#1a9850\"]       # Dark green - high accuracy (robust)\n",
    "        ],\n",
    "        colorbar=dict(\n",
    "            title=\"Robust<br>Accuracy (%)\",\n",
    "            titleside=\"right\",\n",
    "            ticksuffix=\"%\"\n",
    "        ),\n",
    "        text=np.round(vulnerability_matrix, 1),\n",
    "        texttemplate=\"%{text:.1f}%\",\n",
    "        textfont=dict(size=11, color=\"black\"),\n",
    "        hoverongaps=False,\n",
    "        hovertemplate=\"<b>%{y}</b><br>Attack: %{x}<br>Robust Acc: %{z:.2f}%<extra></extra>\"\n",
    "    ))\n",
    "    \n",
    "    # Add clean accuracy comparison as annotation column\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"<b>Per-Class Adversarial Vulnerability Analysis</b><br>\"\n",
    "                 \"<sup>Red = Vulnerable (low accuracy) | Green = Robust (high accuracy)</sup>\",\n",
    "            font=dict(size=16),\n",
    "            x=0.5\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Attack Configuration\",\n",
    "            tickangle=45,\n",
    "            side=\"bottom\"\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Skin Lesion Class\",\n",
    "            autorange=\"reversed\"\n",
    "        ),\n",
    "        height=550,\n",
    "        width=1000,\n",
    "        font=dict(family=\"Arial\", size=12),\n",
    "        paper_bgcolor=\"white\",\n",
    "        plot_bgcolor=\"white\"\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display\n",
    "fig_heatmap = create_vulnerability_heatmap(all_results, clean_results, config)\n",
    "fig_heatmap.show()\n",
    "\n",
    "# Save\n",
    "if paths.figures_dir.exists():\n",
    "    fig_heatmap.write_html(paths.figures_dir / \"vulnerability_heatmap.html\")\n",
    "    fig_heatmap.write_image(paths.figures_dir / \"vulnerability_heatmap.png\", scale=2)\n",
    "    print(f\"âœ… Saved to {paths.figures_dir / 'vulnerability_heatmap.png'}\")\n",
    "\n",
    "# ============================================================================\n",
    "# IDENTIFY MOST VULNERABLE CLASSES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸŽ¯ CLASS VULNERABILITY RANKING (Under PGD-40 Îµ=8/255)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Get PGD-40 at 8/255 per-class results\n",
    "eps_8 = 8/255\n",
    "class_vuln = {}\n",
    "\n",
    "for seed in config.seeds:\n",
    "    if seed not in all_results[\"pgd\"]:\n",
    "        continue\n",
    "    if eps_8 not in all_results[\"pgd\"][seed]:\n",
    "        continue\n",
    "    \n",
    "    result = all_results[\"pgd\"][seed][eps_8]\n",
    "    for class_name, acc in result.get(\"per_class_robust_accuracy\", {}).items():\n",
    "        if class_name not in class_vuln:\n",
    "            class_vuln[class_name] = []\n",
    "        class_vuln[class_name].append(acc * 100)\n",
    "\n",
    "# Sort by vulnerability (lowest accuracy = most vulnerable)\n",
    "sorted_vuln = sorted(\n",
    "    [(name, np.mean(accs), np.std(accs)) for name, accs in class_vuln.items()],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "\n",
    "for rank, (name, mean_acc, std_acc) in enumerate(sorted_vuln, 1):\n",
    "    status = \"ðŸ”´ CRITICAL\" if mean_acc < 20 else \"ðŸŸ¡ MODERATE\" if mean_acc < 50 else \"ðŸŸ¢ ROBUST\"\n",
    "    print(f\"{rank}. {name:6} ({CLASS_DESCRIPTIONS[name]:30}): {mean_acc:5.1f}% Â± {std_acc:4.1f}% {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a849c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸŽ¯ Cell 13: PhD-Level Visualization - Clean vs Adversarial Confusion Matrices\n",
    "#@markdown **Side-by-side confusion matrices showing attack impact on predictions**\n",
    "\n",
    "def create_confusion_matrix_comparison(\n",
    "    clean_results: dict, \n",
    "    all_results: dict, \n",
    "    seed: int,\n",
    "    attack_key: str = \"pgd\",\n",
    "    epsilon: float = 8/255\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create side-by-side confusion matrices for clean vs adversarial.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Get confusion matrices\n",
    "    cm_clean = clean_results[seed][\"confusion_matrix\"]\n",
    "    \n",
    "    # Get adversarial CM\n",
    "    if attack_key == \"cw\":\n",
    "        cm_adv = all_results[attack_key][seed][\"l2\"][\"confusion_matrix\"]\n",
    "        attack_label = \"C&W L2\"\n",
    "    else:\n",
    "        cm_adv = all_results[attack_key][seed][epsilon][\"confusion_matrix\"]\n",
    "        eps_str = f\"{int(epsilon*255)}/255\"\n",
    "        attack_label = f\"PGD-{config.pgd_steps} (Îµ={eps_str})\" if attack_key == \"pgd\" else f\"FGSM (Îµ={eps_str})\"\n",
    "    \n",
    "    # Normalize confusion matrices\n",
    "    cm_clean_norm = cm_clean.astype(float) / cm_clean.sum(axis=1, keepdims=True)\n",
    "    cm_adv_norm = cm_adv.astype(float) / cm_adv.sum(axis=1, keepdims=True)\n",
    "    cm_diff = cm_adv_norm - cm_clean_norm\n",
    "    \n",
    "    # Plot 1: Clean CM\n",
    "    im1 = axes[0].imshow(cm_clean_norm, cmap=\"Blues\", vmin=0, vmax=1)\n",
    "    axes[0].set_title(f\"Clean Predictions\\n(Seed {seed})\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[0].set_xlabel(\"Predicted\")\n",
    "    axes[0].set_ylabel(\"True Label\")\n",
    "    axes[0].set_xticks(range(len(CLASS_NAMES)))\n",
    "    axes[0].set_yticks(range(len(CLASS_NAMES)))\n",
    "    axes[0].set_xticklabels(CLASS_NAMES, rotation=45, ha=\"right\")\n",
    "    axes[0].set_yticklabels(CLASS_NAMES)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(CLASS_NAMES)):\n",
    "        for j in range(len(CLASS_NAMES)):\n",
    "            val = cm_clean_norm[i, j]\n",
    "            color = \"white\" if val > 0.5 else \"black\"\n",
    "            axes[0].text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
    "    \n",
    "    plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Plot 2: Adversarial CM\n",
    "    im2 = axes[1].imshow(cm_adv_norm, cmap=\"Reds\", vmin=0, vmax=1)\n",
    "    axes[1].set_title(f\"Under {attack_label}\\n(Seed {seed})\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[1].set_xlabel(\"Predicted\")\n",
    "    axes[1].set_ylabel(\"True Label\")\n",
    "    axes[1].set_xticks(range(len(CLASS_NAMES)))\n",
    "    axes[1].set_yticks(range(len(CLASS_NAMES)))\n",
    "    axes[1].set_xticklabels(CLASS_NAMES, rotation=45, ha=\"right\")\n",
    "    axes[1].set_yticklabels(CLASS_NAMES)\n",
    "    \n",
    "    for i in range(len(CLASS_NAMES)):\n",
    "        for j in range(len(CLASS_NAMES)):\n",
    "            val = cm_adv_norm[i, j]\n",
    "            color = \"white\" if val > 0.5 else \"black\"\n",
    "            axes[1].text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
    "    \n",
    "    plt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Plot 3: Difference (shows where predictions shifted)\n",
    "    im3 = axes[2].imshow(cm_diff, cmap=\"RdBu_r\", vmin=-0.5, vmax=0.5)\n",
    "    axes[2].set_title(f\"Prediction Shift\\n(Adversarial - Clean)\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[2].set_xlabel(\"Predicted\")\n",
    "    axes[2].set_ylabel(\"True Label\")\n",
    "    axes[2].set_xticks(range(len(CLASS_NAMES)))\n",
    "    axes[2].set_yticks(range(len(CLASS_NAMES)))\n",
    "    axes[2].set_xticklabels(CLASS_NAMES, rotation=45, ha=\"right\")\n",
    "    axes[2].set_yticklabels(CLASS_NAMES)\n",
    "    \n",
    "    for i in range(len(CLASS_NAMES)):\n",
    "        for j in range(len(CLASS_NAMES)):\n",
    "            val = cm_diff[i, j]\n",
    "            color = \"white\" if abs(val) > 0.25 else \"black\"\n",
    "            axes[2].text(j, i, f\"{val:+.2f}\", ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
    "    \n",
    "    cbar = plt.colorbar(im3, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(\"Change in probability\", rotation=270, labelpad=15)\n",
    "    \n",
    "    plt.suptitle(\n",
    "        f\"Confusion Matrix Analysis: Impact of {attack_label} Attack\",\n",
    "        fontsize=16, fontweight=\"bold\", y=1.02\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create confusion matrix for first available seed\n",
    "first_seed = config.seeds[0]\n",
    "if first_seed in clean_results and first_seed in all_results.get(\"pgd\", {}):\n",
    "    fig_cm = create_confusion_matrix_comparison(\n",
    "        clean_results, all_results, \n",
    "        seed=first_seed,\n",
    "        attack_key=\"pgd\",\n",
    "        epsilon=8/255\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Save\n",
    "    if paths.figures_dir.exists():\n",
    "        fig_cm.savefig(paths.figures_dir / \"confusion_matrix_comparison.png\", \n",
    "                       dpi=200, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "        print(f\"âœ… Saved to {paths.figures_dir / 'confusion_matrix_comparison.png'}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Results not available for confusion matrix visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52942c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ•¸ï¸ Cell 14: PhD-Level Visualization - Radar Chart & Attack Effectiveness\n",
    "#@markdown **Multi-dimensional attack comparison using radar charts**\n",
    "\n",
    "def create_radar_chart(all_results: dict, clean_results: dict, config: EvaluationConfig) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create radar chart comparing attack effectiveness across multiple dimensions.\n",
    "    \"\"\"\n",
    "    # Calculate mean metrics across seeds for strongest epsilon\n",
    "    eps_8 = 8/255\n",
    "    \n",
    "    metrics = {\n",
    "        \"FGSM\": {},\n",
    "        f\"PGD-{config.pgd_steps}\": {},\n",
    "        \"C&W-L2\": {}\n",
    "    }\n",
    "    \n",
    "    dimensions = [\n",
    "        \"Attack Success Rate\",\n",
    "        \"Accuracy Drop\",\n",
    "        \"Per-Class Variance\",\n",
    "        \"Mean Perturbation\",\n",
    "        \"Speed (inverse)\"\n",
    "    ]\n",
    "    \n",
    "    # Calculate FGSM metrics\n",
    "    if config.fgsm_enabled:\n",
    "        fgsm_results = [all_results[\"fgsm\"][s][eps_8] for s in config.seeds if s in all_results[\"fgsm\"] and eps_8 in all_results[\"fgsm\"][s]]\n",
    "        if fgsm_results:\n",
    "            metrics[\"FGSM\"] = {\n",
    "                \"Attack Success Rate\": np.mean([r[\"attack_success_rate\"] for r in fgsm_results]) * 100,\n",
    "                \"Accuracy Drop\": np.mean([r[\"accuracy_drop\"] for r in fgsm_results]) * 100,\n",
    "                \"Per-Class Variance\": np.mean([np.std(list(r[\"per_class_robust_accuracy\"].values())) for r in fgsm_results]) * 100,\n",
    "                \"Mean Perturbation\": np.mean([r[\"mean_linf_dist\"] for r in fgsm_results]) * 255,  # Scale to 0-8\n",
    "                \"Speed (inverse)\": 95  # FGSM is single-step, very fast\n",
    "            }\n",
    "    \n",
    "    # Calculate PGD metrics\n",
    "    if config.pgd_enabled:\n",
    "        pgd_results = [all_results[\"pgd\"][s][eps_8] for s in config.seeds if s in all_results[\"pgd\"] and eps_8 in all_results[\"pgd\"][s]]\n",
    "        if pgd_results:\n",
    "            metrics[f\"PGD-{config.pgd_steps}\"] = {\n",
    "                \"Attack Success Rate\": np.mean([r[\"attack_success_rate\"] for r in pgd_results]) * 100,\n",
    "                \"Accuracy Drop\": np.mean([r[\"accuracy_drop\"] for r in pgd_results]) * 100,\n",
    "                \"Per-Class Variance\": np.mean([np.std(list(r[\"per_class_robust_accuracy\"].values())) for r in pgd_results]) * 100,\n",
    "                \"Mean Perturbation\": np.mean([r[\"mean_linf_dist\"] for r in pgd_results]) * 255,\n",
    "                \"Speed (inverse)\": 40  # PGD-40 is 40x slower than FGSM\n",
    "            }\n",
    "    \n",
    "    # Calculate C&W metrics\n",
    "    if config.cw_enabled:\n",
    "        cw_results = [all_results[\"cw\"][s][\"l2\"] for s in config.seeds if s in all_results[\"cw\"] and \"l2\" in all_results[\"cw\"][s]]\n",
    "        if cw_results:\n",
    "            metrics[\"C&W-L2\"] = {\n",
    "                \"Attack Success Rate\": np.mean([r[\"attack_success_rate\"] for r in cw_results]) * 100,\n",
    "                \"Accuracy Drop\": np.mean([r[\"accuracy_drop\"] for r in cw_results]) * 100,\n",
    "                \"Per-Class Variance\": np.mean([np.std(list(r[\"per_class_robust_accuracy\"].values())) for r in cw_results]) * 100,\n",
    "                \"Mean Perturbation\": np.mean([r[\"mean_l2_dist\"] for r in cw_results]) * 10,  # Scale L2 for visibility\n",
    "                \"Speed (inverse)\": 10  # C&W is slowest\n",
    "            }\n",
    "    \n",
    "    # Create radar chart\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    colors = {\"FGSM\": \"#FF6B6B\", f\"PGD-{config.pgd_steps}\": \"#4ECDC4\", \"C&W-L2\": \"#9B59B6\"}\n",
    "    \n",
    "    for attack_name, data in metrics.items():\n",
    "        if not data:\n",
    "            continue\n",
    "        \n",
    "        values = [data.get(dim, 0) for dim in dimensions]\n",
    "        values.append(values[0])  # Close the polygon\n",
    "        \n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=dimensions + [dimensions[0]],\n",
    "            fill='toself',\n",
    "            name=attack_name,\n",
    "            line=dict(color=colors[attack_name], width=2),\n",
    "            fillcolor=colors[attack_name],\n",
    "            opacity=0.3\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 100],\n",
    "                ticksuffix=\"%\"\n",
    "            ),\n",
    "            angularaxis=dict(\n",
    "                tickfont=dict(size=12)\n",
    "            )\n",
    "        ),\n",
    "        title=dict(\n",
    "            text=\"<b>Attack Effectiveness Comparison (Radar Chart)</b><br>\"\n",
    "                 \"<sup>Higher values = more effective/impactful attack</sup>\",\n",
    "            font=dict(size=16),\n",
    "            x=0.5\n",
    "        ),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=-0.2,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        ),\n",
    "        height=600,\n",
    "        width=700\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create radar chart\n",
    "fig_radar = create_radar_chart(all_results, clean_results, config)\n",
    "fig_radar.show()\n",
    "\n",
    "# Save\n",
    "if paths.figures_dir.exists():\n",
    "    fig_radar.write_html(paths.figures_dir / \"attack_radar_chart.html\")\n",
    "    fig_radar.write_image(paths.figures_dir / \"attack_radar_chart.png\", scale=2)\n",
    "    print(f\"âœ… Saved to {paths.figures_dir / 'attack_radar_chart.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc0f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ–¼ï¸ Cell 15: PhD-Level Visualization - Adversarial Example Gallery\n",
    "#@markdown **Visualize clean images, perturbations, and adversarial examples**\n",
    "\n",
    "def visualize_adversarial_examples(\n",
    "    all_results: dict,\n",
    "    attack_key: str = \"pgd\",\n",
    "    epsilon: float = 8/255,\n",
    "    num_examples: int = 5,\n",
    "    amplification: float = 10.0\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create publication-quality adversarial example visualization.\n",
    "    \n",
    "    Shows:\n",
    "    - Clean image\n",
    "    - Perturbation (amplified for visibility)\n",
    "    - Adversarial image\n",
    "    - Prediction change\n",
    "    \"\"\"\n",
    "    # Get saved examples from first seed\n",
    "    first_seed = config.seeds[0]\n",
    "    \n",
    "    if attack_key == \"cw\":\n",
    "        examples = all_results[attack_key][first_seed][\"l2\"].get(\"saved_examples\", [])\n",
    "    else:\n",
    "        examples = all_results[attack_key][first_seed][epsilon].get(\"saved_examples\", [])\n",
    "    \n",
    "    if not examples:\n",
    "        print(\"âš ï¸ No saved examples available\")\n",
    "        return None\n",
    "    \n",
    "    # Select successful attacks (prediction changed)\n",
    "    successful = [e for e in examples if e[\"clean_pred\"] != e[\"adv_pred\"]]\n",
    "    if len(successful) < num_examples:\n",
    "        successful = examples  # Use all if not enough successful\n",
    "    \n",
    "    num_examples = min(num_examples, len(successful))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_examples, 4, figsize=(16, 4 * num_examples))\n",
    "    if num_examples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, example in enumerate(successful[:num_examples]):\n",
    "        # Get tensors\n",
    "        clean = example[\"clean\"].numpy().transpose(1, 2, 0)  # CHW -> HWC\n",
    "        adv = example[\"adversarial\"].numpy().transpose(1, 2, 0)\n",
    "        perturbation = example[\"perturbation\"].numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        true_label = CLASS_NAMES[example[\"true_label\"]]\n",
    "        clean_pred = CLASS_NAMES[example[\"clean_pred\"]]\n",
    "        adv_pred = CLASS_NAMES[example[\"adv_pred\"]]\n",
    "        \n",
    "        # Ensure valid range\n",
    "        clean = np.clip(clean, 0, 1)\n",
    "        adv = np.clip(adv, 0, 1)\n",
    "        \n",
    "        # Amplify perturbation for visibility\n",
    "        pert_amplified = perturbation * amplification + 0.5\n",
    "        pert_amplified = np.clip(pert_amplified, 0, 1)\n",
    "        \n",
    "        # L2 and Linf norms\n",
    "        l2_norm = np.sqrt(np.sum(perturbation ** 2))\n",
    "        linf_norm = np.max(np.abs(perturbation))\n",
    "        \n",
    "        # Plot clean image\n",
    "        axes[idx, 0].imshow(clean)\n",
    "        axes[idx, 0].set_title(f\"Clean Image\\nTrue: {true_label}\\nPred: {clean_pred}\", fontsize=11)\n",
    "        axes[idx, 0].axis(\"off\")\n",
    "        if example[\"clean_pred\"] == example[\"true_label\"]:\n",
    "            axes[idx, 0].spines[:].set_visible(True)\n",
    "            for spine in axes[idx, 0].spines.values():\n",
    "                spine.set_edgecolor('green')\n",
    "                spine.set_linewidth(3)\n",
    "        \n",
    "        # Plot perturbation\n",
    "        axes[idx, 1].imshow(pert_amplified)\n",
    "        axes[idx, 1].set_title(f\"Perturbation (Ã—{amplification:.0f})\\nLâˆž: {linf_norm*255:.2f}/255\\nL2: {l2_norm:.4f}\", fontsize=11)\n",
    "        axes[idx, 1].axis(\"off\")\n",
    "        \n",
    "        # Plot adversarial image\n",
    "        axes[idx, 2].imshow(adv)\n",
    "        axes[idx, 2].set_title(f\"Adversarial Image\\nPred: {adv_pred}\", fontsize=11)\n",
    "        axes[idx, 2].axis(\"off\")\n",
    "        if example[\"adv_pred\"] != example[\"true_label\"]:\n",
    "            for spine in axes[idx, 2].spines.values():\n",
    "                spine.set_visible(True)\n",
    "                spine.set_edgecolor('red')\n",
    "                spine.set_linewidth(3)\n",
    "        \n",
    "        # Plot difference heatmap\n",
    "        diff = np.mean(np.abs(adv - clean), axis=2)  # Average across channels\n",
    "        im = axes[idx, 3].imshow(diff, cmap=\"hot\", vmin=0, vmax=epsilon * 2)\n",
    "        axes[idx, 3].set_title(f\"Absolute Difference\\n({clean_pred} â†’ {adv_pred})\", fontsize=11)\n",
    "        axes[idx, 3].axis(\"off\")\n",
    "        plt.colorbar(im, ax=axes[idx, 3], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    attack_label = \"C&W L2\" if attack_key == \"cw\" else f\"{'PGD' if attack_key == 'pgd' else 'FGSM'}-{config.get_epsilon_str(epsilon)}\"\n",
    "    plt.suptitle(\n",
    "        f\"Adversarial Example Gallery: {attack_label} Attack\\n\"\n",
    "        f\"Green border = correct, Red border = misclassified\",\n",
    "        fontsize=16, fontweight=\"bold\", y=1.02\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create visualization for PGD attack\n",
    "if config.pgd_enabled and config.seeds[0] in all_results.get(\"pgd\", {}):\n",
    "    fig_examples = visualize_adversarial_examples(\n",
    "        all_results,\n",
    "        attack_key=\"pgd\",\n",
    "        epsilon=8/255,\n",
    "        num_examples=5,\n",
    "        amplification=10.0\n",
    "    )\n",
    "    if fig_examples:\n",
    "        plt.show()\n",
    "        \n",
    "        # Save\n",
    "        if paths.figures_dir.exists():\n",
    "            fig_examples.savefig(paths.figures_dir / \"adversarial_examples_pgd.png\",\n",
    "                                 dpi=200, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "            print(f\"âœ… Saved to {paths.figures_dir / 'adversarial_examples_pgd.png'}\")\n",
    "\n",
    "# Also show FGSM examples if available\n",
    "if config.fgsm_enabled and config.seeds[0] in all_results.get(\"fgsm\", {}):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“¸ FGSM Attack Examples\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    fig_fgsm = visualize_adversarial_examples(\n",
    "        all_results,\n",
    "        attack_key=\"fgsm\",\n",
    "        epsilon=8/255,\n",
    "        num_examples=3,\n",
    "        amplification=10.0\n",
    "    )\n",
    "    if fig_fgsm:\n",
    "        plt.show()\n",
    "        \n",
    "        if paths.figures_dir.exists():\n",
    "            fig_fgsm.savefig(paths.figures_dir / \"adversarial_examples_fgsm.png\",\n",
    "                            dpi=200, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "            print(f\"âœ… Saved to {paths.figures_dir / 'adversarial_examples_fgsm.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bbbb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ“Š Cell 16: PhD-Level Analysis - Statistical Significance & Seed Consistency\n",
    "#@markdown **Bootstrap confidence intervals and cross-seed consistency analysis**\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def statistical_analysis(df_results: pd.DataFrame, config: EvaluationConfig) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform rigorous statistical analysis of adversarial evaluation results.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ“Š STATISTICAL ANALYSIS OF ADVERSARIAL ROBUSTNESS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Cross-seed consistency (Coefficient of Variation)\n",
    "    print(\"\\n1ï¸âƒ£ CROSS-SEED CONSISTENCY (Coefficient of Variation)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        for eps in df_results[df_results[\"Attack\"] == attack][\"Epsilon\"].unique():\n",
    "            subset = df_results[(df_results[\"Attack\"] == attack) & (df_results[\"Epsilon\"] == eps)]\n",
    "            if len(subset) >= 2:\n",
    "                mean_acc = subset[\"Robust_Acc\"].mean()\n",
    "                std_acc = subset[\"Robust_Acc\"].std()\n",
    "                cv = (std_acc / mean_acc) * 100 if mean_acc > 0 else 0\n",
    "                \n",
    "                consistency = \"âœ… High\" if cv < 5 else \"âš ï¸ Medium\" if cv < 10 else \"âŒ Low\"\n",
    "                print(f\"   {attack:15} {eps:>7}: CV = {cv:5.2f}% {consistency}\")\n",
    "    \n",
    "    # 2. Bootstrap 95% Confidence Intervals\n",
    "    print(\"\\n2ï¸âƒ£ BOOTSTRAP 95% CONFIDENCE INTERVALS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    def bootstrap_ci(data, n_bootstrap=1000, ci=0.95):\n",
    "        \"\"\"Calculate bootstrap confidence interval.\"\"\"\n",
    "        if len(data) < 2:\n",
    "            return data.mean(), data.mean(), data.mean()\n",
    "        \n",
    "        bootstraps = []\n",
    "        for _ in range(n_bootstrap):\n",
    "            sample = np.random.choice(data, size=len(data), replace=True)\n",
    "            bootstraps.append(sample.mean())\n",
    "        \n",
    "        lower = np.percentile(bootstraps, (1 - ci) / 2 * 100)\n",
    "        upper = np.percentile(bootstraps, (1 + ci) / 2 * 100)\n",
    "        return data.mean(), lower, upper\n",
    "    \n",
    "    ci_results = []\n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack]\n",
    "        \n",
    "        if attack == \"C&W-L2\":\n",
    "            subset = attack_data\n",
    "            mean, lower, upper = bootstrap_ci(subset[\"Robust_Acc\"].values)\n",
    "            ci_results.append({\n",
    "                \"Attack\": attack,\n",
    "                \"Epsilon\": \"N/A\",\n",
    "                \"Mean\": mean,\n",
    "                \"CI_Lower\": lower,\n",
    "                \"CI_Upper\": upper\n",
    "            })\n",
    "            print(f\"   {attack:15} {'N/A':>7}: {mean:5.2f}% [{lower:5.2f}%, {upper:5.2f}%]\")\n",
    "        else:\n",
    "            for eps in attack_data[\"Epsilon\"].unique():\n",
    "                subset = attack_data[attack_data[\"Epsilon\"] == eps]\n",
    "                mean, lower, upper = bootstrap_ci(subset[\"Robust_Acc\"].values)\n",
    "                ci_results.append({\n",
    "                    \"Attack\": attack,\n",
    "                    \"Epsilon\": eps,\n",
    "                    \"Mean\": mean,\n",
    "                    \"CI_Lower\": lower,\n",
    "                    \"CI_Upper\": upper\n",
    "                })\n",
    "                print(f\"   {attack:15} {eps:>7}: {mean:5.2f}% [{lower:5.2f}%, {upper:5.2f}%]\")\n",
    "    \n",
    "    results[\"confidence_intervals\"] = ci_results\n",
    "    \n",
    "    # 3. Attack Comparison (Paired t-test: FGSM vs PGD)\n",
    "    print(\"\\n3ï¸âƒ£ ATTACK COMPARISON (Paired t-test at Îµ=8/255)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if config.fgsm_enabled and config.pgd_enabled:\n",
    "        eps_8 = \"8/255\"\n",
    "        fgsm_acc = df_results[(df_results[\"Attack\"] == \"FGSM\") & (df_results[\"Epsilon\"] == eps_8)][\"Robust_Acc\"].values\n",
    "        pgd_acc = df_results[(df_results[\"Attack\"] == f\"PGD-{config.pgd_steps}\") & (df_results[\"Epsilon\"] == eps_8)][\"Robust_Acc\"].values\n",
    "        \n",
    "        if len(fgsm_acc) >= 2 and len(pgd_acc) >= 2 and len(fgsm_acc) == len(pgd_acc):\n",
    "            t_stat, p_value = stats.ttest_rel(fgsm_acc, pgd_acc)\n",
    "            \n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "            \n",
    "            print(f\"   FGSM mean: {fgsm_acc.mean():.2f}%\")\n",
    "            print(f\"   PGD mean:  {pgd_acc.mean():.2f}%\")\n",
    "            print(f\"   Difference: {(fgsm_acc.mean() - pgd_acc.mean()):.2f}pp\")\n",
    "            print(f\"   t-statistic: {t_stat:.3f}\")\n",
    "            print(f\"   p-value: {p_value:.4f} {significance}\")\n",
    "            \n",
    "            results[\"fgsm_vs_pgd\"] = {\n",
    "                \"t_statistic\": t_stat,\n",
    "                \"p_value\": p_value,\n",
    "                \"significant\": p_value < 0.05\n",
    "            }\n",
    "    \n",
    "    # 4. Effect Size (Cohen's d)\n",
    "    print(\"\\n4ï¸âƒ£ EFFECT SIZE (Cohen's d): Clean vs Adversarial\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack]\n",
    "        clean_acc = attack_data[\"Clean_Acc\"].values\n",
    "        robust_acc = attack_data[\"Robust_Acc\"].values\n",
    "        \n",
    "        # Cohen's d\n",
    "        pooled_std = np.sqrt((clean_acc.std()**2 + robust_acc.std()**2) / 2)\n",
    "        if pooled_std > 0:\n",
    "            cohens_d = (clean_acc.mean() - robust_acc.mean()) / pooled_std\n",
    "        else:\n",
    "            cohens_d = 0\n",
    "        \n",
    "        effect = \"Negligible\" if abs(cohens_d) < 0.2 else \"Small\" if abs(cohens_d) < 0.5 else \"Medium\" if abs(cohens_d) < 0.8 else \"Large\"\n",
    "        print(f\"   {attack:15}: d = {cohens_d:6.2f} ({effect})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run statistical analysis\n",
    "stat_results = statistical_analysis(df_results, config)\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION: SEED CONSISTENCY BOX PLOT\n",
    "# ============================================================================\n",
    "\n",
    "def create_seed_comparison_boxplot(df_results: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"Create box plot comparing robust accuracy across seeds.\"\"\"\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    colors = {\"FGSM\": \"#FF6B6B\", f\"PGD-{config.pgd_steps}\": \"#4ECDC4\", \"C&W-L2\": \"#9B59B6\"}\n",
    "    \n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack]\n",
    "        \n",
    "        fig.add_trace(go.Box(\n",
    "            y=attack_data[\"Robust_Acc\"],\n",
    "            x=[attack] * len(attack_data),\n",
    "            name=attack,\n",
    "            marker_color=colors.get(attack, \"#888888\"),\n",
    "            boxpoints=\"all\",\n",
    "            jitter=0.3,\n",
    "            pointpos=-1.8,\n",
    "            hovertemplate=\"Seed: %{text}<br>Robust Acc: %{y:.2f}%<extra></extra>\",\n",
    "            text=attack_data[\"Seed\"].astype(str)\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"<b>Cross-Seed Consistency: Robust Accuracy Distribution</b>\",\n",
    "            font=dict(size=16),\n",
    "            x=0.5\n",
    "        ),\n",
    "        xaxis_title=\"Attack Type\",\n",
    "        yaxis_title=\"Robust Accuracy (%)\",\n",
    "        yaxis=dict(range=[0, 100]),\n",
    "        showlegend=False,\n",
    "        height=500,\n",
    "        width=800\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig_boxplot = create_seed_comparison_boxplot(df_results)\n",
    "fig_boxplot.show()\n",
    "\n",
    "# Save\n",
    "if paths.figures_dir.exists():\n",
    "    fig_boxplot.write_html(paths.figures_dir / \"seed_consistency_boxplot.html\")\n",
    "    fig_boxplot.write_image(paths.figures_dir / \"seed_consistency_boxplot.png\", scale=2)\n",
    "    print(f\"âœ… Saved to {paths.figures_dir / 'seed_consistency_boxplot.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0bf83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ’¾ Cell 17: Save Results & Export for Dissertation\n",
    "#@markdown **Export all results in multiple formats for dissertation use**\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ’¾ SAVING RESULTS & EXPORTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS AS CSV\n",
    "# ============================================================================\n",
    "\n",
    "# Detailed results CSV\n",
    "csv_path = paths.results_dir / \"adversarial_robustness_results.csv\"\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"âœ… Detailed results: {csv_path}\")\n",
    "\n",
    "# Summary statistics CSV  \n",
    "summary_path = paths.results_dir / \"adversarial_robustness_summary.csv\"\n",
    "df_summary.to_csv(summary_path, index=False)\n",
    "print(f\"âœ… Summary statistics: {summary_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE AS JSON (for programmatic access)\n",
    "# ============================================================================\n",
    "\n",
    "json_results = {\n",
    "    \"metadata\": {\n",
    "        \"evaluation_date\": datetime.now().isoformat(),\n",
    "        \"seeds\": config.seeds,\n",
    "        \"epsilons\": [float(e) for e in config.epsilons],\n",
    "        \"attacks\": {\n",
    "            \"fgsm\": config.fgsm_enabled,\n",
    "            \"pgd\": config.pgd_enabled,\n",
    "            \"pgd_steps\": config.pgd_steps,\n",
    "            \"cw\": config.cw_enabled,\n",
    "            \"cw_iterations\": config.cw_iterations\n",
    "        },\n",
    "        \"dataset\": \"ISIC 2018\",\n",
    "        \"model\": \"ResNet-50\",\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    },\n",
    "    \"clean_accuracy\": {\n",
    "        str(seed): {\n",
    "            \"accuracy\": float(clean_results[seed][\"accuracy\"]),\n",
    "            \"balanced_accuracy\": float(clean_results[seed][\"balanced_accuracy\"]),\n",
    "            \"f1_macro\": float(clean_results[seed][\"f1_macro\"]),\n",
    "            \"auroc\": float(clean_results[seed][\"auroc\"])\n",
    "        }\n",
    "        for seed in config.seeds if seed in clean_results\n",
    "    },\n",
    "    \"adversarial_results\": df_results.to_dict(orient=\"records\")\n",
    "}\n",
    "\n",
    "json_path = paths.results_dir / \"adversarial_robustness_results.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(json_results, f, indent=2, default=str)\n",
    "print(f\"âœ… JSON results: {json_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATE LATEX TABLE FOR DISSERTATION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_latex_table(df_results: pd.DataFrame, config: EvaluationConfig) -> str:\n",
    "    \"\"\"Generate LaTeX table for dissertation.\"\"\"\n",
    "    \n",
    "    lines = [\n",
    "        \"\\\\begin{table}[htbp]\",\n",
    "        \"\\\\centering\",\n",
    "        \"\\\\caption{Adversarial Robustness Evaluation: Baseline ResNet-50 on ISIC 2018}\",\n",
    "        \"\\\\label{tab:adversarial_robustness}\",\n",
    "        \"\\\\begin{tabular}{llcccc}\",\n",
    "        \"\\\\toprule\",\n",
    "        \"Attack & $\\\\epsilon$ & Clean Acc (\\\\%) & Robust Acc (\\\\%) & Acc Drop (pp) & ASR (\\\\%) \\\\\\\\\",\n",
    "        \"\\\\midrule\"\n",
    "    ]\n",
    "    \n",
    "    # Group and average across seeds\n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack]\n",
    "        \n",
    "        for eps in attack_data[\"Epsilon\"].unique():\n",
    "            subset = attack_data[attack_data[\"Epsilon\"] == eps]\n",
    "            \n",
    "            clean = f\"{subset['Clean_Acc'].mean():.1f} $\\\\pm$ {subset['Clean_Acc'].std():.1f}\"\n",
    "            robust = f\"{subset['Robust_Acc'].mean():.1f} $\\\\pm$ {subset['Robust_Acc'].std():.1f}\"\n",
    "            drop = f\"{subset['Acc_Drop'].mean():.1f} $\\\\pm$ {subset['Acc_Drop'].std():.1f}\"\n",
    "            asr = f\"{subset['Attack_Success'].mean():.1f} $\\\\pm$ {subset['Attack_Success'].std():.1f}\"\n",
    "            \n",
    "            lines.append(f\"{attack} & {eps} & {clean} & {robust} & {drop} & {asr} \\\\\\\\\")\n",
    "    \n",
    "    lines.extend([\n",
    "        \"\\\\bottomrule\",\n",
    "        \"\\\\end{tabular}\",\n",
    "        \"\\\\begin{tablenotes}\",\n",
    "        \"\\\\small\",\n",
    "        \"\\\\item Note: Values are mean $\\\\pm$ std across seeds (42, 123, 456). ASR = Attack Success Rate.\",\n",
    "        \"\\\\end{tablenotes}\",\n",
    "        \"\\\\end{table}\"\n",
    "    ])\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "latex_table = generate_latex_table(df_results, config)\n",
    "\n",
    "# Save LaTeX table\n",
    "latex_path = paths.results_dir / \"adversarial_robustness_table.tex\"\n",
    "with open(latex_path, \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "print(f\"âœ… LaTeX table: {latex_path}\")\n",
    "\n",
    "# Print LaTeX table\n",
    "print(\"\\nðŸ“„ LATEX TABLE FOR DISSERTATION:\")\n",
    "print(\"-\" * 60)\n",
    "print(latex_table)\n",
    "\n",
    "# ============================================================================\n",
    "# LIST ALL SAVED FILES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“ ALL SAVED FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if paths.results_dir.exists():\n",
    "    for f in sorted(paths.results_dir.rglob(\"*\")):\n",
    "        if f.is_file():\n",
    "            size_kb = f.stat().st_size / 1024\n",
    "            print(f\"   {f.relative_to(paths.results_dir)} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ðŸ“‹ Cell 18: Executive Summary & Key Findings\n",
    "#@markdown **Final summary of adversarial robustness evaluation**\n",
    "\n",
    "def print_executive_summary(df_results: pd.DataFrame, clean_results: dict, config: EvaluationConfig):\n",
    "    \"\"\"Generate executive summary of evaluation results.\"\"\"\n",
    "    \n",
    "    print(\"â•”\" + \"â•\"*68 + \"â•—\")\n",
    "    print(\"â•‘\" + \" \"*20 + \"EXECUTIVE SUMMARY\" + \" \"*31 + \"â•‘\")\n",
    "    print(\"â•‘\" + \" \"*10 + \"Phase 4: Adversarial Robustness Evaluation\" + \" \"*15 + \"â•‘\")\n",
    "    print(\"â•š\" + \"â•\"*68 + \"â•\")\n",
    "    \n",
    "    # Clean accuracy\n",
    "    mean_clean = np.mean([r[\"accuracy\"] for r in clean_results.values()]) * 100\n",
    "    std_clean = np.std([r[\"accuracy\"] for r in clean_results.values()]) * 100\n",
    "    \n",
    "    print(f\"\\nðŸ“Š BASELINE CLEAN PERFORMANCE\")\n",
    "    print(f\"   â€¢ Clean Accuracy: {mean_clean:.2f}% Â± {std_clean:.2f}%\")\n",
    "    print(f\"   â€¢ Model: ResNet-50 (ImageNet pretrained)\")\n",
    "    print(f\"   â€¢ Dataset: ISIC 2018 (7 classes)\")\n",
    "    print(f\"   â€¢ Seeds evaluated: {config.seeds}\")\n",
    "    \n",
    "    # Adversarial results summary\n",
    "    print(f\"\\nðŸ›¡ï¸ ADVERSARIAL ROBUSTNESS FINDINGS\")\n",
    "    \n",
    "    # FGSM\n",
    "    if config.fgsm_enabled:\n",
    "        fgsm_8 = df_results[(df_results[\"Attack\"] == \"FGSM\") & (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "        if len(fgsm_8) > 0:\n",
    "            mean_robust = fgsm_8[\"Robust_Acc\"].mean()\n",
    "            mean_drop = fgsm_8[\"Acc_Drop\"].mean()\n",
    "            print(f\"\\n   FGSM (Îµ=8/255):\")\n",
    "            print(f\"   â”œâ”€ Robust Accuracy: {mean_robust:.2f}%\")\n",
    "            print(f\"   â”œâ”€ Accuracy Drop: {mean_drop:.2f}pp\")\n",
    "            print(f\"   â””â”€ Interpretation: {'Moderate vulnerability' if mean_drop < 40 else 'High vulnerability'}\")\n",
    "    \n",
    "    # PGD\n",
    "    if config.pgd_enabled:\n",
    "        pgd_8 = df_results[(df_results[\"Attack\"] == f\"PGD-{config.pgd_steps}\") & (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "        if len(pgd_8) > 0:\n",
    "            mean_robust = pgd_8[\"Robust_Acc\"].mean()\n",
    "            mean_drop = pgd_8[\"Acc_Drop\"].mean()\n",
    "            print(f\"\\n   PGD-{config.pgd_steps} (Îµ=8/255):\")\n",
    "            print(f\"   â”œâ”€ Robust Accuracy: {mean_robust:.2f}%\")\n",
    "            print(f\"   â”œâ”€ Accuracy Drop: {mean_drop:.2f}pp\")\n",
    "            print(f\"   â””â”€ Interpretation: {'Severe vulnerability' if mean_drop > 60 else 'High vulnerability' if mean_drop > 40 else 'Moderate'}\")\n",
    "    \n",
    "    # C&W\n",
    "    if config.cw_enabled:\n",
    "        cw = df_results[df_results[\"Attack\"] == \"C&W-L2\"]\n",
    "        if len(cw) > 0:\n",
    "            mean_robust = cw[\"Robust_Acc\"].mean()\n",
    "            mean_drop = cw[\"Acc_Drop\"].mean()\n",
    "            print(f\"\\n   C&W L2:\")\n",
    "            print(f\"   â”œâ”€ Robust Accuracy: {mean_robust:.2f}%\")\n",
    "            print(f\"   â”œâ”€ Accuracy Drop: {mean_drop:.2f}pp\")\n",
    "            print(f\"   â””â”€ Interpretation: Strongest attack, minimal perturbation\")\n",
    "    \n",
    "    # Key insights\n",
    "    print(f\"\\nðŸ”‘ KEY INSIGHTS\")\n",
    "    print(f\"   1. Standard CNNs are highly vulnerable to adversarial attacks\")\n",
    "    print(f\"   2. PGD attacks are stronger than FGSM (multi-step > single-step)\")\n",
    "    print(f\"   3. Some classes (e.g., MEL, BCC) may be more vulnerable than others\")\n",
    "    print(f\"   4. Adversarial training is needed for robust medical AI deployment\")\n",
    "    \n",
    "    # Research implications\n",
    "    print(f\"\\nðŸ“š RESEARCH IMPLICATIONS\")\n",
    "    print(f\"   â€¢ These results motivate Phase 5: Adversarial Training\")\n",
    "    print(f\"   â€¢ Tri-objective optimization will balance accuracy, robustness, and explainability\")\n",
    "    print(f\"   â€¢ Medical AI systems require robustness validation before clinical use\")\n",
    "    \n",
    "    # Files generated\n",
    "    print(f\"\\nðŸ“ GENERATED OUTPUTS\")\n",
    "    print(f\"   â€¢ Results CSV: adversarial_robustness_results.csv\")\n",
    "    print(f\"   â€¢ Summary CSV: adversarial_robustness_summary.csv\")\n",
    "    print(f\"   â€¢ JSON results: adversarial_robustness_results.json\")\n",
    "    print(f\"   â€¢ LaTeX table: adversarial_robustness_table.tex\")\n",
    "    print(f\"   â€¢ Figures: robustness_curves.png, vulnerability_heatmap.png, etc.\")\n",
    "    \n",
    "    print(\"\\n\" + \"â•\"*70)\n",
    "    print(f\"âœ… Phase 4 Adversarial Robustness Evaluation COMPLETE\")\n",
    "    print(f\"â±ï¸  Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"â•\"*70)\n",
    "\n",
    "# Print executive summary\n",
    "print_executive_summary(df_results, clean_results, config)\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL MEMORY CLEANUP\n",
    "# ============================================================================\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nðŸ§¹ Memory cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
