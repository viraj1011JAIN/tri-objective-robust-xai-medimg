{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca3c4bc",
   "metadata": {
    "id": "5ca3c4bc"
   },
   "source": [
    "# üõ°Ô∏è Phase 4: Adversarial Robustness Evaluation\n",
    "\n",
    "## Tri-Objective Robust XAI for Medical Imaging\n",
    "\n",
    "**Author:** Viraj Pankaj Jain  \n",
    "**Institution:** University of Glasgow  \n",
    "**Date:** November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook evaluates the **adversarial robustness** of baseline ResNet-50 models trained on ISIC 2018 dermoscopy images. We systematically assess vulnerability to:\n",
    "\n",
    "| Attack | Type | Strength | Use Case |\n",
    "|--------|------|----------|----------|\n",
    "| **FGSM** | Gradient-based | Fast, single-step | Real-time threat assessment |\n",
    "| **PGD** | Iterative | Strong, multi-step | Reliable robustness benchmark |\n",
    "| **C&W** | Optimization | Strongest, minimal perturbation | Worst-case security analysis |\n",
    "\n",
    "## üéØ Research Questions Addressed\n",
    "\n",
    "- **RQ1:** How vulnerable are standard CNNs to adversarial attacks in medical imaging?\n",
    "- **RQ2:** How does attack strength (Œµ) affect model accuracy degradation?\n",
    "- **RQ3:** Which skin lesion classes are most vulnerable to adversarial perturbations?\n",
    "\n",
    "## üìä Evaluation Protocol\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  ADVERSARIAL EVALUATION PIPELINE                                ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  1. Load trained baseline models (Seeds: 42, 123, 456)          ‚îÇ\n",
    "‚îÇ  2. Evaluate clean accuracy (sanity check)                      ‚îÇ\n",
    "‚îÇ  3. Generate adversarial examples at Œµ ‚àà {2/255, 4/255, 8/255}  ‚îÇ\n",
    "‚îÇ  4. Measure robust accuracy under each attack                   ‚îÇ\n",
    "‚îÇ  5. Analyze per-class vulnerability                             ‚îÇ\n",
    "‚îÇ  6. Visualize perturbations and decision boundaries             ‚îÇ\n",
    "‚îÇ  7. Statistical analysis across seeds                           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## ‚ö° Hardware Requirements\n",
    "\n",
    "- **Recommended:** NVIDIA A100 (40GB) - Full evaluation ~15 minutes\n",
    "- **Minimum:** NVIDIA T4 (16GB) - Full evaluation ~45 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86b350",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd86b350",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764461323355,
     "user_tz": 0,
     "elapsed": 24916,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "6852a7d9-0ef5-40ab-980c-01c2de5e2a4d"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 4: ADVERSARIAL ROBUSTNESS EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Mount Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Google Colab detected, Drive mounted\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚úÖ Local environment detected\")\n",
    "\n",
    "# Clone/update repository\n",
    "if IN_COLAB:\n",
    "    REPO_PATH = Path('/content/tri-objective-robust-xai-medimg')\n",
    "    if not REPO_PATH.exists():\n",
    "        !git clone https://github.com/viraj1011JAIN/tri-objective-robust-xai-medimg.git {REPO_PATH}\n",
    "        print(\"‚úÖ Repository cloned\")\n",
    "    else:\n",
    "        os.chdir(REPO_PATH)\n",
    "        !git pull origin main\n",
    "        print(\"‚úÖ Repository updated\")\n",
    "\n",
    "    os.chdir(REPO_PATH)\n",
    "    sys.path.insert(0, str(REPO_PATH))\n",
    "    PROJECT_ROOT = REPO_PATH\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"üìÅ Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0c6a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80a0c6a3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764461343910,
     "user_tz": 0,
     "elapsed": 20554,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "686aebde-99b4-4cf6-b8a3-ead37389adda"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: INSTALL DEPENDENCIES\n",
    "# ============================================================================\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q timm albumentations scikit-learn pandas matplotlib seaborn tqdm mlflow\n",
    "!pip install -q plotly kaleido scipy statsmodels\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a790d006",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a790d006",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764461358942,
     "user_tz": 0,
     "elapsed": 15031,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "baee6619-f098-41dc-aea3-7ba0af5d318f"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: IMPORTS\n",
    "# ============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Albumentations for transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score\n",
    ")\n",
    "from scipy import stats\n",
    "\n",
    "# Project imports - Attacks\n",
    "from src.attacks.fgsm import FGSM, FGSMConfig\n",
    "from src.attacks.pgd import PGD, PGDConfig\n",
    "from src.attacks.cw import CarliniWagner, CWConfig\n",
    "\n",
    "# Project imports - Data & Model\n",
    "from src.datasets.isic import ISICDataset\n",
    "from src.models.build import build_model\n",
    "from src.utils.reproducibility import set_global_seed\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    # Enable TF32 for A100\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837cc5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d837cc5a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764461360261,
     "user_tz": 0,
     "elapsed": 1318,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "f454bc0c-cadf-417c-f97d-b26bbe5abbcc"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: CONFIGURATION\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths (Google Drive)\n",
    "    'data_root': Path('/content/drive/MyDrive/data/data/isic_2018'),\n",
    "    'checkpoint_dir': Path('/content/drive/MyDrive/checkpoints/baseline'),\n",
    "    'results_dir': Path('/content/drive/MyDrive/results/phase4'),\n",
    "\n",
    "    # Model\n",
    "    'model_name': 'resnet50',\n",
    "    'num_classes': 7,\n",
    "\n",
    "    # Evaluation settings\n",
    "    'batch_size': 32,  # Reduced for stability during attacks\n",
    "    'num_workers': 0,  # Set to 0 for Colab (avoids multiprocessing errors)\n",
    "    'image_size': 224,\n",
    "\n",
    "    # Seeds to evaluate\n",
    "    'seeds': [42, 123, 456],\n",
    "\n",
    "    # Attack configurations\n",
    "    'epsilons': [2/255, 4/255, 8/255],\n",
    "    'pgd_steps': 20,  # Reduced from 40 for faster evaluation (still effective)\n",
    "    'cw_iterations': 50,  # Reduced from 100 for faster evaluation\n",
    "\n",
    "    # Class names\n",
    "    'class_names': ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'],\n",
    "}\n",
    "\n",
    "# Class descriptions for visualization labels\n",
    "CLASS_DESCRIPTIONS = {\n",
    "    'AKIEC': 'Actinic Keratoses (pre-cancerous)',\n",
    "    'BCC': 'Basal Cell Carcinoma (cancerous)',\n",
    "    'BKL': 'Benign Keratosis (benign)',\n",
    "    'DF': 'Dermatofibroma (benign)',\n",
    "    'MEL': 'Melanoma (malignant)',\n",
    "    'NV': 'Melanocytic Nevi (moles)',\n",
    "    'VASC': 'Vascular Lesions (blood vessel)',\n",
    "}\n",
    "\n",
    "# ImageNet normalization\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Create output directories\n",
    "CONFIG['results_dir'].mkdir(parents=True, exist_ok=True)\n",
    "(CONFIG['results_dir'] / 'figures').mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìä Model: {CONFIG['model_name']}\")\n",
    "print(f\"üìä Seeds: {CONFIG['seeds']}\")\n",
    "print(f\"üìä Epsilons: {[f'{int(e*255)}/255' for e in CONFIG['epsilons']]}\")\n",
    "print(f\"üìä Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"üìÅ Data: {CONFIG['data_root']}\")\n",
    "print(f\"üìÅ Checkpoints: {CONFIG['checkpoint_dir']}\")\n",
    "print(f\"üìÅ Results: {CONFIG['results_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f8450",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c8f8450",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764461362468,
     "user_tz": 0,
     "elapsed": 2207,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "8015f66d-b382-4b67-e49d-48369dfed06a"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: DATA PREPARATION\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load and fix metadata\n",
    "metadata_path = CONFIG['data_root'] / 'metadata.csv'\n",
    "print(f\"üìÑ Loading metadata: {metadata_path}\")\n",
    "\n",
    "df = pd.read_csv(metadata_path)\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "\n",
    "# Fix path separators\n",
    "if 'image_path' in df.columns:\n",
    "    df['image_path'] = df['image_path'].str.replace('\\\\', '/', regex=False)\n",
    "    print(\"   ‚úÖ Fixed path separators\")\n",
    "\n",
    "# Save fixed metadata\n",
    "fixed_path = CONFIG['data_root'] / 'metadata_fixed.csv'\n",
    "df.to_csv(fixed_path, index=False)\n",
    "\n",
    "# Show test split info\n",
    "test_df = df[df['split'] == 'test']\n",
    "print(f\"\\nüìä Test samples: {len(test_df)}\")\n",
    "print(f\"üìä Test class distribution:\")\n",
    "print(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d542d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "983d542d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764461420767,
     "user_tz": 0,
     "elapsed": 58285,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "6a12e6c6-fd1d-49df-ee4e-81921858d54a"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: CREATE TEST DATASET\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"CREATING TEST DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test transforms - NO normalization (attacks need [0,1] range)\n",
    "test_transforms = A.Compose([\n",
    "    A.Resize(CONFIG['image_size'], CONFIG['image_size']),\n",
    "    A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),  # Keep in [0, 1]\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Create test dataset\n",
    "print(f\"üìÅ Data root: {CONFIG['data_root']}\")\n",
    "print(f\"üìÑ CSV path: {fixed_path}\")\n",
    "\n",
    "test_dataset = ISICDataset(\n",
    "    root=str(CONFIG['data_root']),\n",
    "    split='test',\n",
    "    transforms=test_transforms,\n",
    "    csv_path=str(fixed_path),\n",
    "    image_column='image_path',\n",
    "    label_column='label'\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Dataset created with {len(test_dataset)} samples\")\n",
    "\n",
    "# Create dataloader - Use num_workers=0 in Colab to avoid multiprocessing issues\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Set to 0 for Colab compatibility (avoids multiprocessing errors)\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Test samples: {len(test_dataset)}\")\n",
    "print(f\"‚úÖ Batches: {len(test_loader)}\")\n",
    "print(f\"‚úÖ Classes: {CONFIG['class_names']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY DATALOADER CAN BE ITERATED\n",
    "# ============================================================================\n",
    "print(\"\\nüîç Verifying DataLoader can be iterated...\")\n",
    "\n",
    "if len(test_dataset) == 0:\n",
    "    print(\"‚ùå ERROR: Dataset has 0 samples!\")\n",
    "    print(\"   üîç Check that test split exists in metadata CSV\")\n",
    "    print(f\"   üìÑ CSV: {fixed_path}\")\n",
    "else:\n",
    "    # Test iteration - get first batch\n",
    "    try:\n",
    "        batch_count = 0\n",
    "        sample_count = 0\n",
    "\n",
    "        for batch in test_loader:\n",
    "            batch_count += 1\n",
    "            if len(batch) == 3:\n",
    "                imgs, lbls, _ = batch\n",
    "            else:\n",
    "                imgs, lbls = batch\n",
    "            sample_count += len(lbls)\n",
    "\n",
    "            if batch_count == 1:\n",
    "                print(f\"   ‚úÖ First batch: images={imgs.shape}, labels={lbls.shape}\")\n",
    "                print(f\"   ‚úÖ Image range: [{imgs.min():.3f}, {imgs.max():.3f}]\")\n",
    "                print(f\"   ‚úÖ Labels in batch: {lbls.unique().tolist()}\")\n",
    "\n",
    "            # Just check first few batches\n",
    "            if batch_count >= 3:\n",
    "                break\n",
    "\n",
    "        print(f\"   ‚úÖ Successfully iterated {batch_count} batches ({sample_count} samples)\")\n",
    "        print(f\"   ‚úÖ Full iteration should process {len(test_loader)} batches\")\n",
    "\n",
    "    except StopIteration:\n",
    "        print(\"   ‚ùå ERROR: DataLoader iteration stopped immediately!\")\n",
    "        print(\"   üîç Dataset __getitem__ may be failing\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ERROR during iteration: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd9df2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19dd9df2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764461420776,
     "user_tz": 0,
     "elapsed": 8,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "706b79a6-2031-4165-81c1-bbb3bf8108f3"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_normalizer(device):\n",
    "    \"\"\"Create ImageNet normalization function.\"\"\"\n",
    "    mean = torch.tensor(IMAGENET_MEAN).view(1, 3, 1, 1).to(device)\n",
    "    std = torch.tensor(IMAGENET_STD).view(1, 3, 1, 1).to(device)\n",
    "\n",
    "    def normalize(x):\n",
    "        return (x - mean) / std\n",
    "    return normalize\n",
    "\n",
    "def evaluate_clean(model, dataloader, device, normalize_fn):\n",
    "    \"\"\"Evaluate model on clean data with robust error handling.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    batch_count = 0\n",
    "\n",
    "    # First, verify the dataloader has data\n",
    "    print(f\"   üìä DataLoader length: {len(dataloader)} batches\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Clean Eval', leave=False):\n",
    "            batch_count += 1\n",
    "            if len(batch) == 3:\n",
    "                images, labels, _ = batch\n",
    "            else:\n",
    "                images, labels = batch\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Normalize and predict\n",
    "            outputs = model(normalize_fn(images))\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    print(f\"   ‚úÖ Processed {batch_count} batches, {len(all_preds)} samples\")\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Check if we have any samples\n",
    "    if len(all_preds) == 0:\n",
    "        print(\"   ‚ùå ERROR: No samples processed! DataLoader is empty.\")\n",
    "        print(\"   üîç Troubleshooting:\")\n",
    "        print(\"      1. Check if test_dataset has samples: len(test_dataset)\")\n",
    "        print(\"      2. Check if metadata CSV has test split samples\")\n",
    "        print(\"      3. Check if image paths in CSV are correct\")\n",
    "        return {\n",
    "            'accuracy': 0.0,\n",
    "            'balanced_accuracy': 0.0,\n",
    "            'f1_macro': 0.0,\n",
    "            'auroc': 0.0,\n",
    "            'predictions': all_preds,\n",
    "            'labels': all_labels,\n",
    "            'probs': all_probs,\n",
    "            'error': 'No samples processed'\n",
    "        }\n",
    "\n",
    "    # Check if we have enough classes for AUROC\n",
    "    unique_labels = np.unique(all_labels)\n",
    "    num_classes = all_probs.shape[1] if len(all_probs.shape) > 1 else 1\n",
    "\n",
    "    # Calculate basic metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "    balanced_acc = balanced_accuracy_score(all_labels, all_preds) * 100\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro') * 100\n",
    "\n",
    "    # Calculate AUROC with proper handling\n",
    "    try:\n",
    "        if len(unique_labels) < 2:\n",
    "            print(f\"   ‚ö†Ô∏è Only {len(unique_labels)} class(es) in labels, AUROC not meaningful\")\n",
    "            auroc = 0.0\n",
    "        elif num_classes == 2:\n",
    "            # Binary classification\n",
    "            auroc = roc_auc_score(all_labels, all_probs[:, 1]) * 100\n",
    "        else:\n",
    "            # Multi-class\n",
    "            auroc = roc_auc_score(all_labels, all_probs, multi_class='ovr') * 100\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è AUROC calculation failed: {e}\")\n",
    "        auroc = 0.0\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'auroc': auroc,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'probs': all_probs\n",
    "    }\n",
    "\n",
    "def evaluate_attack(model, dataloader, device, normalize_fn, attack_fn, desc='Attack'):\n",
    "    \"\"\"Evaluate model under adversarial attack.\"\"\"\n",
    "    model.eval()\n",
    "    all_clean_preds, all_adv_preds, all_labels = [], [], []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=desc, leave=False):\n",
    "        if len(batch) == 3:\n",
    "            images, labels, _ = batch\n",
    "        else:\n",
    "            images, labels = batch\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clean predictions\n",
    "        with torch.no_grad():\n",
    "            clean_preds = model(normalize_fn(images)).argmax(dim=1)\n",
    "\n",
    "        # Generate adversarial examples\n",
    "        x_adv = attack_fn(images, labels)\n",
    "\n",
    "        # Adversarial predictions\n",
    "        with torch.no_grad():\n",
    "            adv_preds = model(normalize_fn(x_adv)).argmax(dim=1)\n",
    "\n",
    "        all_clean_preds.extend(clean_preds.cpu().numpy())\n",
    "        all_adv_preds.extend(adv_preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_clean_preds = np.array(all_clean_preds)\n",
    "    all_adv_preds = np.array(all_adv_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Check if we have samples\n",
    "    if len(all_labels) == 0:\n",
    "        print(f\"   ‚ùå ERROR: No samples for {desc}\")\n",
    "        return {\n",
    "            'clean_accuracy': 0.0,\n",
    "            'robust_accuracy': 0.0,\n",
    "            'accuracy_drop': 0.0,\n",
    "            'per_class_robust_acc': {},\n",
    "            'confusion_matrix': None,\n",
    "            'error': 'No samples processed'\n",
    "        }\n",
    "\n",
    "    clean_acc = accuracy_score(all_labels, all_clean_preds) * 100\n",
    "    robust_acc = accuracy_score(all_labels, all_adv_preds) * 100\n",
    "\n",
    "    # Per-class robust accuracy\n",
    "    cm = confusion_matrix(all_labels, all_adv_preds)\n",
    "    per_class_acc = (cm.diagonal() / cm.sum(axis=1)) * 100\n",
    "\n",
    "    return {\n",
    "        'clean_accuracy': clean_acc,\n",
    "        'robust_accuracy': robust_acc,\n",
    "        'accuracy_drop': clean_acc - robust_acc,\n",
    "        'per_class_robust_acc': dict(zip(CONFIG['class_names'], per_class_acc)),\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3148d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989,
     "referenced_widgets": [
      "30e4d78bab1d4c37ad02e655e01000e7",
      "e4b731aaff6640df9731e24cddcde3e4",
      "66f1d0e12feb440ca901c43f04034a8f",
      "4e5464a98f6543e1b1510499553333d6",
      "3014efe01e8b469b991e77d0cec8e1c8",
      "9991324dfa6d4c269d850b2f79a4d360",
      "1dca74cb6dae48c4ab3bae0ffd304f70",
      "c05c10e32c3649cfb1322ca19aa9df50",
      "86be7d216ae5486f9cec6a03e1f74487",
      "ffaef27f4e5247309c20f04a2158ff66",
      "a606723d81244536b89894b19306e9d6",
      "f10ec0a7da134e78b50c55323d833edf",
      "5d02ad3267b84919b352303fa635f234",
      "3796582f69064c53810fa3a8539b4b32",
      "0027160e85d6439282adf9b0ac53f628",
      "7aa846cfde7d4eb0b6aa5101aa464b60",
      "acce2abbd125460e8d683187c01f00fe",
      "0f8dda4b8e5f43e89193dbe97d2ddbff",
      "369068b499c441749ba2bed2fdacb3d6",
      "54535f1d77a04ad9b316e0fd28f2ee44",
      "9dcbb7a30adc4fcf9a73c56c7ee4b567",
      "09d5cdffd6594f6aa026a68e58975ca3",
      "aa7119aaaeae4962bf07e6b6c5bbe9db",
      "fe7dd09bd9254c80a7fa91099c6eb6c1",
      "c5908f46ca7d4a4b9071693cb13266a7",
      "6632c63dc0334019971ae714e5be493c",
      "d43c144be9394a1ab91a9834cb130688",
      "f678ad5054674a5e955d8e4f10e2dadd",
      "cb0b92752410456795f2e76ab3e2ae08",
      "03e8797e0c3f4df1ae7106cd304772a9",
      "9fd50f045b894e069020f8467d01717e",
      "1ea0449ffb6a4dc48848865b071b9737",
      "94d5483da2e743cfb0d13fcc44836111"
     ]
    },
    "id": "1a3148d8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764462214032,
     "user_tz": 0,
     "elapsed": 793255,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "10517202-3a4a-47e2-9357-8758ae6d3a2f"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: LOAD MODELS AND VERIFY CLEAN ACCURACY\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MODELS & VERIFYING CLEAN ACCURACY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY DATA LOADER FIRST\n",
    "# ============================================================================\n",
    "print(\"\\nüîç Verifying DataLoader before model loading...\")\n",
    "\n",
    "# Check if test_loader exists\n",
    "if 'test_loader' not in dir():\n",
    "    print(\"‚ùå ERROR: test_loader not defined. Please run Cell 6 first.\")\n",
    "    raise RuntimeError(\"test_loader not defined\")\n",
    "\n",
    "# Check DataLoader length\n",
    "print(f\"   üìä test_loader batches: {len(test_loader)}\")\n",
    "print(f\"   üìä test_dataset samples: {len(test_dataset)}\")\n",
    "\n",
    "if len(test_loader) == 0:\n",
    "    print(\"‚ùå ERROR: DataLoader is empty! No batches available.\")\n",
    "    print(\"\\nüí° DEBUGGING: Check your test_dataset\")\n",
    "    raise RuntimeError(\"Empty DataLoader\")\n",
    "\n",
    "# Actually try to get a batch\n",
    "print(\"   üì• Testing batch retrieval...\")\n",
    "try:\n",
    "    test_iter = iter(test_loader)\n",
    "    test_batch = next(test_iter)\n",
    "    if len(test_batch) == 3:\n",
    "        test_imgs, test_lbls, _ = test_batch\n",
    "    else:\n",
    "        test_imgs, test_lbls = test_batch\n",
    "    print(f\"   ‚úÖ Test batch retrieved: images={test_imgs.shape}, labels={test_lbls.shape}\")\n",
    "    del test_iter, test_batch, test_imgs, test_lbls  # Cleanup\n",
    "except StopIteration:\n",
    "    print(\"   ‚ùå ERROR: DataLoader iteration returned no batches!\")\n",
    "    print(\"   üîç The dataset may have 0 samples or images are unreadable\")\n",
    "    raise RuntimeError(\"DataLoader iteration failed\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå ERROR retrieving batch: {e}\")\n",
    "    print(\"   üîç Check if image paths in metadata CSV exist and are readable\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# CHECK CHECKPOINT DIRECTORY\n",
    "# ============================================================================\n",
    "print(f\"\\nüîç Checking checkpoint directory: {CONFIG['checkpoint_dir']}\")\n",
    "\n",
    "if not CONFIG['checkpoint_dir'].exists():\n",
    "    print(f\"‚ùå ERROR: Checkpoint directory does not exist!\")\n",
    "    print(f\"   Path: {CONFIG['checkpoint_dir']}\")\n",
    "    print(\"\\nüí° SOLUTIONS:\")\n",
    "    print(\"   1. If using Google Colab, make sure Google Drive is mounted\")\n",
    "    print(\"   2. Update CONFIG['checkpoint_dir'] in Cell 3 to point to correct location\")\n",
    "    print(\"   3. For Colab, typical path: /content/drive/MyDrive/checkpoints/baseline\")\n",
    "else:\n",
    "    print(f\"‚úÖ Checkpoint directory exists\")\n",
    "    # List contents\n",
    "    contents = list(CONFIG['checkpoint_dir'].iterdir())\n",
    "    print(f\"   Contents: {[c.name for c in contents[:10]]}{'...' if len(contents) > 10 else ''}\")\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_checkpoint_flexible(model, checkpoint_path, device):\n",
    "    \"\"\"\n",
    "    Load checkpoint with flexible state_dict handling.\n",
    "    Handles torch.compile() prefix (_orig_mod.) and other variations.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "\n",
    "    # Get state dict from checkpoint\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    elif 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "\n",
    "    # Check for _orig_mod. prefix (from torch.compile)\n",
    "    sample_key = list(state_dict.keys())[0]\n",
    "    if sample_key.startswith('_orig_mod.'):\n",
    "        print(\"   üîß Detected torch.compile() checkpoint, removing '_orig_mod.' prefix...\")\n",
    "        state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "    # Check for 'module.' prefix (from DataParallel)\n",
    "    sample_key = list(state_dict.keys())[0]\n",
    "    if sample_key.startswith('module.'):\n",
    "        print(\"   üîß Detected DataParallel checkpoint, removing 'module.' prefix...\")\n",
    "        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "    # Load state dict\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Return metadata\n",
    "    metadata = {\n",
    "        'epoch': checkpoint.get('epoch', 'unknown'),\n",
    "        'val_acc': checkpoint.get('val_acc', checkpoint.get('best_val_acc', 'unknown')),\n",
    "    }\n",
    "    return metadata\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODELS\n",
    "# ============================================================================\n",
    "\n",
    "normalize = get_normalizer(device)\n",
    "models = {}\n",
    "clean_results = {}\n",
    "\n",
    "# Check for different checkpoint path patterns\n",
    "def find_checkpoint(seed):\n",
    "    \"\"\"Try multiple checkpoint path patterns.\"\"\"\n",
    "    patterns = [\n",
    "        CONFIG['checkpoint_dir'] / f'seed_{seed}' / 'best.pt',\n",
    "        CONFIG['checkpoint_dir'] / f'seed_{seed}' / 'last.pt',\n",
    "        CONFIG['checkpoint_dir'] / f'seed{seed}' / 'best.pt',\n",
    "        CONFIG['checkpoint_dir'] / f'baseline_seed_{seed}.pt',\n",
    "        CONFIG['checkpoint_dir'] / f'best_seed_{seed}.pt',\n",
    "        CONFIG['checkpoint_dir'] / 'best.pt',  # Single checkpoint\n",
    "    ]\n",
    "\n",
    "    for path in patterns:\n",
    "        if path.exists():\n",
    "            return path\n",
    "    return None\n",
    "\n",
    "print(f\"\\nüì• Loading models for seeds: {CONFIG['seeds']}\")\n",
    "\n",
    "for seed in CONFIG['seeds']:\n",
    "    print(f\"\\nüì• Loading seed {seed}...\")\n",
    "\n",
    "    # Find checkpoint using multiple patterns\n",
    "    checkpoint_path = find_checkpoint(seed)\n",
    "\n",
    "    if checkpoint_path is None:\n",
    "        print(f\"   ‚ùå No checkpoint found for seed {seed}\")\n",
    "        print(f\"   üìÅ Searched in: {CONFIG['checkpoint_dir']}\")\n",
    "\n",
    "        # Try to list what's actually in the directory\n",
    "        if CONFIG['checkpoint_dir'].exists():\n",
    "            try:\n",
    "                files = list(CONFIG['checkpoint_dir'].rglob('*.pt'))[:5]\n",
    "                if files:\n",
    "                    print(f\"   üìã Available .pt files: {[str(f.relative_to(CONFIG['checkpoint_dir'])) for f in files]}\")\n",
    "            except:\n",
    "                pass\n",
    "        continue\n",
    "\n",
    "    print(f\"   üìÇ Found: {checkpoint_path}\")\n",
    "\n",
    "    try:\n",
    "        # Load model\n",
    "        model = build_model(\n",
    "            architecture=CONFIG['model_name'],\n",
    "            num_classes=CONFIG['num_classes'],\n",
    "            pretrained=False\n",
    "        ).to(device)\n",
    "\n",
    "        # Use flexible checkpoint loading\n",
    "        metadata = load_checkpoint_flexible(model, checkpoint_path, device)\n",
    "        model.eval()\n",
    "        models[seed] = model\n",
    "\n",
    "        print(f\"   üìä Checkpoint epoch: {metadata['epoch']}\")\n",
    "\n",
    "        # Verify clean accuracy\n",
    "        print(f\"   üîÑ Evaluating clean accuracy...\")\n",
    "        result = evaluate_clean(model, test_loader, device, normalize)\n",
    "\n",
    "        # Check for errors\n",
    "        if 'error' in result:\n",
    "            print(f\"   ‚ö†Ô∏è Evaluation had issues: {result['error']}\")\n",
    "            clean_results[seed] = result\n",
    "        else:\n",
    "            clean_results[seed] = result\n",
    "            print(f\"   ‚úÖ Clean Accuracy: {result['accuracy']:.2f}%\")\n",
    "            if 'auroc' in result and result['auroc'] > 0:\n",
    "                print(f\"   ‚úÖ AUROC: {result['auroc']:.2f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error loading seed {seed}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìä LOADING SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if len(models) == 0:\n",
    "    print(\"‚ùå NO MODELS LOADED!\")\n",
    "    print(\"\\nüí° TROUBLESHOOTING:\")\n",
    "    print(\"   1. Check that checkpoint files exist in the specified directory\")\n",
    "    print(\"   2. For Colab: Ensure Google Drive is mounted with drive.mount('/content/drive')\")\n",
    "    print(\"   3. Update CONFIG['checkpoint_dir'] to match your checkpoint location\")\n",
    "    print(f\"\\n   Current checkpoint_dir: {CONFIG['checkpoint_dir']}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Loaded {len(models)} model(s): seeds {list(models.keys())}\")\n",
    "\n",
    "    # Check for valid clean results\n",
    "    valid_results = {s: r for s, r in clean_results.items() if 'error' not in r}\n",
    "\n",
    "    if len(valid_results) > 0:\n",
    "        accs = [valid_results[s]['accuracy'] for s in valid_results]\n",
    "        print(f\"\\nüìä CLEAN ACCURACY SUMMARY\")\n",
    "        print(f\"   Mean: {np.mean(accs):.2f}% ¬± {np.std(accs):.2f}%\")\n",
    "        for seed in valid_results:\n",
    "            print(f\"   Seed {seed}: {valid_results[seed]['accuracy']:.2f}%\")\n",
    "    elif len(clean_results) > 0:\n",
    "        print(\"\\n‚ö†Ô∏è Clean accuracy evaluation had issues for all seeds\")\n",
    "        print(\"   Check DataLoader and dataset paths\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No clean accuracy results - evaluation may have failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6691f32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d01f452448bc4b09ac20d58aa8e109f6",
      "797dc9ee5cf24873be4c86d75d0a6747",
      "b6b468ef52bf467c8318c28f994e0c11",
      "947d385aed78413d92d0fbd416e120e9",
      "7efbf06b245b490382041d6c614fa3af",
      "44f4e69c2a964c349a049241232596f2",
      "344fa36e091b4cebae4a845338dea741",
      "584336eb2d204bc68566bed998c98f81",
      "c8a5dbc81661468eb326aa268d01c7ba",
      "c0d8851adb9244ca8f662e71a0a038ab",
      "9d3858a884f443cc8ece690caa78c961",
      "618530994af84a9785d8f6ee89097222",
      "37995b6e66fb42aca43dc7782f819e8a",
      "829307b393054530a879e757e2bf5318",
      "e4021785dd1149c492d2b0a8d55f6a8f",
      "b3a60596ef624572acee1328f3d5f3e5",
      "f41b42ea30b54777972acccb5ccb33ad",
      "88596998ca83471782783b975aa48d06",
      "9f43afbaa279452dac4b2c79f2a1a47c",
      "6395dd907e994ec19a73279e03791e42",
      "f8405dc7c2564dce980f215f7aa9ff4b",
      "7774189b35444d4799cd99681d77af44",
      "aa58661befd2496d9c678529b91296ed",
      "1bb6cf0a65064d69a915507bcded3cf5",
      "72efc827887a49f8976af7dcb1e64613",
      "2ee601cf04094313ac0b662fb41c94f3",
      "1ae1a7f4c00a46f9a68ed0799c7d3bf8",
      "7c773242f7904506a61c64c99405b39b",
      "21a0441f79c2496785daa2f41c97dfa4",
      "2011c8e3c11f4a54b691441e045442f8",
      "6534c30860324bfd92deb1f82aadf11d",
      "9680f2b6c49a463f80cdd67eb072aa00",
      "0f659467b8634055a9083b598fd0e4bf",
      "d83fdfaa192e47cc9d4363aa7307fe8b",
      "b5ef26521c42419ea35e6e5cda3d9bf5",
      "dd0584df79374e5eafe9ebcd6d75b563",
      "0a1f5f2ce87745d0a8ee3a4cb8a0c74d",
      "979e7766039a470cad5f06321a2c6bec",
      "1c662924ba19429fa2c926580440b8a9",
      "1e0dd2020b7b4d54b07d54c7bfda4d74",
      "c3277830b9ba47d3a0465039052c4df2",
      "901d7173ba054fb492a0a24362d84a3b",
      "2a0a69f3ee0b435492ceffd3a7b0b319",
      "ec5cad165ced460f8525df8a3b68b7aa",
      "53316f2458924a308069ab2460b526f4",
      "d7c7f202a37b4992964502275ac9273b",
      "bd4caf265c824d629479b04a0c253418",
      "c95ce0d633f545f0ab1f0298e1a9db78",
      "67994c21a918498e9208f2043c93542f",
      "4864f0fb7a8d48fda0e9fa988328a18d",
      "09a70b2543c6443e85fc5bdfd65527ce",
      "409d7ace80eb4b2eb0883a6d27e390b7",
      "c3d6d79d0b8e492cb77a44be6538bf10",
      "b8d02cc50fa94404a83afefb041a543e",
      "bf682b91c9ba42b4bf86b76ecaaa45e9",
      "c8109119180848558ab7da42845141ce",
      "e9fc877ae7a64ac7a496daf7a9489ba9",
      "f88717035ef94c929eac94c43e8f871a",
      "aa61d53a4c454a65ac6ce0a30c4e74d6",
      "3bc8dcc696d5418d9bfaab6f268bcaf3",
      "432a117649fc492e9b226df7d1ffe2a4",
      "35df3674ef604bbdb73d9c342939d537",
      "8b42aee5c196464091a4110f243f7525",
      "c611acec5fb34b02a0955bab35832742",
      "a4e5c8c132d0421795151972a35690b1",
      "36babc3311e146d683e12061eb264c00",
      "ffad29ab75e9494ea0927636e7474366",
      "3d6d9dd05ffe44cbb2f1a9ece3b5f00b",
      "b67049fabe6f49bf842fc7cdfddb33d0",
      "66841e91b253430da7f2ec16b9749dde",
      "a4de1190d35547bba95b5ed09429d73c",
      "bf6672133a3d498194f61b58215839c9",
      "f77e9b70bb8b48b28ce35027d68a623c",
      "b0ddc64d0f6b477a9595d8681d080122",
      "757a35a7d1914085ad704bff2356e5f2",
      "020a378959f9465ea2030182b1458aa1",
      "83e9f705be8646a889a41cb53f779faf",
      "2d04c0fd2b8a4023a07f8dedc7863a47",
      "201a8722eada4b5f84ef65acba201e21",
      "cb3e15eb0f6045f5a1884d80404a9af6",
      "b6e61ce4a0494c5093d687fd69b03693",
      "b3c47ec8339145dcb9d77a51828d114a",
      "b7b1b24d522c435d9995b9be908bf310",
      "ca06377ec0d64a008f12fa4eff8b7dee",
      "8f8e30ed4a494d77aff083ad41f7d132",
      "cba164ef2eea48399f98dc56e67e72e7",
      "4e9604abfb2b4861abcc5ac9e22de9b1",
      "2eb69070f0e04585bffb28382150bb77",
      "423eac8d89414b40935f8558851e09d3",
      "c4e623867b6b44c19650f65bfb08aa83",
      "6510acecd8af42c29689cb7be400c6b7",
      "7e3fb7a4e7db4633a6297ee465a20656",
      "b95144148c8140b9894ed5c5e02ce1ff",
      "18a16828a1474c4d9f20544ac6248825",
      "251662e114424a7dbd45d961f4ebf69e",
      "92217f7ed5c3421cbe15edfb0f283b9b",
      "62baf972ec984c0c817e1489bb279cac",
      "71179a31971d4ca994ac0ea14491bcbf",
      "d43c436efa164ac9a22584d845564fdd",
      "3eba0e78c31f4e6ba8780f566b5bc63c",
      "2969766bb41f4fa78aff427de65b3c4d",
      "56f21c4da65047049555f78506748f21",
      "33f975d551304e609beffeffb75247f8",
      "c0d338c144714baf8627cff92cf7ff3c",
      "a98b1b38589844dea1bf405d494c7d5d",
      "9a829d5d4b224c23bbe1faa9ba25b86e",
      "89c067bb23754c5ab181b241bdba40f7",
      "f61d38eae05449419b252e494129cd33",
      "f4dea77c4cfc4624a961489e1d3b034e",
      "9bb62c89a7ad43458176408c93749ce1",
      "5eff74ab8287463aa44f76193ef76597",
      "96dc9441358d4a1d9803a897c295de95",
      "8af4cf2c6a9d4ad88e293edab521f677",
      "40ebd1c533ad410f8f75b5a0361372bd",
      "d3c95111eb444a59a69b5ff316d222bd",
      "30fdf171a7854926bb29f4477ddb5aaa",
      "ddb1b54e2e164861a7730f374d4a9d7c",
      "4129d4b91dd24283a30556e353310d3c",
      "27c2c058a46747aab3f6c63049dece93",
      "36f1426648b4421fa3c012df02b09b11",
      "c64d870e3d0543b2802e46bcfa998dc5",
      "3d2cbc3a899549e2850b9ec1c384e2a3",
      "2281ce7bed1948a8b87d6b2ae29f4178",
      "e0fa26e868a9424a8cce3885c50c21c3",
      "ceda364e20dd4d10b03f6973f6709f9b",
      "478e0885c8b7477bbf104a1a3aecdc42",
      "3873caa1eb894db793634ee938aaece8",
      "fd6632d3888742feafc7bc9887294e43",
      "1950f427a66c4ad58cf650fe6b0bd029",
      "736667f7196446f884be806236f00259",
      "a8d664d249054b67baae7ae24fda6480",
      "9836e73e91614b3db9b978891db1c38c",
      "e2fa0653029f4e04968b9c0bc6ce6d33",
      "09bfc41e1f754c39b9c991607d78c1ef",
      "f9f41239bed84121970331fd840742ea",
      "6673f77b18114b9eb60a4793dca06e3e",
      "0e5e96dcfc1d49d98bf38f1e451daad6",
      "f1295dae014d4c1b82f753b4fbde31da",
      "231eee805dba49c69418630f24dc1937",
      "b8e66ea239144a2990ca4200fc09418f",
      "c434bbe29da1481ca07f1da4a0986612",
      "da18ab07a9b748e79fc380392f481366",
      "b0036e461f8b4066bffefac5042b13e6",
      "f76de7fae7ff4ab7bc13f052340dd353",
      "667ed9a8f2084849a79e77724cbd7b2c",
      "123455b1c9e9410cafc4ce499a29dae0",
      "f835656c40be48a7824598e06bde73b4",
      "34803203bbab4cd1885a61caffd5b325",
      "fa8b05f18e6a4b659e7e80d39047475b",
      "445cc3fb09a5486aa0361940b47b296f",
      "a1942508f9b84a03aa3b5cd81b344ae1",
      "e04761c7022c43078d8a2f65557d93c4",
      "ce114fe57c674ef1a6fecd705326bb72",
      "c6ecc28399a4497f96420217101f8c4c",
      "638ffd41256d49a4a6ad4e133f028bfd",
      "5838ee2ac6504e92bf44b4ddc01d025d",
      "14dadfbd9658408dabf66dc96e85a24b",
      "1daa0ee7bfae401d9a7b0f7d21b0867a",
      "0a50c5b33af646c79717834c7b886e8d",
      "d1ffe449e27144849721060877d6eadd",
      "8288c263a77c4540b57a13f8609fc8e1",
      "35df8df0201e430983f4cff2ed032ffb",
      "601790ad1d3e49ee8583a1bf3456ae0f",
      "07b003b3608c4896ae30f742dbd63e40",
      "b314220dd55746d0b4ba0226a92b30ce",
      "9775b68f481c458f90811bebd7111b7f",
      "1d85fd086fa14de99f6ad46312d55bf2",
      "8b977d26291c4947a0d182436e469a02",
      "50977180a482460c93b440792f4b1c9d",
      "dbdb5dea2e44424ea031752583bf26ff",
      "d5c610500b284d1ba9abc77177ddf15e",
      "38e6c212b030477aa23f1c85c0df1e57",
      "2fe5b0f257e94429bcfb1d9d348701f0",
      "176409b11aab4814843598f41edaf042",
      "2eeaa2762c004de582c7c4e75fd9c27e",
      "230e0fd7d5a840b6a80c4cb615a7bc48",
      "1e04d21b7b0b485a981ff694070042c5",
      "5ad8c96982af4cd48e75897f8d143a76",
      "915b9bb93b1f4b81a04fe98ed68223cd",
      "4db891dcd9dc43278a11ae8862fb4fa4",
      "7ae73588dba04d949d49dc71c55d0153",
      "5d230c0df9c143ad861599684ad632ba",
      "17c69ca90661424199c092e51b11cc41",
      "b148b0beb17e4bc5b89a6f1bcf8e65b9",
      "c7155b505b5f4cba9ecf151a2073774b",
      "63a032e8a7af443dadfc2e1b45f27543",
      "0d9c164f3c9347d4bdd06e907e177f1f",
      "a88c3c887a9742e9abc050fcf72b0c13",
      "0c869b901c2849ccbdb32103d7e2fa06",
      "41994c22317c4a22bc09e70133c4b245",
      "4449b0c610b94812b0214105f5c72141",
      "0d28fbf019cc49fba995a9fe74e3d35e",
      "3d8a66cd20a24a4fb8bd99351c9fb29c",
      "4c4d38be47144a17b35ff38e7e8cc3b7",
      "2b2056970d7740b391e2d928c6e0ad01",
      "73e4a0cb96d6428ba5306f2d33d157af",
      "7e0952f8bac9404b9c881467c60b4dbc",
      "2d6dc0e161304c35935ea361c96849b5",
      "0b3f747e67e44fe6b85b0aa536898308",
      "35449405b6744e5b9a4cf4a1af052089",
      "3e2d010d10f342d3970e11f6799f7c7a",
      "92e10d482d38492881d4d36e01131c53",
      "3e970f39e76f4e78b8aa5cd3f93c336f",
      "46f4f21ba4ba4e60ab7cd3667df5a497",
      "9b71dec524a74fc4a13f2f240d0c222e",
      "67c5bd93c22447e9acd381e4bf40042b",
      "6c7879a8b7954da5b10b9cc3da5a00a6",
      "85cb7d26d0b04ed98365b7b49d2986f3",
      "345b10b29c0e4d2ea718df6c38e2f780",
      "f92cedecd7a943c984d48093e1e6cd6d",
      "46b3636bd5bf491ea94ecbf4355ed01d",
      "35348f153bb8434c94a6f3464b36cd20",
      "86fa9f5d79e345b39ac31b9bec2eac14",
      "629c93e14b844e35be17dbd0757479fb",
      "091a2ab72dee4b0cbadff2b7bcacb0a9",
      "3d877d9eef3348a1afe18b801e270b4d",
      "f805e1220f434e2d9045f4cc88fdc3a5",
      "5189d029f26b4ce08ec8d944d18447b7",
      "0ee3ed1421af4fd3b1d731b30126e456",
      "c442c9e5ec264a82bc5c046491598d02",
      "3ca4cb18dabd4d48acd4d82cc9f0f2f6",
      "ee5158e836354ff590309cb31e52c59e",
      "944b880585c84919bba05745d20ad0f2",
      "569a93a3835e4eeab3c28cff82e27d92",
      "6932d79bfa5c4f34820f2b2ca5d4f110",
      "ae97a8001ac04e5ea0c4f6964bbc5768",
      "c8c8b14712814a479166388bb9240781",
      "697e6c0f6b594b439feac0b63fdf45c8",
      "28e6ba5be3934b7e8ca1158e35a1a9e2",
      "a2c46e6fbe974f9db58e9f2f70b2e745",
      "da3f9ef0e74f4b5faad0cd441d0f6ffc"
     ]
    },
    "id": "a6691f32",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764463610384,
     "user_tz": 0,
     "elapsed": 1396325,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "4ede1e46-579b-4eb4-c39d-da21c59cfa10"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: RUN ADVERSARIAL EVALUATION - ALL ATTACKS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"ADVERSARIAL EVALUATION - ALL ATTACKS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# PREREQUISITE CHECK\n",
    "# ============================================================================\n",
    "print(\"\\nüîç Checking prerequisites...\")\n",
    "\n",
    "prereqs_ok = True\n",
    "\n",
    "# Check models\n",
    "if 'models' not in dir() or len(models) == 0:\n",
    "    print(\"‚ùå ERROR: 'models' dictionary is empty or not defined!\")\n",
    "    print(\"   ‚Üí Please run Cell 8 (Load Models) first\")\n",
    "    prereqs_ok = False\n",
    "else:\n",
    "    print(f\"‚úÖ Models loaded: {list(models.keys())}\")\n",
    "\n",
    "# Check test_loader\n",
    "if 'test_loader' not in dir():\n",
    "    print(\"‚ùå ERROR: 'test_loader' not defined!\")\n",
    "    print(\"   ‚Üí Please run Cell 5 (Create DataLoader) first\")\n",
    "    prereqs_ok = False\n",
    "else:\n",
    "    print(f\"‚úÖ Test loader available\")\n",
    "\n",
    "# Check normalize function\n",
    "if 'normalize' not in dir():\n",
    "    print(\"‚ùå ERROR: 'normalize' function not defined!\")\n",
    "    print(\"   ‚Üí Please run Cell 8 first\")\n",
    "    prereqs_ok = False\n",
    "else:\n",
    "    print(f\"‚úÖ Normalize function available\")\n",
    "\n",
    "# Check clean_results\n",
    "if 'clean_results' not in dir() or len(clean_results) == 0:\n",
    "    print(\"‚ùå ERROR: 'clean_results' dictionary is empty!\")\n",
    "    print(\"   ‚Üí Please run Cell 8 first\")\n",
    "    prereqs_ok = False\n",
    "else:\n",
    "    print(f\"‚úÖ Clean results available for seeds: {list(clean_results.keys())}\")\n",
    "\n",
    "# Check CONFIG\n",
    "if 'CONFIG' not in dir():\n",
    "    print(\"‚ùå ERROR: 'CONFIG' dictionary not defined!\")\n",
    "    print(\"   ‚Üí Please run Cell 3 (Configuration) first\")\n",
    "    prereqs_ok = False\n",
    "else:\n",
    "    print(f\"‚úÖ CONFIG available\")\n",
    "\n",
    "# Check attack classes\n",
    "try:\n",
    "    from src.attacks.fgsm import FGSM, FGSMConfig\n",
    "    from src.attacks.pgd import PGD, PGDConfig\n",
    "    from src.attacks.cw import CarliniWagner, CWConfig\n",
    "    print(\"‚úÖ Attack classes imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå ERROR: Could not import attack classes: {e}\")\n",
    "    prereqs_ok = False\n",
    "\n",
    "if not prereqs_ok:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö†Ô∏è  PREREQUISITES NOT MET - Cannot run evaluation!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Please run the cells above in order before running this cell.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All prerequisites met! Starting evaluation...\\n\")\n",
    "\n",
    "    print(f\"‚è±Ô∏è  Start time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "    # Clear CUDA cache before starting\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for seed in CONFIG['seeds']:\n",
    "        if seed not in models:\n",
    "            print(f\"‚ö†Ô∏è Seed {seed} not in models, skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        model = models[seed]\n",
    "        seed_results = {'clean': clean_results[seed]}\n",
    "\n",
    "        # ==================== FGSM ====================\n",
    "        print(\"\\nüî• FGSM Attacks:\")\n",
    "        for eps in CONFIG['epsilons']:\n",
    "            eps_str = f\"{int(eps*255)}/255\"\n",
    "\n",
    "            # Create FGSM attack with explicit config\n",
    "            fgsm_config = FGSMConfig(\n",
    "                epsilon=eps,\n",
    "                clip_min=0.0,\n",
    "                clip_max=1.0,\n",
    "                targeted=False\n",
    "            )\n",
    "            fgsm = FGSM(fgsm_config)\n",
    "\n",
    "            # Create attack function (avoid closure issues)\n",
    "            def make_fgsm_fn(attack_obj, mdl, norm_fn):\n",
    "                def fn(x, y):\n",
    "                    return attack_obj.generate(mdl, x, y, loss_fn=nn.CrossEntropyLoss(), normalize=norm_fn)\n",
    "                return fn\n",
    "\n",
    "            fgsm_fn = make_fgsm_fn(fgsm, model, normalize)\n",
    "            result = evaluate_attack(model, test_loader, device, normalize, fgsm_fn, f\"FGSM Œµ={eps_str}\")\n",
    "            seed_results[f'fgsm_{eps_str}'] = result\n",
    "            print(f\"   Œµ={eps_str}: {result['robust_accuracy']:.2f}% (drop: {result['accuracy_drop']:.2f}%)\")\n",
    "\n",
    "            # Clear cache between attacks\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        # ==================== PGD ====================\n",
    "        print(\"\\nüî• PGD Attacks:\")\n",
    "        for eps in CONFIG['epsilons']:\n",
    "            eps_str = f\"{int(eps*255)}/255\"\n",
    "            step_size = eps / 4\n",
    "\n",
    "            # Create PGD attack\n",
    "            pgd_config = PGDConfig(\n",
    "                epsilon=eps,\n",
    "                num_steps=CONFIG['pgd_steps'],\n",
    "                step_size=step_size,\n",
    "                random_start=True,\n",
    "                clip_min=0.0,\n",
    "                clip_max=1.0,\n",
    "                targeted=False\n",
    "            )\n",
    "            pgd = PGD(pgd_config)\n",
    "\n",
    "            # Create attack function (avoid closure issues)\n",
    "            def make_pgd_fn(attack_obj, mdl, norm_fn):\n",
    "                def fn(x, y):\n",
    "                    return attack_obj.generate(mdl, x, y, loss_fn=nn.CrossEntropyLoss(), normalize=norm_fn)\n",
    "                return fn\n",
    "\n",
    "            pgd_fn = make_pgd_fn(pgd, model, normalize)\n",
    "            result = evaluate_attack(model, test_loader, device, normalize, pgd_fn, f\"PGD Œµ={eps_str}\")\n",
    "            seed_results[f'pgd_{eps_str}'] = result\n",
    "            print(f\"   Œµ={eps_str}: {result['robust_accuracy']:.2f}% (drop: {result['accuracy_drop']:.2f}%)\")\n",
    "\n",
    "            # Clear cache between attacks\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        # ==================== C&W ====================\n",
    "        print(\"\\nüî• Carlini-Wagner Attack:\")\n",
    "        cw_config = CWConfig(\n",
    "            confidence=0.0,\n",
    "            learning_rate=0.01,\n",
    "            max_iterations=CONFIG['cw_iterations'],\n",
    "            binary_search_steps=3,  # Reduced for speed\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False\n",
    "        )\n",
    "        cw = CarliniWagner(cw_config)\n",
    "\n",
    "        # Create attack function\n",
    "        def make_cw_fn(attack_obj, mdl, norm_fn):\n",
    "            def fn(x, y):\n",
    "                return attack_obj.generate(mdl, x, y, normalize=norm_fn)\n",
    "            return fn\n",
    "\n",
    "        cw_fn = make_cw_fn(cw, model, normalize)\n",
    "        result = evaluate_attack(model, test_loader, device, normalize, cw_fn, \"C&W L2\")\n",
    "        seed_results['cw'] = result\n",
    "        print(f\"   C&W: {result['robust_accuracy']:.2f}% (drop: {result['accuracy_drop']:.2f}%)\")\n",
    "\n",
    "        all_results[seed] = seed_results\n",
    "\n",
    "        # Clear cache after each seed\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\n‚è±Ô∏è  End time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(\"‚úÖ Evaluation complete!\")\n",
    "    print(f\"\\nüìä Results stored in 'all_results' with seeds: {list(all_results.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466cb6e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0466cb6e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764463611164,
     "user_tz": 0,
     "elapsed": 759,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "7bb3e504-d735-41e9-ded2-37333d4b1460"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: ADVANCED RESULTS TABLE WITH STYLED OUTPUT\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä COMPREHENSIVE ADVERSARIAL EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===================== BUILD COMPREHENSIVE RESULTS TABLE =====================\n",
    "rows = []\n",
    "attacks = ['clean'] + [f'fgsm_{int(e*255)}/255' for e in CONFIG['epsilons']] + \\\n",
    "          [f'pgd_{int(e*255)}/255' for e in CONFIG['epsilons']] + ['cw']\n",
    "\n",
    "for attack in attacks:\n",
    "    if attack == 'clean':\n",
    "        accs = [all_results[s]['clean']['accuracy'] for s in all_results]\n",
    "        attack_name = 'üü¢ Clean (No Attack)'\n",
    "        attack_type = 'Baseline'\n",
    "        severity = '‚Äî'\n",
    "    elif attack == 'cw':\n",
    "        accs = [all_results[s]['cw']['robust_accuracy'] for s in all_results]\n",
    "        attack_name = 'üî¥ Carlini-Wagner L2'\n",
    "        attack_type = 'Optimization'\n",
    "        severity = 'Maximum'\n",
    "    elif 'fgsm' in attack:\n",
    "        accs = [all_results[s][attack]['robust_accuracy'] for s in all_results]\n",
    "        eps = int(attack.split('_')[1].split('/')[0])\n",
    "        attack_name = f'üîµ FGSM (Œµ={eps}/255)'\n",
    "        attack_type = 'Gradient (1-step)'\n",
    "        severity = 'Weak' if eps == 2 else 'Medium' if eps == 4 else 'Strong'\n",
    "    else:  # pgd\n",
    "        accs = [all_results[s][attack]['robust_accuracy'] for s in all_results]\n",
    "        eps = int(attack.split('_')[1].split('/')[0])\n",
    "        attack_name = f'üü† PGD-40 (Œµ={eps}/255)'\n",
    "        attack_type = 'Iterative (40-step)'\n",
    "        severity = 'Weak' if eps == 2 else 'Medium' if eps == 4 else 'Strong'\n",
    "\n",
    "    mean_acc = np.mean(accs)\n",
    "    clean_acc = np.mean([all_results[s]['clean']['accuracy'] for s in all_results])\n",
    "    drop = clean_acc - mean_acc if attack != 'clean' else 0\n",
    "\n",
    "    rows.append({\n",
    "        'Attack': attack_name,\n",
    "        'Type': attack_type,\n",
    "        'Severity': severity,\n",
    "        'Mean Acc (%)': mean_acc,\n",
    "        'Std (%)': np.std(accs),\n",
    "        'Drop (%)': drop,\n",
    "        'Seed 42': accs[0] if len(accs) > 0 else np.nan,\n",
    "        'Seed 123': accs[1] if len(accs) > 1 else np.nan,\n",
    "        'Seed 456': accs[2] if len(accs) > 2 else np.nan,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "\n",
    "# ===================== STYLED TABLE DISPLAY =====================\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def color_accuracy(val):\n",
    "    \"\"\"Color code accuracy values.\"\"\"\n",
    "    if pd.isna(val): return ''\n",
    "    if val >= 70: return 'background-color: #a8e6cf; color: black'\n",
    "    elif val >= 40: return 'background-color: #ffd3b6; color: black'\n",
    "    else: return 'background-color: #ffaaa5; color: black'\n",
    "\n",
    "def color_drop(val):\n",
    "    \"\"\"Color code accuracy drop.\"\"\"\n",
    "    if pd.isna(val) or val == 0: return ''\n",
    "    if val <= 20: return 'background-color: #dcedc1; color: black'\n",
    "    elif val <= 50: return 'background-color: #ffeead; color: black'\n",
    "    else: return 'background-color: #ff6f69; color: white'\n",
    "\n",
    "# Format for display\n",
    "display_df = results_df.copy()\n",
    "display_df['Mean Acc (%)'] = display_df['Mean Acc (%)'].apply(lambda x: f'{x:.2f}')\n",
    "display_df['Std (%)'] = display_df['Std (%)'].apply(lambda x: f'{x:.2f}')\n",
    "display_df['Drop (%)'] = display_df['Drop (%)'].apply(lambda x: f'{x:.2f}' if x > 0 else '‚Äî')\n",
    "display_df['Seed 42'] = display_df['Seed 42'].apply(lambda x: f'{x:.2f}')\n",
    "display_df['Seed 123'] = display_df['Seed 123'].apply(lambda x: f'{x:.2f}')\n",
    "display_df['Seed 456'] = display_df['Seed 456'].apply(lambda x: f'{x:.2f}')\n",
    "\n",
    "print(\"\\nüìã DETAILED RESULTS TABLE:\")\n",
    "print(display_df.to_string(index=False))\n",
    "\n",
    "# ===================== EXECUTIVE SUMMARY STATISTICS =====================\n",
    "clean_mean = np.mean([all_results[s]['clean']['accuracy'] for s in all_results])\n",
    "fgsm8_mean = np.mean([all_results[s]['fgsm_8/255']['robust_accuracy'] for s in all_results])\n",
    "pgd8_mean = np.mean([all_results[s]['pgd_8/255']['robust_accuracy'] for s in all_results])\n",
    "cw_mean = np.mean([all_results[s]['cw']['robust_accuracy'] for s in all_results])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìà EXECUTIVE SUMMARY - KEY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "summary_table = f\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  BASELINE MODEL VULNERABILITY ASSESSMENT                          ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Clean Accuracy (Baseline):     {clean_mean:>6.2f}%                         ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ\n",
    "‚îÇ  FGSM Œµ=8/255:                  {fgsm8_mean:>6.2f}%  (‚Üì {clean_mean-fgsm8_mean:>5.2f}% drop)         ‚îÇ\n",
    "‚îÇ  PGD-40 Œµ=8/255:                {pgd8_mean:>6.2f}%  (‚Üì {clean_mean-pgd8_mean:>5.2f}% drop)         ‚îÇ\n",
    "‚îÇ  Carlini-Wagner L2:             {cw_mean:>6.2f}%  (‚Üì {clean_mean-cw_mean:>5.2f}% drop)         ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÇ\n",
    "‚îÇ  Average Robustness Degradation: {np.mean([clean_mean-fgsm8_mean, clean_mean-pgd8_mean, clean_mean-cw_mean]):>5.2f}% under strong attacks  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\"\n",
    "print(summary_table)\n",
    "\n",
    "# Save to CSV with full precision\n",
    "results_df.to_csv(CONFIG['results_dir'] / 'adversarial_results.csv', index=False, float_format='%.4f')\n",
    "print(f\"‚úÖ Results saved to: {CONFIG['results_dir'] / 'adversarial_results.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48242b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 696
    },
    "id": "5d48242b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764463615182,
     "user_tz": 0,
     "elapsed": 3996,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "4e03547b-9f28-49be-b206-45638ed27629"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: ADVANCED VISUALIZATION - PUBLICATION-QUALITY ROBUSTNESS ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä GENERATING ADVANCED VISUALIZATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===================== STYLE CONFIGURATION =====================\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.labelweight': 'bold',\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'legend.framealpha': 0.9,\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "})\n",
    "\n",
    "# Custom color palette - professional & accessible\n",
    "COLORS = {\n",
    "    'clean': '#2E8B57',      # Sea green\n",
    "    'fgsm': '#4169E1',       # Royal blue\n",
    "    'pgd': '#DC143C',        # Crimson\n",
    "    'cw': '#9400D3',         # Dark violet\n",
    "    'seeds': ['#FF6B6B', '#4ECDC4', '#45B7D1'],  # Coral, Teal, Sky blue\n",
    "    'gradient': ['#00C853', '#FFD600', '#FF6D00', '#D50000']  # Green to red\n",
    "}\n",
    "\n",
    "# ===================== FIGURE 1: COMPREHENSIVE ROBUSTNESS DASHBOARD =====================\n",
    "fig = plt.figure(figsize=(18, 14))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3, height_ratios=[1.2, 1, 1])\n",
    "\n",
    "# ------ Panel A: Robustness Degradation Curves (FGSM vs PGD) ------\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "\n",
    "eps_vals = [0] + [e*255 for e in CONFIG['epsilons']]\n",
    "markers = ['o', 's', '^']\n",
    "linestyles = ['-', '--', ':']\n",
    "\n",
    "# Plot for each seed\n",
    "for i, seed in enumerate(CONFIG['seeds']):\n",
    "    if seed not in all_results:\n",
    "        continue\n",
    "\n",
    "    # FGSM curve\n",
    "    fgsm_accs = [all_results[seed]['clean']['accuracy']]\n",
    "    for eps in CONFIG['epsilons']:\n",
    "        fgsm_accs.append(all_results[seed][f'fgsm_{int(eps*255)}/255']['robust_accuracy'])\n",
    "    ax1.plot(eps_vals, fgsm_accs, marker=markers[i], linestyle='-',\n",
    "             color=COLORS['fgsm'], linewidth=2.5, markersize=10,\n",
    "             label=f'FGSM (Seed {seed})', alpha=0.7 + 0.1*i)\n",
    "\n",
    "    # PGD curve\n",
    "    pgd_accs = [all_results[seed]['clean']['accuracy']]\n",
    "    for eps in CONFIG['epsilons']:\n",
    "        pgd_accs.append(all_results[seed][f'pgd_{int(eps*255)}/255']['robust_accuracy'])\n",
    "    ax1.plot(eps_vals, pgd_accs, marker=markers[i], linestyle='--',\n",
    "             color=COLORS['pgd'], linewidth=2.5, markersize=10,\n",
    "             label=f'PGD-40 (Seed {seed})', alpha=0.7 + 0.1*i)\n",
    "\n",
    "# Add mean curves with confidence band\n",
    "fgsm_means = [np.mean([all_results[s]['clean']['accuracy'] for s in all_results])]\n",
    "fgsm_stds = [np.std([all_results[s]['clean']['accuracy'] for s in all_results])]\n",
    "pgd_means = [np.mean([all_results[s]['clean']['accuracy'] for s in all_results])]\n",
    "pgd_stds = [np.std([all_results[s]['clean']['accuracy'] for s in all_results])]\n",
    "\n",
    "for eps in CONFIG['epsilons']:\n",
    "    fgsm_vals = [all_results[s][f'fgsm_{int(eps*255)}/255']['robust_accuracy'] for s in all_results]\n",
    "    fgsm_means.append(np.mean(fgsm_vals))\n",
    "    fgsm_stds.append(np.std(fgsm_vals))\n",
    "    pgd_vals = [all_results[s][f'pgd_{int(eps*255)}/255']['robust_accuracy'] for s in all_results]\n",
    "    pgd_means.append(np.mean(pgd_vals))\n",
    "    pgd_stds.append(np.std(pgd_vals))\n",
    "\n",
    "# Shade confidence bands\n",
    "ax1.fill_between(eps_vals, np.array(fgsm_means)-np.array(fgsm_stds),\n",
    "                 np.array(fgsm_means)+np.array(fgsm_stds), alpha=0.15, color=COLORS['fgsm'])\n",
    "ax1.fill_between(eps_vals, np.array(pgd_means)-np.array(pgd_stds),\n",
    "                 np.array(pgd_means)+np.array(pgd_stds), alpha=0.15, color=COLORS['pgd'])\n",
    "\n",
    "# Styling\n",
    "ax1.set_xlabel('Perturbation Budget Œµ (√ó255)', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "ax1.set_title('A) Adversarial Robustness Degradation: FGSM vs PGD-40', fontsize=15, fontweight='bold', pad=15)\n",
    "ax1.set_xticks(eps_vals)\n",
    "ax1.set_xticklabels(['0\\n(Clean)', '2', '4', '8'])\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.axhline(y=50, color='gray', linestyle=':', alpha=0.5, label='Random Guess (50%)')\n",
    "ax1.legend(loc='upper right', ncol=2, frameon=True, fancybox=True, shadow=True)\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add annotations for key drops\n",
    "clean_mean = np.mean([all_results[s]['clean']['accuracy'] for s in all_results])\n",
    "pgd8_mean = np.mean([all_results[s]['pgd_8/255']['robust_accuracy'] for s in all_results])\n",
    "ax1.annotate(f'‚Üì{clean_mean - pgd8_mean:.1f}%', xy=(8, pgd8_mean), xytext=(8.5, pgd8_mean + 15),\n",
    "             fontsize=12, fontweight='bold', color=COLORS['pgd'],\n",
    "             arrowprops=dict(arrowstyle='->', color=COLORS['pgd'], lw=2))\n",
    "\n",
    "# ------ Panel B: Attack Severity Radar Chart ------\n",
    "ax2 = fig.add_subplot(gs[0, 2], projection='polar')\n",
    "\n",
    "# Radar data for strongest attacks (Œµ=8/255)\n",
    "categories = ['FGSM\\nŒµ=2/255', 'FGSM\\nŒµ=8/255', 'PGD\\nŒµ=2/255', 'PGD\\nŒµ=8/255', 'C&W\\nL2']\n",
    "attack_keys = ['fgsm_2/255', 'fgsm_8/255', 'pgd_2/255', 'pgd_8/255', 'cw']\n",
    "\n",
    "values = []\n",
    "for key in attack_keys:\n",
    "    accs = [all_results[s][key]['robust_accuracy'] for s in all_results]\n",
    "    values.append(np.mean(accs))\n",
    "\n",
    "# Normalize to 0-100 and invert (higher = more vulnerable)\n",
    "vulnerability = [100 - v for v in values]\n",
    "vulnerability.append(vulnerability[0])  # Close the polygon\n",
    "\n",
    "angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()\n",
    "angles.append(angles[0])\n",
    "\n",
    "ax2.plot(angles, vulnerability, 'o-', linewidth=2.5, color='#E74C3C', markersize=8)\n",
    "ax2.fill(angles, vulnerability, alpha=0.25, color='#E74C3C')\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels(categories, size=9)\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.set_title('B) Vulnerability Profile\\n(Higher = More Vulnerable)', fontsize=13, fontweight='bold', pad=20)\n",
    "\n",
    "# ------ Panel C: Per-Seed Comparison ------\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "seed_data = []\n",
    "for seed in CONFIG['seeds']:\n",
    "    seed_data.append({\n",
    "        'Seed': str(seed),\n",
    "        'Clean': all_results[seed]['clean']['accuracy'],\n",
    "        'FGSM-8': all_results[seed]['fgsm_8/255']['robust_accuracy'],\n",
    "        'PGD-8': all_results[seed]['pgd_8/255']['robust_accuracy'],\n",
    "        'C&W': all_results[seed]['cw']['robust_accuracy']\n",
    "    })\n",
    "\n",
    "seed_df = pd.DataFrame(seed_data)\n",
    "x = np.arange(len(CONFIG['seeds']))\n",
    "width = 0.2\n",
    "\n",
    "bars1 = ax3.bar(x - 1.5*width, seed_df['Clean'], width, label='Clean', color=COLORS['clean'], edgecolor='white', linewidth=1.5)\n",
    "bars2 = ax3.bar(x - 0.5*width, seed_df['FGSM-8'], width, label='FGSM Œµ=8', color=COLORS['fgsm'], edgecolor='white', linewidth=1.5)\n",
    "bars3 = ax3.bar(x + 0.5*width, seed_df['PGD-8'], width, label='PGD Œµ=8', color=COLORS['pgd'], edgecolor='white', linewidth=1.5)\n",
    "bars4 = ax3.bar(x + 1.5*width, seed_df['C&W'], width, label='C&W', color=COLORS['cw'], edgecolor='white', linewidth=1.5)\n",
    "\n",
    "ax3.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "ax3.set_xlabel('Random Seed', fontweight='bold')\n",
    "ax3.set_title('C) Cross-Seed Consistency', fontsize=13, fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels([f'Seed {s}' for s in CONFIG['seeds']])\n",
    "ax3.legend(loc='upper right', fontsize=9)\n",
    "ax3.set_ylim(0, 100)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2, bars3, bars4]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.annotate(f'{height:.0f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                     xytext=(0, 3), textcoords='offset points', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "# ------ Panel D: Accuracy Drop Waterfall ------\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "attacks_order = ['FGSM\\nŒµ=2', 'FGSM\\nŒµ=4', 'FGSM\\nŒµ=8', 'PGD\\nŒµ=2', 'PGD\\nŒµ=4', 'PGD\\nŒµ=8', 'C&W']\n",
    "attack_keys_order = ['fgsm_2/255', 'fgsm_4/255', 'fgsm_8/255', 'pgd_2/255', 'pgd_4/255', 'pgd_8/255', 'cw']\n",
    "\n",
    "drops = []\n",
    "for key in attack_keys_order:\n",
    "    accs = [all_results[s][key]['robust_accuracy'] for s in all_results]\n",
    "    drop = clean_mean - np.mean(accs)\n",
    "    drops.append(drop)\n",
    "\n",
    "colors_drop = [COLORS['fgsm']]*3 + [COLORS['pgd']]*3 + [COLORS['cw']]\n",
    "bars = ax4.barh(attacks_order, drops, color=colors_drop, edgecolor='white', linewidth=1.5, alpha=0.85)\n",
    "\n",
    "ax4.set_xlabel('Accuracy Drop (%)', fontweight='bold')\n",
    "ax4.set_title('D) Robustness Degradation by Attack', fontsize=13, fontweight='bold')\n",
    "ax4.axvline(x=np.mean(drops), color='red', linestyle='--', linewidth=2, label=f'Mean Drop: {np.mean(drops):.1f}%')\n",
    "ax4.legend(loc='lower right')\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for bar, drop in zip(bars, drops):\n",
    "    ax4.annotate(f'{drop:.1f}%', xy=(bar.get_width(), bar.get_y() + bar.get_height()/2),\n",
    "                 xytext=(5, 0), textcoords='offset points', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ------ Panel E: Statistical Significance ------\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "\n",
    "# Calculate p-values (t-test: clean vs each attack)\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "p_values = []\n",
    "attack_labels = ['FGSM-8', 'PGD-8', 'C&W']\n",
    "for key in ['fgsm_8/255', 'pgd_8/255', 'cw']:\n",
    "    clean_accs = [all_results[s]['clean']['accuracy'] for s in all_results]\n",
    "    attack_accs = [all_results[s][key]['robust_accuracy'] for s in all_results]\n",
    "    _, p = scipy_stats.ttest_rel(clean_accs, attack_accs)\n",
    "    p_values.append(p)\n",
    "\n",
    "colors_p = ['green' if p < 0.05 else 'orange' for p in p_values]\n",
    "bars = ax5.barh(attack_labels, [-np.log10(p) for p in p_values], color=colors_p, edgecolor='white', linewidth=1.5)\n",
    "ax5.axvline(x=-np.log10(0.05), color='red', linestyle='--', linewidth=2, label='p=0.05 threshold')\n",
    "ax5.set_xlabel('-log‚ÇÅ‚ÇÄ(p-value)', fontweight='bold')\n",
    "ax5.set_title('E) Statistical Significance\\n(Paired t-test vs Clean)', fontsize=13, fontweight='bold')\n",
    "ax5.legend(loc='lower right')\n",
    "\n",
    "for bar, p in zip(bars, p_values):\n",
    "    ax5.annotate(f'p={p:.4f}', xy=(bar.get_width(), bar.get_y() + bar.get_height()/2),\n",
    "                 xytext=(5, 0), textcoords='offset points', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ------ Panel F: Key Findings Summary ------\n",
    "ax6 = fig.add_subplot(gs[2, :])\n",
    "ax6.axis('off')\n",
    "\n",
    "findings_text = f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                                    üî¨ KEY FINDINGS: ADVERSARIAL ROBUSTNESS ANALYSIS                                  ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                                                                      ‚ïë\n",
    "‚ïë  üìä BASELINE MODEL PERFORMANCE                           ‚îÇ  ‚ö†Ô∏è  VULNERABILITY ASSESSMENT                            ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚ïë\n",
    "‚ïë  ‚Ä¢ Clean Accuracy: {clean_mean:.2f}% (Mean ¬± {np.std([all_results[s]['clean']['accuracy'] for s in all_results]):.2f}%)         ‚îÇ  ‚Ä¢ Single-step FGSM reduces accuracy by {clean_mean - fgsm8_mean:.1f}% at Œµ=8/255      ‚ïë\n",
    "‚ïë  ‚Ä¢ Model: ResNet-50 (ImageNet pretrained)                ‚îÇ  ‚Ä¢ Iterative PGD-40 causes {clean_mean - pgd8_mean:.1f}% drop at Œµ=8/255          ‚ïë\n",
    "‚ïë  ‚Ä¢ Dataset: ISIC 2018 (7 dermoscopy classes)             ‚îÇ  ‚Ä¢ Optimization-based C&W achieves {clean_mean - cw_mean:.1f}% degradation    ‚ïë\n",
    "‚ïë  ‚Ä¢ Seeds evaluated: {', '.join(map(str, CONFIG['seeds']))}                        ‚îÇ  ‚Ä¢ Maximum observed drop: {max([clean_mean - fgsm8_mean, clean_mean - pgd8_mean, clean_mean - cw_mean]):.1f}%                           ‚ïë\n",
    "‚ïë                                                          ‚îÇ                                                           ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                                                                      ‚ïë\n",
    "‚ïë  üí° IMPLICATIONS FOR MEDICAL IMAGING                                                                                 ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚ïë\n",
    "‚ïë  1. Standard CNNs are HIGHLY VULNERABLE to adversarial perturbations - poses serious risk in clinical deployment     ‚ïë\n",
    "‚ïë  2. Even small perturbations (Œµ=2/255) cause measurable accuracy degradation - imperceptible to human eye            ‚ïë\n",
    "‚ïë  3. Cross-seed consistency shows vulnerability is SYSTEMATIC, not random - fundamental model weakness                ‚ïë\n",
    "‚ïë  4. ADVERSARIAL TRAINING (Phase 5) is ESSENTIAL before clinical deployment                                          ‚ïë\n",
    "‚ïë                                                                                                                      ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\"\n",
    "\n",
    "ax6.text(0.5, 0.5, findings_text, transform=ax6.transAxes, fontsize=10,\n",
    "         fontfamily='monospace', verticalalignment='center', horizontalalignment='center',\n",
    "         bbox=dict(boxstyle='round,pad=0.5', facecolor='#f8f9fa', edgecolor='#dee2e6', linewidth=2))\n",
    "\n",
    "plt.suptitle('Phase 4: Adversarial Robustness Evaluation ‚Äî ResNet-50 Baseline on ISIC 2018',\n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "plt.savefig(CONFIG['results_dir'] / 'figures' / 'robustness_dashboard.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: robustness_dashboard.png (300 DPI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef3ef4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "2eef3ef4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764463618474,
     "user_tz": 0,
     "elapsed": 3291,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "b239114a-1a8d-407a-babe-12632fb8ddba"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: ADVANCED PER-CLASS VULNERABILITY ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä PER-CLASS VULNERABILITY HEATMAP & ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===================== COMPREHENSIVE HEATMAP DATA =====================\n",
    "attacks_to_show = [\n",
    "    ('clean', 'Clean\\n(No Attack)'),\n",
    "    ('fgsm_2/255', 'FGSM\\nŒµ=2/255'),\n",
    "    ('fgsm_4/255', 'FGSM\\nŒµ=4/255'),\n",
    "    ('fgsm_8/255', 'FGSM\\nŒµ=8/255'),\n",
    "    ('pgd_2/255', 'PGD\\nŒµ=2/255'),\n",
    "    ('pgd_4/255', 'PGD\\nŒµ=4/255'),\n",
    "    ('pgd_8/255', 'PGD\\nŒµ=8/255'),\n",
    "    ('cw', 'C&W\\nL2'),\n",
    "]\n",
    "\n",
    "# Build comprehensive heatmap matrix\n",
    "heatmap_matrix = []\n",
    "for attack_key, attack_label in attacks_to_show:\n",
    "    row = []\n",
    "    for cls in CONFIG['class_names']:\n",
    "        if attack_key == 'clean':\n",
    "            # For clean, use predictions vs labels to compute per-class accuracy\n",
    "            all_preds = np.concatenate([all_results[s]['clean']['predictions'] for s in all_results])\n",
    "            all_labels = np.concatenate([all_results[s]['clean']['labels'] for s in all_results])\n",
    "            cls_idx = CONFIG['class_names'].index(cls)\n",
    "            cls_mask = all_labels == cls_idx\n",
    "            if cls_mask.sum() > 0:\n",
    "                cls_acc = (all_preds[cls_mask] == all_labels[cls_mask]).mean() * 100\n",
    "            else:\n",
    "                cls_acc = 0\n",
    "            row.append(cls_acc)\n",
    "        else:\n",
    "            accs = [all_results[s][attack_key]['per_class_robust_acc'][cls] for s in all_results]\n",
    "            row.append(np.mean(accs))\n",
    "    heatmap_matrix.append(row)\n",
    "\n",
    "heatmap_array = np.array(heatmap_matrix)\n",
    "\n",
    "# ===================== FIGURE: MULTI-PANEL VULNERABILITY ANALYSIS =====================\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.25, height_ratios=[1.3, 1])\n",
    "\n",
    "# ------ Panel A: Full Vulnerability Heatmap ------\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "\n",
    "# Create custom colormap (green = robust, red = vulnerable)\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors_cmap = ['#D32F2F', '#FF5722', '#FF9800', '#FFC107', '#CDDC39', '#8BC34A', '#4CAF50', '#2E7D32']\n",
    "cmap = LinearSegmentedColormap.from_list('vulnerability', colors_cmap)\n",
    "\n",
    "im = ax1.imshow(heatmap_array, cmap=cmap, aspect='auto', vmin=0, vmax=100)\n",
    "\n",
    "# Styling\n",
    "ax1.set_xticks(range(len(CONFIG['class_names'])))\n",
    "ax1.set_xticklabels([f'{name}\\n({desc.split(\"(\")[0].strip()[:15]})'\n",
    "                      for name, desc in zip(CONFIG['class_names'],\n",
    "                      [CLASS_DESCRIPTIONS.get(c, c) if 'CLASS_DESCRIPTIONS' in dir() else c for c in CONFIG['class_names']])],\n",
    "                    fontsize=10, fontweight='bold')\n",
    "ax1.set_yticks(range(len(attacks_to_show)))\n",
    "ax1.set_yticklabels([label for _, label in attacks_to_show], fontsize=10)\n",
    "\n",
    "# Add text annotations with adaptive coloring\n",
    "for i in range(len(attacks_to_show)):\n",
    "    for j in range(len(CONFIG['class_names'])):\n",
    "        val = heatmap_array[i, j]\n",
    "        text_color = 'white' if val < 40 else 'black'\n",
    "        ax1.text(j, i, f'{val:.1f}%', ha='center', va='center',\n",
    "                 color=text_color, fontsize=9, fontweight='bold')\n",
    "\n",
    "ax1.set_title('A) Per-Class Robustness Under All Attack Conditions\\n(Mean across 3 seeds)',\n",
    "              fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "# Colorbar with labels\n",
    "cbar = plt.colorbar(im, ax=ax1, shrink=0.8, pad=0.02)\n",
    "cbar.set_label('Accuracy (%)', fontsize=11, fontweight='bold')\n",
    "cbar.ax.set_yticks([0, 25, 50, 75, 100])\n",
    "cbar.ax.set_yticklabels(['0%\\n(Failed)', '25%', '50%', '75%', '100%\\n(Robust)'])\n",
    "\n",
    "# Add class vulnerability ranking annotation\n",
    "most_vulnerable_idx = np.argmin(heatmap_array[-2, :])  # PGD Œµ=8/255\n",
    "most_robust_idx = np.argmax(heatmap_array[-2, :])\n",
    "ax1.annotate('Most Vulnerable', xy=(most_vulnerable_idx, 6), xytext=(most_vulnerable_idx, 8),\n",
    "             fontsize=10, color='red', fontweight='bold', ha='center',\n",
    "             arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "\n",
    "# ------ Panel B: Class Vulnerability Ranking ------\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "# Calculate mean robustness across all attacks for each class\n",
    "class_robustness = []\n",
    "for j, cls in enumerate(CONFIG['class_names']):\n",
    "    mean_rob = np.mean(heatmap_array[1:, j])  # Exclude clean\n",
    "    class_robustness.append((cls, mean_rob))\n",
    "\n",
    "class_robustness.sort(key=lambda x: x[1])  # Sort by robustness (ascending = most vulnerable first)\n",
    "\n",
    "cls_names = [c[0] for c in class_robustness]\n",
    "cls_values = [c[1] for c in class_robustness]\n",
    "\n",
    "# Color bars by vulnerability\n",
    "colors_bars = [plt.cm.RdYlGn(v/100) for v in cls_values]\n",
    "bars = ax2.barh(cls_names, cls_values, color=colors_bars, edgecolor='white', linewidth=1.5)\n",
    "\n",
    "ax2.set_xlabel('Mean Robust Accuracy (%)', fontweight='bold')\n",
    "ax2.set_title('B) Class Vulnerability Ranking\\n(Lower = More Vulnerable)', fontsize=13, fontweight='bold')\n",
    "ax2.axvline(x=np.mean(cls_values), color='navy', linestyle='--', linewidth=2, label=f'Mean: {np.mean(cls_values):.1f}%')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.set_xlim(0, 100)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, val in zip(bars, cls_values):\n",
    "    ax2.annotate(f'{val:.1f}%', xy=(val, bar.get_y() + bar.get_height()/2),\n",
    "                 xytext=(5, 0), textcoords='offset points', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ------ Panel C: Attack Strength Impact by Class ------\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "# Line plot: accuracy degradation by epsilon for each class\n",
    "x_positions = [0, 2, 4, 8]  # Epsilon values * 255\n",
    "colors_class = plt.cm.tab10(np.linspace(0, 1, len(CONFIG['class_names'])))\n",
    "\n",
    "for j, cls in enumerate(CONFIG['class_names']):\n",
    "    class_accs = [heatmap_array[0, j]]  # Clean\n",
    "    class_accs.append(heatmap_array[4, j])  # PGD Œµ=2/255\n",
    "    class_accs.append(heatmap_array[5, j])  # PGD Œµ=4/255\n",
    "    class_accs.append(heatmap_array[6, j])  # PGD Œµ=8/255\n",
    "    ax3.plot(x_positions, class_accs, 'o-', color=colors_class[j],\n",
    "             label=cls, linewidth=2, markersize=8, alpha=0.8)\n",
    "\n",
    "ax3.set_xlabel('Perturbation Œµ (√ó255)', fontweight='bold')\n",
    "ax3.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "ax3.set_title('C) PGD Attack Impact by Class\\n(Accuracy vs Epsilon)', fontsize=13, fontweight='bold')\n",
    "ax3.set_xticks(x_positions)\n",
    "ax3.set_xticklabels(['0\\n(Clean)', '2', '4', '8'])\n",
    "ax3.legend(loc='upper right', ncol=2, fontsize=9)\n",
    "ax3.set_ylim(0, 100)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Per-Class Adversarial Vulnerability Analysis ‚Äî ISIC 2018 Dermoscopy',\n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.savefig(CONFIG['results_dir'] / 'figures' / 'class_vulnerability_analysis.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: class_vulnerability_analysis.png (300 DPI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fadd00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "40fadd00",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764463624713,
     "user_tz": 0,
     "elapsed": 6237,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "9a774744-6439-43d8-f36d-f48d7312559e"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: INTERACTIVE PLOTLY VISUALIZATIONS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä GENERATING INTERACTIVE PLOTLY VISUALIZATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ===================== FIGURE 1: INTERACTIVE ATTACK COMPARISON =====================\n",
    "# Prepare data for Plotly\n",
    "attack_data = []\n",
    "attack_keys = ['clean', 'fgsm_2/255', 'fgsm_4/255', 'fgsm_8/255',\n",
    "               'pgd_2/255', 'pgd_4/255', 'pgd_8/255', 'cw']\n",
    "attack_labels = ['Clean', 'FGSM Œµ=2/255', 'FGSM Œµ=4/255', 'FGSM Œµ=8/255',\n",
    "                 'PGD Œµ=2/255', 'PGD Œµ=4/255', 'PGD Œµ=8/255', 'C&W L2']\n",
    "attack_types = ['Baseline', 'FGSM', 'FGSM', 'FGSM', 'PGD', 'PGD', 'PGD', 'C&W']\n",
    "attack_colors = ['#2E8B57', '#4169E1', '#4169E1', '#4169E1', '#DC143C', '#DC143C', '#DC143C', '#9400D3']\n",
    "\n",
    "for seed in all_results:\n",
    "    for key, label, atype, color in zip(attack_keys, attack_labels, attack_types, attack_colors):\n",
    "        if key == 'clean':\n",
    "            acc = all_results[seed]['clean']['accuracy']\n",
    "        else:\n",
    "            acc = all_results[seed][key]['robust_accuracy']\n",
    "        attack_data.append({\n",
    "            'Seed': f'Seed {seed}',\n",
    "            'Attack': label,\n",
    "            'Attack Type': atype,\n",
    "            'Accuracy': acc,\n",
    "            'Color': color\n",
    "        })\n",
    "\n",
    "attack_df = pd.DataFrame(attack_data)\n",
    "\n",
    "# Create grouped bar chart\n",
    "fig1 = px.bar(\n",
    "    attack_df,\n",
    "    x='Attack',\n",
    "    y='Accuracy',\n",
    "    color='Seed',\n",
    "    barmode='group',\n",
    "    title='<b>Interactive Attack Comparison: Baseline Model Robustness</b><br><sup>Click legend to toggle seeds | Hover for details</sup>',\n",
    "    labels={'Accuracy': 'Accuracy (%)', 'Attack': 'Attack Configuration'},\n",
    "    color_discrete_sequence=['#FF6B6B', '#4ECDC4', '#45B7D1'],\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig1.update_layout(\n",
    "    font=dict(family='Arial', size=12),\n",
    "    title_font_size=18,\n",
    "    title_x=0.5,\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
    "    xaxis_tickangle=-45,\n",
    "    yaxis=dict(range=[0, 100], title_font_size=14),\n",
    "    xaxis=dict(title_font_size=14),\n",
    "    hovermode='x unified',\n",
    "    bargap=0.15,\n",
    "    bargroupgap=0.1\n",
    ")\n",
    "\n",
    "# Add mean line\n",
    "mean_clean = attack_df[attack_df['Attack'] == 'Clean']['Accuracy'].mean()\n",
    "fig1.add_hline(y=mean_clean, line_dash='dash', line_color='gray',\n",
    "               annotation_text=f'Clean Baseline: {mean_clean:.1f}%', annotation_position='top right')\n",
    "\n",
    "fig1.show()\n",
    "fig1.write_html(str(CONFIG['results_dir'] / 'figures' / 'attack_comparison_interactive.html'))\n",
    "print(\"‚úÖ Saved: attack_comparison_interactive.html\")\n",
    "\n",
    "# ===================== FIGURE 2: ROBUSTNESS DEGRADATION SURFACE =====================\n",
    "# Create 3D surface for PGD attack analysis\n",
    "fig2 = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    specs=[[{'type': 'scatter3d'}, {'type': 'heatmap'}]],\n",
    "    subplot_titles=('3D Robustness Surface', 'Attack Intensity Heatmap'),\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# Prepare 3D data\n",
    "epsilons = [0, 2, 4, 8]\n",
    "seeds_list = list(all_results.keys())\n",
    "\n",
    "X, Y = np.meshgrid(epsilons, range(len(seeds_list)))\n",
    "Z_fgsm = np.zeros_like(X, dtype=float)\n",
    "Z_pgd = np.zeros_like(X, dtype=float)\n",
    "\n",
    "for i, seed in enumerate(seeds_list):\n",
    "    Z_fgsm[i, 0] = all_results[seed]['clean']['accuracy']\n",
    "    Z_pgd[i, 0] = all_results[seed]['clean']['accuracy']\n",
    "    for j, eps in enumerate([2, 4, 8]):\n",
    "        Z_fgsm[i, j+1] = all_results[seed][f'fgsm_{eps}/255']['robust_accuracy']\n",
    "        Z_pgd[i, j+1] = all_results[seed][f'pgd_{eps}/255']['robust_accuracy']\n",
    "\n",
    "# Add 3D surface for PGD\n",
    "fig2.add_trace(\n",
    "    go.Surface(\n",
    "        x=X, y=Y, z=Z_pgd,\n",
    "        colorscale='RdYlGn',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title='Accuracy (%)', x=0.45),\n",
    "        name='PGD Robustness',\n",
    "        hovertemplate='Œµ=%{x}/255<br>Seed=%{y}<br>Accuracy=%{z:.1f}%<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add heatmap for both attacks\n",
    "combined_heatmap = np.vstack([Z_fgsm, Z_pgd])\n",
    "heatmap_labels = [f'FGSM-S{s}' for s in seeds_list] + [f'PGD-S{s}' for s in seeds_list]\n",
    "\n",
    "fig2.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=combined_heatmap,\n",
    "        x=['Œµ=0', 'Œµ=2/255', 'Œµ=4/255', 'Œµ=8/255'],\n",
    "        y=heatmap_labels,\n",
    "        colorscale='RdYlGn',\n",
    "        showscale=False,\n",
    "        text=np.round(combined_heatmap, 1),\n",
    "        texttemplate='%{text}%',\n",
    "        textfont=dict(size=10, color='black'),\n",
    "        hovertemplate='%{y}<br>%{x}<br>Accuracy: %{z:.1f}%<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig2.update_layout(\n",
    "    title='<b>3D Robustness Analysis: PGD Attack Surface</b><br><sup>Drag to rotate | Scroll to zoom</sup>',\n",
    "    title_font_size=16,\n",
    "    title_x=0.5,\n",
    "    font=dict(family='Arial', size=11),\n",
    "    template='plotly_white',\n",
    "    scene=dict(\n",
    "        xaxis_title='Epsilon (√ó255)',\n",
    "        yaxis_title='Seed Index',\n",
    "        zaxis_title='Accuracy (%)',\n",
    "        zaxis=dict(range=[0, 100]),\n",
    "        camera=dict(eye=dict(x=1.5, y=1.5, z=1))\n",
    "    ),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig2.show()\n",
    "fig2.write_html(str(CONFIG['results_dir'] / 'figures' / 'robustness_3d_surface.html'))\n",
    "print(\"‚úÖ Saved: robustness_3d_surface.html\")\n",
    "\n",
    "# ===================== FIGURE 3: ANIMATED ROBUSTNESS DEGRADATION =====================\n",
    "# Create animation data\n",
    "animation_data = []\n",
    "for eps_idx, eps in enumerate([0, 2, 4, 8]):\n",
    "    for seed in all_results:\n",
    "        for cls_idx, cls in enumerate(CONFIG['class_names']):\n",
    "            if eps == 0:\n",
    "                # Clean accuracy per class\n",
    "                all_preds = all_results[seed]['clean']['predictions']\n",
    "                all_labels = all_results[seed]['clean']['labels']\n",
    "                cls_mask = all_labels == cls_idx\n",
    "                acc = (all_preds[cls_mask] == all_labels[cls_mask]).mean() * 100 if cls_mask.sum() > 0 else 0\n",
    "            else:\n",
    "                acc = all_results[seed][f'pgd_{eps}/255']['per_class_robust_acc'][cls]\n",
    "\n",
    "            animation_data.append({\n",
    "                'Epsilon': f'Œµ={eps}/255' if eps > 0 else 'Clean',\n",
    "                'Epsilon_num': eps,\n",
    "                'Class': cls,\n",
    "                'Accuracy': acc,\n",
    "                'Seed': f'Seed {seed}'\n",
    "            })\n",
    "\n",
    "anim_df = pd.DataFrame(animation_data)\n",
    "\n",
    "fig3 = px.bar(\n",
    "    anim_df,\n",
    "    x='Class',\n",
    "    y='Accuracy',\n",
    "    color='Seed',\n",
    "    animation_frame='Epsilon',\n",
    "    title='<b>Animated: Per-Class Robustness Under Increasing PGD Attack Strength</b><br><sup>Press play to see degradation | Each bar = seed performance</sup>',\n",
    "    labels={'Accuracy': 'Accuracy (%)', 'Class': 'Skin Lesion Class'},\n",
    "    color_discrete_sequence=['#FF6B6B', '#4ECDC4', '#45B7D1'],\n",
    "    template='plotly_white',\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig3.update_layout(\n",
    "    font=dict(family='Arial', size=12),\n",
    "    title_font_size=16,\n",
    "    title_x=0.5,\n",
    "    yaxis=dict(range=[0, 100]),\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
    "    updatemenus=[dict(\n",
    "        type='buttons',\n",
    "        showactive=False,\n",
    "        y=0,\n",
    "        x=0.1,\n",
    "        xanchor='right',\n",
    "        yanchor='top',\n",
    "        buttons=[\n",
    "            dict(label='‚ñ∂ Play', method='animate', args=[None, {'frame': {'duration': 1000, 'redraw': True}, 'fromcurrent': True}]),\n",
    "            dict(label='‚è∏ Pause', method='animate', args=[[None], {'frame': {'duration': 0, 'redraw': False}, 'mode': 'immediate'}])\n",
    "        ]\n",
    "    )]\n",
    ")\n",
    "\n",
    "fig3.show()\n",
    "fig3.write_html(str(CONFIG['results_dir'] / 'figures' / 'animated_robustness.html'))\n",
    "print(\"‚úÖ Saved: animated_robustness.html\")\n",
    "\n",
    "# ===================== FIGURE 4: SUNBURST VULNERABILITY BREAKDOWN =====================\n",
    "sunburst_data = []\n",
    "for attack_type in ['FGSM', 'PGD', 'C&W']:\n",
    "    for eps in ['Weak (Œµ=2)', 'Medium (Œµ=4)', 'Strong (Œµ=8)'] if attack_type != 'C&W' else ['Optimization']:\n",
    "        for cls in CONFIG['class_names']:\n",
    "            if attack_type == 'FGSM':\n",
    "                key = f'fgsm_{eps.split(\"=\")[1].split(\")\")[0]}/255' if eps != 'Optimization' else None\n",
    "            elif attack_type == 'PGD':\n",
    "                key = f'pgd_{eps.split(\"=\")[1].split(\")\")[0]}/255' if eps != 'Optimization' else None\n",
    "            else:\n",
    "                key = 'cw'\n",
    "\n",
    "            if key:\n",
    "                accs = [all_results[s][key]['per_class_robust_acc'][cls] for s in all_results]\n",
    "                vulnerability = 100 - np.mean(accs)\n",
    "                sunburst_data.append({\n",
    "                    'Attack Type': attack_type,\n",
    "                    'Strength': eps,\n",
    "                    'Class': cls,\n",
    "                    'Vulnerability': vulnerability,\n",
    "                    'Path': f'{attack_type}/{eps}/{cls}'\n",
    "                })\n",
    "\n",
    "sunburst_df = pd.DataFrame(sunburst_data)\n",
    "\n",
    "fig4 = px.sunburst(\n",
    "    sunburst_df,\n",
    "    path=['Attack Type', 'Strength', 'Class'],\n",
    "    values='Vulnerability',\n",
    "    color='Vulnerability',\n",
    "    color_continuous_scale='Reds',\n",
    "    title='<b>Hierarchical Vulnerability Breakdown</b><br><sup>Click to drill down | Size = Vulnerability (100% - Accuracy)</sup>',\n",
    ")\n",
    "\n",
    "fig4.update_layout(\n",
    "    font=dict(family='Arial', size=12),\n",
    "    title_font_size=16,\n",
    "    title_x=0.5,\n",
    ")\n",
    "\n",
    "fig4.show()\n",
    "fig4.write_html(str(CONFIG['results_dir'] / 'figures' / 'vulnerability_sunburst.html'))\n",
    "print(\"‚úÖ Saved: vulnerability_sunburst.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868a778",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4868a778",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764463630549,
     "user_tz": 0,
     "elapsed": 5786,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "2771cd34-1123-4461-9540-320da1e9563b"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 14: COMPREHENSIVE RESULTS EXPORT & DISSERTATION-READY FIGURES\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"üíæ SAVING ALL RESULTS & GENERATING DISSERTATION FIGURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ===================== DISSERTATION-QUALITY SUMMARY FIGURE =====================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# ------ Panel A: Clean vs Robust Accuracy Comparison ------\n",
    "ax = axes[0, 0]\n",
    "clean_mean = np.mean([all_results[s]['clean']['accuracy'] for s in all_results])\n",
    "attacks_compare = [\n",
    "    ('Clean', clean_mean, '#2E8B57'),\n",
    "    ('FGSM Œµ=8/255', np.mean([all_results[s]['fgsm_8/255']['robust_accuracy'] for s in all_results]), '#4169E1'),\n",
    "    ('PGD-40 Œµ=8/255', np.mean([all_results[s]['pgd_8/255']['robust_accuracy'] for s in all_results]), '#DC143C'),\n",
    "    ('C&W L2', np.mean([all_results[s]['cw']['robust_accuracy'] for s in all_results]), '#9400D3'),\n",
    "]\n",
    "\n",
    "names = [a[0] for a in attacks_compare]\n",
    "values = [a[1] for a in attacks_compare]\n",
    "colors = [a[2] for a in attacks_compare]\n",
    "\n",
    "bars = ax.bar(names, values, color=colors, edgecolor='white', linewidth=2, alpha=0.85)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('(a) Model Accuracy Under Strongest Attacks', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.axhline(y=50, color='gray', linestyle=':', alpha=0.5)\n",
    "\n",
    "for bar, val in zip(bars, values):\n",
    "    ax.annotate(f'{val:.1f}%', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 5), textcoords='offset points', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add drop annotations\n",
    "for i, (name, val, color) in enumerate(attacks_compare[1:], 1):\n",
    "    drop = clean_mean - val\n",
    "    ax.annotate(f'‚Üì{drop:.1f}%', xy=(i, val/2), ha='center', va='center',\n",
    "                fontsize=10, color='white', fontweight='bold')\n",
    "\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# ------ Panel B: Epsilon Sensitivity ------\n",
    "ax = axes[0, 1]\n",
    "eps_vals = [2, 4, 8]\n",
    "\n",
    "fgsm_means = [np.mean([all_results[s][f'fgsm_{e}/255']['robust_accuracy'] for s in all_results]) for e in eps_vals]\n",
    "pgd_means = [np.mean([all_results[s][f'pgd_{e}/255']['robust_accuracy'] for s in all_results]) for e in eps_vals]\n",
    "fgsm_stds = [np.std([all_results[s][f'fgsm_{e}/255']['robust_accuracy'] for s in all_results]) for e in eps_vals]\n",
    "pgd_stds = [np.std([all_results[s][f'pgd_{e}/255']['robust_accuracy'] for s in all_results]) for e in eps_vals]\n",
    "\n",
    "ax.errorbar(eps_vals, fgsm_means, yerr=fgsm_stds, marker='o', markersize=12, linewidth=3,\n",
    "            color='#4169E1', label='FGSM (1-step)', capsize=5, capthick=2)\n",
    "ax.errorbar(eps_vals, pgd_means, yerr=pgd_stds, marker='s', markersize=12, linewidth=3,\n",
    "            color='#DC143C', label='PGD-40 (iterative)', capsize=5, capthick=2)\n",
    "ax.axhline(y=clean_mean, color='#2E8B57', linestyle='--', linewidth=2, label=f'Clean ({clean_mean:.1f}%)')\n",
    "\n",
    "ax.set_xlabel('Perturbation Budget Œµ (√ó255)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Robust Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('(b) Impact of Perturbation Strength', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(eps_vals)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.legend(loc='upper right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Fill area between curves\n",
    "ax.fill_between(eps_vals, fgsm_means, pgd_means, alpha=0.1, color='gray')\n",
    "ax.annotate('Gap', xy=(5, (fgsm_means[1] + pgd_means[1])/2), fontsize=10, style='italic')\n",
    "\n",
    "# ------ Panel C: Per-Class Vulnerability Radar ------\n",
    "ax = axes[1, 0]\n",
    "ax.axis('off')\n",
    "\n",
    "# Create radar subplot\n",
    "ax_radar = fig.add_subplot(2, 2, 3, projection='polar')\n",
    "\n",
    "# Data for strongest attack (PGD Œµ=8/255)\n",
    "values_radar = []\n",
    "for cls in CONFIG['class_names']:\n",
    "    accs = [all_results[s]['pgd_8/255']['per_class_robust_acc'][cls] for s in all_results]\n",
    "    values_radar.append(np.mean(accs))\n",
    "\n",
    "# Close the loop\n",
    "values_radar_closed = values_radar + [values_radar[0]]\n",
    "angles = np.linspace(0, 2*np.pi, len(CONFIG['class_names']), endpoint=False).tolist()\n",
    "angles += [angles[0]]\n",
    "\n",
    "ax_radar.plot(angles, values_radar_closed, 'o-', linewidth=2.5, color='#DC143C', markersize=8)\n",
    "ax_radar.fill(angles, values_radar_closed, alpha=0.25, color='#DC143C')\n",
    "ax_radar.set_xticks(angles[:-1])\n",
    "ax_radar.set_xticklabels(CONFIG['class_names'], fontsize=11, fontweight='bold')\n",
    "ax_radar.set_ylim(0, 100)\n",
    "ax_radar.set_title('(c) Per-Class Robustness Profile\\n(PGD-40 Œµ=8/255)', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# ------ Panel D: Confusion of Attack Success ------\n",
    "ax = axes[1, 1]\n",
    "\n",
    "# Calculate success rate per class (correctly classified ‚Üí misclassified)\n",
    "success_rates = []\n",
    "for cls in CONFIG['class_names']:\n",
    "    clean_acc = []\n",
    "    attack_acc = []\n",
    "    for seed in all_results:\n",
    "        all_preds = all_results[seed]['clean']['predictions']\n",
    "        all_labels = all_results[seed]['clean']['labels']\n",
    "        cls_idx = CONFIG['class_names'].index(cls)\n",
    "        cls_mask = all_labels == cls_idx\n",
    "        if cls_mask.sum() > 0:\n",
    "            clean_acc.append((all_preds[cls_mask] == all_labels[cls_mask]).mean() * 100)\n",
    "            attack_acc.append(all_results[seed]['pgd_8/255']['per_class_robust_acc'][cls])\n",
    "\n",
    "    if clean_acc:\n",
    "        drop = np.mean(clean_acc) - np.mean(attack_acc)\n",
    "        success_rates.append(drop)\n",
    "    else:\n",
    "        success_rates.append(0)\n",
    "\n",
    "sorted_indices = np.argsort(success_rates)[::-1]\n",
    "sorted_classes = [CONFIG['class_names'][i] for i in sorted_indices]\n",
    "sorted_rates = [success_rates[i] for i in sorted_indices]\n",
    "\n",
    "colors_sr = plt.cm.Reds(np.linspace(0.3, 0.9, len(sorted_classes)))\n",
    "bars = ax.barh(sorted_classes, sorted_rates, color=colors_sr, edgecolor='white', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Accuracy Drop Under PGD-40 Œµ=8/255 (%)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('(d) Class-wise Attack Success Rate', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=np.mean(sorted_rates), color='navy', linestyle='--', linewidth=2, label=f'Mean: {np.mean(sorted_rates):.1f}%')\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for bar, rate in zip(bars, sorted_rates):\n",
    "    ax.annotate(f'{rate:.1f}%', xy=(rate, bar.get_y() + bar.get_height()/2),\n",
    "                xytext=(5, 0), textcoords='offset points', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Phase 4: Adversarial Robustness Evaluation Summary\\nResNet-50 Baseline on ISIC 2018 Dermoscopy Dataset',\n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(CONFIG['results_dir'] / 'figures' / 'dissertation_figure_robustness.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.savefig(CONFIG['results_dir'] / 'figures' / 'dissertation_figure_robustness.pdf', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: dissertation_figure_robustness.png (300 DPI)\")\n",
    "print(\"‚úÖ Saved: dissertation_figure_robustness.pdf (Vector)\")\n",
    "\n",
    "# ===================== EXPORT ALL RESULTS =====================\n",
    "# Prepare comprehensive JSON export\n",
    "export_results = {\n",
    "    'metadata': {\n",
    "        'experiment': 'Phase 4 Adversarial Robustness Evaluation',\n",
    "        'model': 'ResNet-50 (ImageNet pretrained)',\n",
    "        'dataset': 'ISIC 2018 Dermoscopy',\n",
    "        'num_classes': 7,\n",
    "        'class_names': CONFIG['class_names'],\n",
    "        'seeds_evaluated': list(all_results.keys()),\n",
    "        'attacks_evaluated': ['FGSM', 'PGD-40', 'Carlini-Wagner L2'],\n",
    "        'epsilons': [2/255, 4/255, 8/255],\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    },\n",
    "    'summary': {\n",
    "        'clean_accuracy': {\n",
    "            'mean': float(np.mean([all_results[s]['clean']['accuracy'] for s in all_results])),\n",
    "            'std': float(np.std([all_results[s]['clean']['accuracy'] for s in all_results])),\n",
    "            'per_seed': {str(s): float(all_results[s]['clean']['accuracy']) for s in all_results}\n",
    "        },\n",
    "        'robust_accuracy_fgsm_8': {\n",
    "            'mean': float(np.mean([all_results[s]['fgsm_8/255']['robust_accuracy'] for s in all_results])),\n",
    "            'std': float(np.std([all_results[s]['fgsm_8/255']['robust_accuracy'] for s in all_results])),\n",
    "        },\n",
    "        'robust_accuracy_pgd_8': {\n",
    "            'mean': float(np.mean([all_results[s]['pgd_8/255']['robust_accuracy'] for s in all_results])),\n",
    "            'std': float(np.std([all_results[s]['pgd_8/255']['robust_accuracy'] for s in all_results])),\n",
    "        },\n",
    "        'robust_accuracy_cw': {\n",
    "            'mean': float(np.mean([all_results[s]['cw']['robust_accuracy'] for s in all_results])),\n",
    "            'std': float(np.std([all_results[s]['cw']['robust_accuracy'] for s in all_results])),\n",
    "        },\n",
    "    },\n",
    "    'detailed_results': {}\n",
    "}\n",
    "\n",
    "for seed in all_results:\n",
    "    export_results['detailed_results'][str(seed)] = {}\n",
    "    for attack_key, attack_result in all_results[seed].items():\n",
    "        export_results['detailed_results'][str(seed)][attack_key] = {\n",
    "            k: v.tolist() if isinstance(v, np.ndarray) else v\n",
    "            for k, v in attack_result.items()\n",
    "            if k not in ['predictions', 'labels', 'probs', 'confusion_matrix']\n",
    "        }\n",
    "\n",
    "# Save JSON\n",
    "results_file = CONFIG['results_dir'] / 'adversarial_results_full.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(export_results, f, indent=2)\n",
    "print(f\"‚úÖ Full results saved to: {results_file}\")\n",
    "\n",
    "# ===================== LIST ALL SAVED FILES =====================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìÅ ALL SAVED FILES IN {CONFIG['results_dir']}:\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for f in sorted(CONFIG['results_dir'].glob('**/*')):\n",
    "    if f.is_file():\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        rel_path = f.relative_to(CONFIG['results_dir'])\n",
    "        icon = 'üìä' if f.suffix in ['.png', '.pdf'] else 'üìÑ' if f.suffix == '.html' else 'üíæ'\n",
    "        print(f\"   {icon} {rel_path} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2c5df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cf2c5df",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764463630573,
     "user_tz": 0,
     "elapsed": 16,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "cf893dff-1d97-4adb-add3-a440b3f25516"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 15: EXECUTIVE SUMMARY & NEXT STEPS\n",
    "# ============================================================================\n",
    "print(\"\\n\")\n",
    "print(\"‚ïî\" + \"‚ïê\"*78 + \"‚ïó\")\n",
    "print(\"‚ïë\" + \" \"*25 + \"üéØ PHASE 4 COMPLETE\" + \" \"*25 + \"‚ïë\")\n",
    "print(\"‚ïë\" + \" \"*15 + \"ADVERSARIAL ROBUSTNESS EVALUATION SUMMARY\" + \" \"*14 + \"‚ïë\")\n",
    "print(\"‚ïö\" + \"‚ïê\"*78 + \"‚ïù\")\n",
    "\n",
    "# ===================== KEY METRICS =====================\n",
    "clean_mean = np.mean([all_results[s]['clean']['accuracy'] for s in all_results])\n",
    "clean_std = np.std([all_results[s]['clean']['accuracy'] for s in all_results])\n",
    "fgsm8_mean = np.mean([all_results[s]['fgsm_8/255']['robust_accuracy'] for s in all_results])\n",
    "pgd8_mean = np.mean([all_results[s]['pgd_8/255']['robust_accuracy'] for s in all_results])\n",
    "cw_mean = np.mean([all_results[s]['cw']['robust_accuracy'] for s in all_results])\n",
    "\n",
    "# Calculate average drop\n",
    "avg_drop = np.mean([clean_mean - fgsm8_mean, clean_mean - pgd8_mean, clean_mean - cw_mean])\n",
    "\n",
    "print(f\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                      üìä BASELINE MODEL PERFORMANCE                          ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ   üéØ Clean Accuracy:         {clean_mean:>6.2f}% ¬± {clean_std:.2f}%                            ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ   ‚öîÔ∏è  Adversarial Robustness (Strong Attacks):                              ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ FGSM (Œµ=8/255):        {fgsm8_mean:>6.2f}%  ‚îÇ  Drop: {clean_mean - fgsm8_mean:>5.2f}%                 ‚îÇ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ PGD-40 (Œµ=8/255):      {pgd8_mean:>6.2f}%  ‚îÇ  Drop: {clean_mean - pgd8_mean:>5.2f}%                 ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Carlini-Wagner L2:     {cw_mean:>6.2f}%  ‚îÇ  Drop: {clean_mean - cw_mean:>5.2f}%                 ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ   üìâ Average Robustness Drop: {avg_drop:.2f}%                                       ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")\n",
    "\n",
    "# ===================== KEY FINDINGS =====================\n",
    "most_vulnerable = CONFIG['class_names'][np.argmin([\n",
    "    np.mean([all_results[s]['pgd_8/255']['per_class_robust_acc'][c] for s in all_results])\n",
    "    for c in CONFIG['class_names']\n",
    "])]\n",
    "most_robust = CONFIG['class_names'][np.argmax([\n",
    "    np.mean([all_results[s]['pgd_8/255']['per_class_robust_acc'][c] for s in all_results])\n",
    "    for c in CONFIG['class_names']\n",
    "])]\n",
    "\n",
    "print(f\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                         üî¨ KEY RESEARCH FINDINGS                            ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ  1Ô∏è‚É£  CRITICAL VULNERABILITY                                                 ‚îÇ\n",
    "‚îÇ      Standard CNNs show SEVERE vulnerability to adversarial attacks         ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Up to {max([clean_mean - fgsm8_mean, clean_mean - pgd8_mean, clean_mean - cw_mean]):.1f}% accuracy degradation under imperceptible perturbations    ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Perturbations invisible to human eye (Œµ ‚â§ 8/255)                     ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ  2Ô∏è‚É£  ATTACK COMPARISON                                                      ‚îÇ\n",
    "‚îÇ      ‚Ä¢ PGD-40 is more effective than single-step FGSM                       ‚îÇ\n",
    "‚îÇ      ‚Ä¢ C&W finds minimum perturbation for misclassification                 ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Iterative attacks reveal true model fragility                        ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ  3Ô∏è‚É£  CLASS-WISE ANALYSIS                                                    ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Most vulnerable class: {most_vulnerable:<8}                                      ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Most robust class: {most_robust:<8}                                          ‚îÇ\n",
    "‚îÇ      ‚Ä¢ Vulnerability varies significantly across classes                    ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ  4Ô∏è‚É£  CLINICAL IMPLICATIONS                                                  ‚îÇ\n",
    "‚îÇ      ‚ö†Ô∏è  Baseline models are NOT SAFE for clinical deployment               ‚îÇ\n",
    "‚îÇ      ‚ö†Ô∏è  Adversarial training is ESSENTIAL before real-world use            ‚îÇ\n",
    "‚îÇ      ‚ö†Ô∏è  All skin lesion classes show significant vulnerability             ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")\n",
    "\n",
    "# ===================== VISUALIZATIONS GENERATED =====================\n",
    "print(f\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                      üìä VISUALIZATIONS GENERATED                            ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ  Static Figures (PNG/PDF, 300 DPI):                                         ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ‚îÄ üìà robustness_dashboard.png          - Multi-panel analysis            ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ‚îÄ üî• class_vulnerability_analysis.png  - Per-class breakdown             ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ üìÑ dissertation_figure_robustness.pdf - Publication-ready              ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ  Interactive Figures (HTML):                                                ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ‚îÄ üñ±Ô∏è  attack_comparison_interactive.html                                 ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ‚îÄ üåê robustness_3d_surface.html        - 3D rotatable surface            ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ‚îÄ ‚ñ∂Ô∏è  animated_robustness.html          - Epsilon animation              ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ üå≥ vulnerability_sunburst.html       - Hierarchical breakdown          ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")\n",
    "\n",
    "# ===================== NEXT STEPS =====================\n",
    "print(f\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                          üöÄ NEXT STEPS                                      ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ  ‚úÖ PHASE 4 COMPLETE: Adversarial Robustness Evaluation                     ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ  üìå PHASE 5: Tri-Objective Robust Training                                  ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Implement adversarial training with PGD-AT                            ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Add explainability preservation objective                             ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Multi-objective optimization (Accuracy + Robustness + XAI)            ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ  üìå PHASE 6: Explainability Analysis                                        ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Grad-CAM visualization comparison                                     ‚îÇ\n",
    "‚îÇ     ‚Ä¢ SHAP analysis for feature importance                                  ‚îÇ\n",
    "‚îÇ     ‚Ä¢ XAI consistency under adversarial perturbations                       ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ  üìå PHASE 7: Comparative Evaluation                                         ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Baseline vs Robust model comparison                                   ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Trade-off analysis (Accuracy-Robustness-Explainability)               ‚îÇ\n",
    "‚îÇ     ‚Ä¢ Statistical significance testing                                      ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚ïê\"*80)\n",
    "print(\"   ‚úÖ PHASE 4 ADVERSARIAL ROBUSTNESS EVALUATION SUCCESSFULLY COMPLETED!\")\n",
    "print(\"‚ïê\"*80)\n",
    "print(f\"\\nüìÅ All results saved to: {CONFIG['results_dir']}\")\n",
    "print(f\"üìä Total figures generated: 7 (4 static + 4 interactive)\")\n",
    "print(f\"üíæ Data files: adversarial_results.csv, adversarial_results_full.json\")\n",
    "print(\"\\nüîó Run Phase 5 notebook to continue with adversarial training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1c302",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29e1c302",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764463666647,
     "user_tz": 0,
     "elapsed": 36073,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "f922194a-b9ce-4635-f553-3fbd8d5898eb"
   },
   "outputs": [],
   "source": [
    "#@title üîß Cell 2: Environment Setup & Dependencies\n",
    "#@markdown **Run this cell first to install all required packages**\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for adversarial evaluation.\"\"\"\n",
    "    packages = [\n",
    "        \"torch>=2.0.0\",\n",
    "        \"torchvision>=0.15.0\",\n",
    "        \"timm>=0.9.0\",\n",
    "        \"albumentations>=1.3.0\",\n",
    "        \"scikit-learn>=1.3.0\",\n",
    "        \"pandas>=2.0.0\",\n",
    "        \"numpy>=1.24.0\",\n",
    "        \"matplotlib>=3.7.0\",\n",
    "        \"seaborn>=0.12.0\",\n",
    "        \"plotly>=5.15.0\",\n",
    "        \"kaleido\",  # For plotly static export\n",
    "        \"tqdm>=4.65.0\",\n",
    "        \"mlflow>=2.5.0\",\n",
    "        \"scipy>=1.11.0\",\n",
    "    ]\n",
    "\n",
    "    print(\"üì¶ Installing packages...\")\n",
    "    for pkg in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "    print(\"‚úÖ All packages installed!\")\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üåê Running in Google Colab\")\n",
    "    install_packages()\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Running locally\")\n",
    "\n",
    "# Core imports\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 8),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.dpi': 100,\n",
    "})\n",
    "\n",
    "# GPU Configuration\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üñ•Ô∏è  HARDWARE CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úÖ GPU: {gpu_name}\")\n",
    "    print(f\"‚úÖ VRAM: {gpu_mem:.1f} GB\")\n",
    "\n",
    "    # Enable optimizations for A100/Ampere GPUs\n",
    "    if \"A100\" in gpu_name or torch.cuda.get_device_capability()[0] >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"‚úÖ TF32 enabled for Ampere GPU\")\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"‚úÖ cuDNN benchmark enabled\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è No GPU found, using CPU (will be slow)\")\n",
    "\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ Device: {device}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc9edb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ffc9edb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764463668144,
     "user_tz": 0,
     "elapsed": 1498,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "1aa88d66-a54f-4ac2-80f4-525047c56e08"
   },
   "outputs": [],
   "source": [
    "#@title üóÇÔ∏è Cell 3: Mount Google Drive & Configure Paths\n",
    "#@markdown **Configure data and checkpoint paths**\n",
    "\n",
    "# Mount Google Drive (Colab only)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    print(\"‚úÖ Google Drive mounted\")\n",
    "\n",
    "# ============================================================================\n",
    "# PATH CONFIGURATION - Adjust these paths as needed\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class PathConfig:\n",
    "    \"\"\"Central path configuration for the evaluation pipeline.\"\"\"\n",
    "\n",
    "    # Base paths\n",
    "    drive_base: Path = Path(\"/content/drive/MyDrive\")\n",
    "\n",
    "    # Data paths\n",
    "    data_root: Path = field(default=None)\n",
    "    train_dir: Path = field(default=None)\n",
    "    val_dir: Path = field(default=None)\n",
    "    test_dir: Path = field(default=None)\n",
    "\n",
    "    # Checkpoint paths\n",
    "    checkpoint_dir: Path = field(default=None)\n",
    "\n",
    "    # Output paths\n",
    "    results_dir: Path = field(default=None)\n",
    "    figures_dir: Path = field(default=None)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize derived paths with auto-detection.\"\"\"\n",
    "        # Try multiple common data path patterns\n",
    "        if self.data_root is None:\n",
    "            data_candidates = [\n",
    "                self.drive_base / \"isic2018\",                    # isic2018/\n",
    "                self.drive_base / \"data\" / \"isic2018\",           # data/isic2018/\n",
    "                self.drive_base / \"data\" / \"isic_2018\",          # data/isic_2018/\n",
    "                self.drive_base / \"data\" / \"data\" / \"isic_2018\", # data/data/isic_2018/\n",
    "                self.drive_base / \"ISIC2018\",                    # ISIC2018/\n",
    "            ]\n",
    "\n",
    "            for candidate in data_candidates:\n",
    "                if candidate.exists():\n",
    "                    self.data_root = candidate\n",
    "                    print(f\"üìÅ Auto-detected data root: {candidate}\")\n",
    "                    break\n",
    "\n",
    "            if self.data_root is None:\n",
    "                # Default fallback\n",
    "                self.data_root = self.drive_base / \"isic2018\"\n",
    "                print(f\"‚ö†Ô∏è Using default data root: {self.data_root}\")\n",
    "\n",
    "        if self.train_dir is None:\n",
    "            self.train_dir = self.data_root / \"train\"\n",
    "        if self.val_dir is None:\n",
    "            self.val_dir = self.data_root / \"val\"\n",
    "        if self.test_dir is None:\n",
    "            self.test_dir = self.data_root / \"test\"\n",
    "        if self.checkpoint_dir is None:\n",
    "            self.checkpoint_dir = self.drive_base / \"checkpoints\" / \"baseline\"\n",
    "        if self.results_dir is None:\n",
    "            self.results_dir = self.drive_base / \"results\" / \"phase4_adversarial\"\n",
    "        if self.figures_dir is None:\n",
    "            self.figures_dir = self.results_dir / \"figures\"\n",
    "\n",
    "    def validate(self) -> bool:\n",
    "        \"\"\"Validate that required paths exist.\"\"\"\n",
    "        required = [self.data_root, self.test_dir, self.checkpoint_dir]\n",
    "        missing = [p for p in required if not p.exists()]\n",
    "\n",
    "        if missing:\n",
    "            print(\"‚ùå Missing paths:\")\n",
    "            for p in missing:\n",
    "                print(f\"   - {p}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def create_output_dirs(self):\n",
    "        \"\"\"Create output directories if they don't exist.\"\"\"\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"‚úÖ Results directory: {self.results_dir}\")\n",
    "        print(f\"‚úÖ Figures directory: {self.figures_dir}\")\n",
    "\n",
    "# Initialize paths\n",
    "paths = PathConfig()\n",
    "\n",
    "# Validate paths\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìÅ PATH VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if paths.validate():\n",
    "    print(f\"‚úÖ Data root: {paths.data_root}\")\n",
    "    print(f\"‚úÖ Test directory: {paths.test_dir}\")\n",
    "    print(f\"‚úÖ Checkpoint directory: {paths.checkpoint_dir}\")\n",
    "    paths.create_output_dirs()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Paths not found! Let's diagnose...\")\n",
    "\n",
    "    # Show what's actually in Google Drive\n",
    "    drive_base = Path(\"/content/drive/MyDrive\")\n",
    "    print(f\"\\nüîç Contents of {drive_base}:\")\n",
    "    if drive_base.exists():\n",
    "        for item in sorted(drive_base.iterdir())[:20]:\n",
    "            item_type = \"üìÅ\" if item.is_dir() else \"üìÑ\"\n",
    "            print(f\"   {item_type} {item.name}\")\n",
    "\n",
    "        # Look for data folders\n",
    "        print(f\"\\nüîç Looking for data folders...\")\n",
    "        for pattern in [\"*isic*\", \"*ISIC*\", \"*data*\", \"*Data*\"]:\n",
    "            matches = list(drive_base.glob(pattern))\n",
    "            if matches:\n",
    "                print(f\"   Found matching '{pattern}':\")\n",
    "                for m in matches[:5]:\n",
    "                    print(f\"      üìÅ {m}\")\n",
    "                    if m.is_dir():\n",
    "                        # Show contents\n",
    "                        try:\n",
    "                            contents = list(m.iterdir())[:5]\n",
    "                            for c in contents:\n",
    "                                print(f\"         {'üìÅ' if c.is_dir() else 'üìÑ'} {c.name}\")\n",
    "                        except:\n",
    "                            pass\n",
    "    else:\n",
    "        print(\"   ‚ùå Google Drive not mounted!\")\n",
    "\n",
    "    print(\"\\nüí° To fix, update the PathConfig class with your actual paths:\")\n",
    "    print(\"   1. Find where your ISIC data is stored\")\n",
    "    print(\"   2. Update 'data_root' in PathConfig.__post_init__()\")\n",
    "\n",
    "# List available checkpoints\n",
    "print(\"\\nüì¶ Available Checkpoints:\")\n",
    "if paths.checkpoint_dir.exists():\n",
    "    # Check for subdirectories (seed_42/, seed_123/, etc.)\n",
    "    subdirs = [d for d in paths.checkpoint_dir.iterdir() if d.is_dir()]\n",
    "    if subdirs:\n",
    "        print(\"   üìÅ Checkpoint subdirectories:\")\n",
    "        for subdir in sorted(subdirs):\n",
    "            pt_files = list(subdir.glob(\"*.pt\"))\n",
    "            print(f\"      {subdir.name}/: {[f.name for f in pt_files]}\")\n",
    "\n",
    "    # Check for flat .pt files\n",
    "    checkpoints = list(paths.checkpoint_dir.glob(\"*.pt\"))\n",
    "    if checkpoints:\n",
    "        print(\"   üìÑ Checkpoint files:\")\n",
    "        for ckpt in sorted(checkpoints):\n",
    "            size_mb = ckpt.stat().st_size / 1e6\n",
    "            print(f\"      - {ckpt.name} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Checkpoint directory not found!\")\n",
    "    print(f\"   Expected: {paths.checkpoint_dir}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e498f9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e498f9e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764463671649,
     "user_tz": 0,
     "elapsed": 3504,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "68f5d8bb-b12d-4935-9478-3c210910a53d"
   },
   "outputs": [],
   "source": [
    "#@title üì• Cell 4: Clone Repository & Import Attack Classes\n",
    "#@markdown **Clone the project repository and import custom modules**\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Clone repository (Colab only)\n",
    "REPO_URL = \"https://github.com/viraj1011JAIN/tri-objective-robust-xai-medimg.git\"\n",
    "REPO_DIR = \"/content/tri-objective-robust-xai-medimg\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists(REPO_DIR):\n",
    "        print(f\"üì• Cloning repository...\")\n",
    "        os.system(f\"git clone {REPO_URL} {REPO_DIR}\")\n",
    "        print(\"‚úÖ Repository cloned!\")\n",
    "    else:\n",
    "        print(\"üìÅ Repository already exists, pulling latest...\")\n",
    "        os.system(f\"cd {REPO_DIR} && git pull\")\n",
    "\n",
    "    # Add to Python path\n",
    "    if REPO_DIR not in sys.path:\n",
    "        sys.path.insert(0, REPO_DIR)\n",
    "    print(f\"‚úÖ Added {REPO_DIR} to Python path\")\n",
    "else:\n",
    "    # Local development - find project root\n",
    "    current_dir = Path.cwd()\n",
    "    if \"notebooks\" in str(current_dir):\n",
    "        project_root = current_dir.parent\n",
    "    else:\n",
    "        project_root = current_dir\n",
    "\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "    print(f\"‚úÖ Using local project: {project_root}\")\n",
    "\n",
    "# Import custom attack classes\n",
    "print(\"\\nüîß Importing attack modules...\")\n",
    "\n",
    "try:\n",
    "    from src.attacks.fgsm import FGSM, FGSMConfig, fgsm_attack\n",
    "    from src.attacks.pgd import PGD, PGDConfig, pgd_attack\n",
    "    from src.attacks.cw import CarliniWagner, CWConfig, cw_attack\n",
    "    from src.attacks.base import AttackConfig, AttackResult\n",
    "    print(\"‚úÖ FGSM attack imported\")\n",
    "    print(\"‚úÖ PGD attack imported\")\n",
    "    print(\"‚úÖ Carlini-Wagner attack imported\")\n",
    "    print(\"‚úÖ Base attack classes imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"‚ö†Ô∏è Please ensure the repository is properly cloned\")\n",
    "    raise\n",
    "\n",
    "# Import dataset utilities\n",
    "try:\n",
    "    from src.datasets.isic import ISICDataset\n",
    "    print(\"‚úÖ ISICDataset imported\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è ISICDataset not found, will use custom implementation\")\n",
    "    ISICDataset = None\n",
    "\n",
    "# Import model utilities\n",
    "import timm\n",
    "print(f\"‚úÖ timm version: {timm.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ALL MODULES IMPORTED SUCCESSFULLY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf885ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edf885ef",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764464138529,
     "user_tz": 0,
     "elapsed": 48,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "7bfd064e-007e-4f3c-d607-0359b4438614"
   },
   "outputs": [],
   "source": [
    "#@title üìä Cell 5: Dataset & Model Loading Utilities\n",
    "#@markdown **Define dataset wrapper and model loading functions**\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ============================================================================\n",
    "# ISIC 2018 CLASS INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"AKIEC\",  # Actinic Keratoses\n",
    "    \"BCC\",    # Basal Cell Carcinoma\n",
    "    \"BKL\",    # Benign Keratosis\n",
    "    \"DF\",     # Dermatofibroma\n",
    "    \"MEL\",    # Melanoma\n",
    "    \"NV\",     # Melanocytic Nevi\n",
    "    \"VASC\"    # Vascular Lesions\n",
    "]\n",
    "\n",
    "CLASS_DESCRIPTIONS = {\n",
    "    \"AKIEC\": \"Actinic Keratoses (pre-cancerous)\",\n",
    "    \"BCC\": \"Basal Cell Carcinoma (cancerous)\",\n",
    "    \"BKL\": \"Benign Keratosis (non-cancerous)\",\n",
    "    \"DF\": \"Dermatofibroma (benign)\",\n",
    "    \"MEL\": \"Melanoma (malignant, dangerous)\",\n",
    "    \"NV\": \"Melanocytic Nevi (common moles)\",\n",
    "    \"VASC\": \"Vascular Lesions (blood vessel)\"\n",
    "}\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# ImageNet normalization (used by pretrained models)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class ISICTestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ISIC 2018 Test Dataset for adversarial evaluation.\n",
    "\n",
    "    Returns unnormalized images in [0, 1] range for adversarial attacks.\n",
    "    Normalization is applied separately during model inference.\n",
    "\n",
    "    Supports two data formats:\n",
    "    1. Class subdirectories: test/AKIEC/*.jpg, test/BCC/*.jpg, etc.\n",
    "    2. Flat structure with CSV: test/*.jpg + metadata.csv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: Path,\n",
    "        transform: Optional[A.Compose] = None,\n",
    "        max_samples: Optional[int] = None,\n",
    "        csv_path: Optional[Path] = None\n",
    "    ):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform or self._default_transform()\n",
    "\n",
    "        # Collect samples\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {name: idx for idx, name in enumerate(CLASS_NAMES)}\n",
    "\n",
    "        # Diagnostic: show what's in root_dir\n",
    "        print(f\"üìÅ Looking for test data in: {self.root_dir}\")\n",
    "        if self.root_dir.exists():\n",
    "            contents = list(self.root_dir.iterdir())[:10]\n",
    "            print(f\"   üìÇ Contents: {[c.name for c in contents]}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Directory does not exist!\")\n",
    "            # Try to find test data elsewhere\n",
    "            parent = self.root_dir.parent\n",
    "            print(f\"   üîç Checking parent: {parent}\")\n",
    "            if parent.exists():\n",
    "                contents = list(parent.iterdir())[:10]\n",
    "                print(f\"   üìÇ Parent contents: {[c.name for c in contents]}\")\n",
    "\n",
    "        # Method 1: Try class subdirectories\n",
    "        found_via_subdirs = False\n",
    "        for class_name in CLASS_NAMES:\n",
    "            class_dir = self.root_dir / class_name\n",
    "            if class_dir.exists():\n",
    "                images = list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\")) + list(class_dir.glob(\"*.jpeg\"))\n",
    "                for img_path in images:\n",
    "                    self.samples.append((img_path, self.class_to_idx[class_name]))\n",
    "                if images:\n",
    "                    found_via_subdirs = True\n",
    "\n",
    "        # Method 2: Try CSV if no samples found\n",
    "        if not self.samples:\n",
    "            # Look for metadata CSV\n",
    "            csv_candidates = [\n",
    "                csv_path,\n",
    "                self.root_dir / \"metadata.csv\",\n",
    "                self.root_dir.parent / \"metadata.csv\",\n",
    "                self.root_dir.parent / \"metadata_fixed.csv\",\n",
    "            ]\n",
    "\n",
    "            for csv_file in csv_candidates:\n",
    "                if csv_file and Path(csv_file).exists():\n",
    "                    print(f\"   üìÑ Found CSV: {csv_file}\")\n",
    "                    try:\n",
    "                        import pandas as pd\n",
    "                        df = pd.read_csv(csv_file)\n",
    "\n",
    "                        # Filter for test split\n",
    "                        if 'split' in df.columns:\n",
    "                            test_df = df[df['split'] == 'test']\n",
    "                        else:\n",
    "                            test_df = df\n",
    "\n",
    "                        print(f\"   üìä CSV has {len(test_df)} test samples\")\n",
    "\n",
    "                        # Get image paths\n",
    "                        img_col = 'image_path' if 'image_path' in df.columns else 'image'\n",
    "                        label_col = 'label' if 'label' in df.columns else 'dx'\n",
    "\n",
    "                        # Data root for resolving relative paths\n",
    "                        data_root = self.root_dir.parent\n",
    "\n",
    "                        valid_count = 0\n",
    "                        missing_count = 0\n",
    "\n",
    "                        for _, row in test_df.iterrows():\n",
    "                            raw_path = str(row[img_col])\n",
    "\n",
    "                            # Fix Windows backslashes to forward slashes\n",
    "                            raw_path = raw_path.replace('\\\\', '/')\n",
    "\n",
    "                            # Try multiple path resolutions\n",
    "                            candidates = [\n",
    "                                data_root / raw_path,                    # Relative to data root\n",
    "                                self.root_dir / raw_path,                # Relative to test dir\n",
    "                                Path(raw_path),                          # Absolute path\n",
    "                                data_root / Path(raw_path).name,         # Just filename in data root\n",
    "                            ]\n",
    "\n",
    "                            img_path = None\n",
    "                            for candidate in candidates:\n",
    "                                if candidate.exists():\n",
    "                                    img_path = candidate\n",
    "                                    break\n",
    "\n",
    "                            if img_path is None:\n",
    "                                missing_count += 1\n",
    "                                if missing_count <= 3:\n",
    "                                    print(f\"   ‚ö†Ô∏è Missing: {raw_path}\")\n",
    "                                continue\n",
    "\n",
    "                            label = row[label_col]\n",
    "                            if isinstance(label, str):\n",
    "                                label_idx = self.class_to_idx.get(label.upper(), -1)\n",
    "                            else:\n",
    "                                label_idx = int(label)\n",
    "\n",
    "                            if label_idx >= 0:\n",
    "                                self.samples.append((img_path, label_idx))\n",
    "                                valid_count += 1\n",
    "\n",
    "                        if missing_count > 3:\n",
    "                            print(f\"   ‚ö†Ô∏è ... and {missing_count - 3} more missing files\")\n",
    "\n",
    "                        if self.samples:\n",
    "                            print(f\"   ‚úÖ Loaded {len(self.samples)} samples from CSV\")\n",
    "                            if missing_count > 0:\n",
    "                                print(f\"   ‚ö†Ô∏è Skipped {missing_count} missing files\")\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        print(f\"   ‚ö†Ô∏è CSV loading error: {e}\")\n",
    "                        import traceback\n",
    "                        traceback.print_exc()\n",
    "\n",
    "        # Limit samples if specified\n",
    "        if max_samples and len(self.samples) > max_samples:\n",
    "            # Stratified sampling\n",
    "            from collections import defaultdict\n",
    "            by_class = defaultdict(list)\n",
    "            for path, label in self.samples:\n",
    "                by_class[label].append((path, label))\n",
    "\n",
    "            per_class = max_samples // NUM_CLASSES\n",
    "            self.samples = []\n",
    "            for label, items in by_class.items():\n",
    "                self.samples.extend(items[:per_class])\n",
    "\n",
    "        print(f\"üìä Loaded {len(self.samples)} test samples from {self.root_dir}\")\n",
    "\n",
    "        # Print class distribution\n",
    "        class_counts = {}\n",
    "        for _, label in self.samples:\n",
    "            class_counts[label] = class_counts.get(label, 0) + 1\n",
    "        for idx, name in enumerate(CLASS_NAMES):\n",
    "            print(f\"   {name}: {class_counts.get(idx, 0)} samples\")\n",
    "\n",
    "    def _default_transform(self) -> A.Compose:\n",
    "        \"\"\"Default test transform: resize and convert to tensor.\"\"\"\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),  # Keep in [0, 1]\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, dict]:\n",
    "        img_path, label = self.samples[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed[\"image\"]\n",
    "\n",
    "        # Metadata\n",
    "        metadata = {\n",
    "            \"image_path\": str(img_path),\n",
    "            \"class_name\": CLASS_NAMES[label]\n",
    "        }\n",
    "\n",
    "        return image.float(), label, metadata\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def create_model(num_classes: int = NUM_CLASSES, pretrained: bool = False) -> nn.Module:\n",
    "    \"\"\"Create ResNet-50 model for ISIC classification.\"\"\"\n",
    "    model = timm.create_model(\n",
    "        \"resnet50\",\n",
    "        pretrained=pretrained,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def load_checkpoint(model: nn.Module, checkpoint_path: Path, device: torch.device) -> dict:\n",
    "    \"\"\"\n",
    "    Load model checkpoint and return metadata.\n",
    "    Handles torch.compile() and custom model wrapper prefixes.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        device: Target device\n",
    "\n",
    "    Returns:\n",
    "        Checkpoint metadata dictionary\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "\n",
    "    # Get state dict from checkpoint\n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        state_dict = checkpoint[\"model_state_dict\"]\n",
    "        metadata = {\n",
    "            \"epoch\": checkpoint.get(\"epoch\", \"unknown\"),\n",
    "            \"val_acc\": checkpoint.get(\"val_acc\", checkpoint.get(\"best_val_acc\", \"unknown\")),\n",
    "            \"seed\": checkpoint.get(\"seed\", \"unknown\")\n",
    "        }\n",
    "    elif \"state_dict\" in checkpoint:\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "        metadata = {\"epoch\": \"unknown\", \"val_acc\": \"unknown\", \"seed\": \"unknown\"}\n",
    "    else:\n",
    "        # Direct state dict\n",
    "        state_dict = checkpoint\n",
    "        metadata = {\"epoch\": \"unknown\", \"val_acc\": \"unknown\", \"seed\": \"unknown\"}\n",
    "\n",
    "    # Handle prefix transformations\n",
    "    # Check for _orig_mod. prefix (from torch.compile)\n",
    "    sample_key = list(state_dict.keys())[0]\n",
    "    if sample_key.startswith('_orig_mod.'):\n",
    "        print(\"   üîß Removing '_orig_mod.' prefix (torch.compile)\")\n",
    "        state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}\n",
    "        sample_key = list(state_dict.keys())[0]\n",
    "\n",
    "    # Check for 'backbone.' prefix (custom model wrapper)\n",
    "    if sample_key.startswith('backbone.'):\n",
    "        print(\"   üîß Removing 'backbone.' prefix (custom wrapper)\")\n",
    "        state_dict = {k.replace('backbone.', ''): v for k, v in state_dict.items()}\n",
    "        sample_key = list(state_dict.keys())[0]\n",
    "\n",
    "    # Check for 'module.' prefix (DataParallel)\n",
    "    if sample_key.startswith('module.'):\n",
    "        print(\"   üîß Removing 'module.' prefix (DataParallel)\")\n",
    "        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "    # Handle classifier head mismatch (fc vs classifier naming)\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    ckpt_keys = set(state_dict.keys())\n",
    "\n",
    "    # Check if checkpoint uses 'classifier' but model uses 'fc'\n",
    "    if 'classifier.weight' in ckpt_keys and 'fc.weight' in model_keys:\n",
    "        print(\"   üîß Renaming 'classifier' -> 'fc' for classifier head\")\n",
    "        state_dict = {k.replace('classifier.', 'fc.'): v for k, v in state_dict.items()}\n",
    "    elif 'fc.weight' in ckpt_keys and 'classifier.weight' in model_keys:\n",
    "        print(\"   üîß Renaming 'fc' -> 'classifier' for classifier head\")\n",
    "        state_dict = {k.replace('fc.', 'classifier.'): v for k, v in state_dict.items()}\n",
    "\n",
    "    # Load state dict\n",
    "    try:\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"   ‚ö†Ô∏è Strict loading failed, trying non-strict...\")\n",
    "        missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "        if missing:\n",
    "            print(f\"   ‚ö†Ô∏è Missing keys: {missing[:5]}{'...' if len(missing) > 5 else ''}\")\n",
    "        if unexpected:\n",
    "            print(f\"   ‚ö†Ô∏è Unexpected keys: {unexpected[:5]}{'...' if len(unexpected) > 5 else ''}\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def get_normalizer(device: torch.device):\n",
    "    \"\"\"Get ImageNet normalization function.\"\"\"\n",
    "    mean = torch.tensor(IMAGENET_MEAN).view(1, 3, 1, 1).to(device)\n",
    "    std = torch.tensor(IMAGENET_STD).view(1, 3, 1, 1).to(device)\n",
    "\n",
    "    def normalize(x: torch.Tensor) -> torch.Tensor:\n",
    "        return (x - mean) / std\n",
    "\n",
    "    return normalize\n",
    "\n",
    "print(\"‚úÖ Dataset and model utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ab186",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "367ab186",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764463962887,
     "user_tz": 0,
     "elapsed": 45,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "369942f1-40dc-44bc-80d2-64d003efb488"
   },
   "outputs": [],
   "source": [
    "#@title ‚öôÔ∏è Cell 6: Evaluation Configuration\n",
    "#@markdown **Configure attack parameters and evaluation settings**\n",
    "\n",
    "@dataclass\n",
    "class EvaluationConfig:\n",
    "    \"\"\"Configuration for adversarial evaluation.\"\"\"\n",
    "\n",
    "    # Seeds to evaluate\n",
    "    seeds: List[int] = field(default_factory=lambda: [42, 123, 456])\n",
    "\n",
    "    # Epsilon values (perturbation budgets)\n",
    "    epsilons: List[float] = field(default_factory=lambda: [2/255, 4/255, 8/255])\n",
    "\n",
    "    # Attack configurations\n",
    "    fgsm_enabled: bool = True\n",
    "    pgd_enabled: bool = True\n",
    "    pgd_steps: int = 40\n",
    "    pgd_step_size: Optional[float] = None  # Auto: epsilon/4\n",
    "\n",
    "    cw_enabled: bool = True\n",
    "    cw_iterations: int = 100  # Reduced for speed (default 1000)\n",
    "    cw_confidence: float = 0.0\n",
    "    cw_learning_rate: float = 0.01\n",
    "\n",
    "    # Evaluation settings\n",
    "    batch_size: int = 64  # Increase for A100\n",
    "    num_workers: int = 4\n",
    "    max_test_samples: Optional[int] = None  # None = all samples\n",
    "\n",
    "    # Output settings\n",
    "    save_adversarial_examples: bool = True\n",
    "    num_examples_to_save: int = 50\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Adjust settings based on hardware.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "\n",
    "            # Optimize batch size for GPU memory\n",
    "            if gpu_mem >= 35:  # A100 40GB\n",
    "                self.batch_size = 128\n",
    "                self.num_workers = 8\n",
    "                self.cw_iterations = 200\n",
    "                print(f\"‚ö° A100 optimizations: batch={self.batch_size}, C&W iters={self.cw_iterations}\")\n",
    "            elif gpu_mem >= 14:  # T4/V100\n",
    "                self.batch_size = 64\n",
    "                self.num_workers = 4\n",
    "                self.cw_iterations = 100\n",
    "                print(f\"‚ö° T4/V100 settings: batch={self.batch_size}\")\n",
    "            else:  # Smaller GPU\n",
    "                self.batch_size = 32\n",
    "                self.num_workers = 2\n",
    "                self.cw_iterations = 50\n",
    "                print(f\"‚ö†Ô∏è Limited GPU: batch={self.batch_size}\")\n",
    "\n",
    "    def get_epsilon_str(self, eps: float) -> str:\n",
    "        \"\"\"Convert epsilon to readable string.\"\"\"\n",
    "        return f\"{int(eps * 255)}/255\"\n",
    "\n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Get configuration summary.\"\"\"\n",
    "        lines = [\n",
    "            \"=\"*60,\n",
    "            \"üìã EVALUATION CONFIGURATION\",\n",
    "            \"=\"*60,\n",
    "            f\"Seeds: {self.seeds}\",\n",
    "            f\"Epsilons: {[self.get_epsilon_str(e) for e in self.epsilons]}\",\n",
    "            \"\",\n",
    "            \"Attacks:\",\n",
    "            f\"  FGSM: {'‚úì' if self.fgsm_enabled else '‚úó'}\",\n",
    "            f\"  PGD:  {'‚úì' if self.pgd_enabled else '‚úó'} (steps={self.pgd_steps})\",\n",
    "            f\"  C&W:  {'‚úì' if self.cw_enabled else '‚úó'} (iters={self.cw_iterations})\",\n",
    "            \"\",\n",
    "            f\"Batch size: {self.batch_size}\",\n",
    "            f\"Max samples: {self.max_test_samples or 'all'}\",\n",
    "            \"=\"*60\n",
    "        ]\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# Initialize configuration\n",
    "config = EvaluationConfig()\n",
    "print(config.summary())\n",
    "\n",
    "# Epsilon display helper\n",
    "EPSILON_LABELS = {\n",
    "    2/255: \"Œµ=2/255 (weak)\",\n",
    "    4/255: \"Œµ=4/255 (medium)\",\n",
    "    8/255: \"Œµ=8/255 (strong)\"\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Configuration ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7777fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd7777fe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764463967113,
     "user_tz": 0,
     "elapsed": 17,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "a1f41f29-e866-4cb8-b3dd-cd5f13570afc"
   },
   "outputs": [],
   "source": [
    "#@title üéØ Cell 7: Adversarial Attack Engine\n",
    "#@markdown **Core attack generation and evaluation functions**\n",
    "\n",
    "class AdversarialEvaluator:\n",
    "    \"\"\"\n",
    "    Unified adversarial evaluation engine.\n",
    "\n",
    "    Supports FGSM, PGD, and Carlini-Wagner attacks with batch processing\n",
    "    and detailed metrics collection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        device: torch.device,\n",
    "        normalize_fn: callable\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.normalize = normalize_fn\n",
    "        self.model.eval()\n",
    "\n",
    "        # Results storage\n",
    "        self.results = {}\n",
    "\n",
    "    def evaluate_clean(\n",
    "        self,\n",
    "        dataloader: DataLoader,\n",
    "        desc: str = \"Clean Evaluation\"\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate model on clean (unperturbed) data.\"\"\"\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=desc, leave=False):\n",
    "                # Handle 3-tuple (image, label, metadata)\n",
    "                images, labels = batch[0], batch[1]\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                # Forward pass with normalization\n",
    "                logits = self.model(self.normalize(images))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                preds = logits.argmax(dim=1)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_probs = np.array(all_probs)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "        f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        # Per-class accuracy\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "        # AUROC (one-vs-rest)\n",
    "        try:\n",
    "            auroc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
    "        except ValueError:\n",
    "            auroc = 0.0\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"balanced_accuracy\": balanced_acc,\n",
    "            \"f1_macro\": f1_macro,\n",
    "            \"f1_weighted\": f1_weighted,\n",
    "            \"auroc\": auroc,\n",
    "            \"per_class_accuracy\": dict(zip(CLASS_NAMES, per_class_acc)),\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"predictions\": all_preds,\n",
    "            \"labels\": all_labels,\n",
    "            \"probabilities\": all_probs\n",
    "        }\n",
    "\n",
    "    def evaluate_attack(\n",
    "        self,\n",
    "        dataloader: DataLoader,\n",
    "        attack_name: str,\n",
    "        epsilon: float,\n",
    "        attack_fn: callable,\n",
    "        desc: str = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate model under adversarial attack.\n",
    "\n",
    "        Args:\n",
    "            dataloader: Test data loader\n",
    "            attack_name: Name of attack (FGSM, PGD, CW)\n",
    "            epsilon: Perturbation budget\n",
    "            attack_fn: Function that generates adversarial examples\n",
    "            desc: Progress bar description\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with attack results and metrics\n",
    "        \"\"\"\n",
    "        if desc is None:\n",
    "            desc = f\"{attack_name} Œµ={config.get_epsilon_str(epsilon)}\"\n",
    "\n",
    "        all_preds_clean = []\n",
    "        all_preds_adv = []\n",
    "        all_labels = []\n",
    "        all_l2_dists = []\n",
    "        all_linf_dists = []\n",
    "        successful_attacks = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Store some examples for visualization\n",
    "        saved_examples = []\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=desc, leave=False):\n",
    "            images, labels = batch[0], batch[1]\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            # Clean predictions\n",
    "            with torch.no_grad():\n",
    "                clean_logits = self.model(self.normalize(images))\n",
    "                clean_preds = clean_logits.argmax(dim=1)\n",
    "\n",
    "            # Generate adversarial examples\n",
    "            try:\n",
    "                x_adv = attack_fn(images, labels)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Attack failed on batch: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Adversarial predictions\n",
    "            with torch.no_grad():\n",
    "                adv_logits = self.model(self.normalize(x_adv))\n",
    "                adv_preds = adv_logits.argmax(dim=1)\n",
    "\n",
    "            # Calculate perturbation norms\n",
    "            delta = (x_adv - images).view(images.size(0), -1)\n",
    "            l2_dist = torch.norm(delta, p=2, dim=1)\n",
    "            linf_dist = torch.norm(delta, p=float('inf'), dim=1)\n",
    "\n",
    "            # Track successful attacks (correctly classified ‚Üí misclassified)\n",
    "            was_correct = (clean_preds == labels)\n",
    "            is_wrong = (adv_preds != labels)\n",
    "            successful = was_correct & is_wrong\n",
    "\n",
    "            successful_attacks += successful.sum().item()\n",
    "            total_samples += was_correct.sum().item()  # Only count correctly classified\n",
    "\n",
    "            # Store results\n",
    "            all_preds_clean.extend(clean_preds.cpu().numpy())\n",
    "            all_preds_adv.extend(adv_preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_l2_dists.extend(l2_dist.cpu().numpy())\n",
    "            all_linf_dists.extend(linf_dist.cpu().numpy())\n",
    "\n",
    "            # Save examples for visualization\n",
    "            if len(saved_examples) < config.num_examples_to_save:\n",
    "                for i in range(min(5, images.size(0))):\n",
    "                    if len(saved_examples) >= config.num_examples_to_save:\n",
    "                        break\n",
    "                    saved_examples.append({\n",
    "                        \"clean\": images[i].cpu(),\n",
    "                        \"adversarial\": x_adv[i].cpu(),\n",
    "                        \"perturbation\": (x_adv[i] - images[i]).cpu(),\n",
    "                        \"true_label\": labels[i].item(),\n",
    "                        \"clean_pred\": clean_preds[i].item(),\n",
    "                        \"adv_pred\": adv_preds[i].item()\n",
    "                    })\n",
    "\n",
    "        # Convert to arrays\n",
    "        all_preds_clean = np.array(all_preds_clean)\n",
    "        all_preds_adv = np.array(all_preds_adv)\n",
    "        all_labels = np.array(all_labels)\n",
    "\n",
    "        # Calculate metrics\n",
    "        clean_acc = accuracy_score(all_labels, all_preds_clean)\n",
    "        robust_acc = accuracy_score(all_labels, all_preds_adv)\n",
    "        attack_success_rate = successful_attacks / max(total_samples, 1)\n",
    "\n",
    "        # Per-class robust accuracy\n",
    "        cm_adv = confusion_matrix(all_labels, all_preds_adv)\n",
    "        per_class_robust_acc = cm_adv.diagonal() / cm_adv.sum(axis=1)\n",
    "\n",
    "        return {\n",
    "            \"attack_name\": attack_name,\n",
    "            \"epsilon\": epsilon,\n",
    "            \"epsilon_str\": config.get_epsilon_str(epsilon),\n",
    "            \"clean_accuracy\": clean_acc,\n",
    "            \"robust_accuracy\": robust_acc,\n",
    "            \"accuracy_drop\": clean_acc - robust_acc,\n",
    "            \"attack_success_rate\": attack_success_rate,\n",
    "            \"mean_l2_dist\": np.mean(all_l2_dists),\n",
    "            \"mean_linf_dist\": np.mean(all_linf_dists),\n",
    "            \"per_class_robust_accuracy\": dict(zip(CLASS_NAMES, per_class_robust_acc)),\n",
    "            \"confusion_matrix\": cm_adv,\n",
    "            \"saved_examples\": saved_examples,\n",
    "            \"predictions_clean\": all_preds_clean,\n",
    "            \"predictions_adv\": all_preds_adv,\n",
    "            \"labels\": all_labels\n",
    "        }\n",
    "\n",
    "    def create_fgsm_attack(self, epsilon: float) -> callable:\n",
    "        \"\"\"Create FGSM attack function.\"\"\"\n",
    "        fgsm_config = FGSMConfig(\n",
    "            epsilon=epsilon,\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False,\n",
    "            device=str(self.device)\n",
    "        )\n",
    "        attack = FGSM(fgsm_config)\n",
    "\n",
    "        def attack_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "            return attack.generate(\n",
    "                self.model, x, y,\n",
    "                loss_fn=nn.CrossEntropyLoss(),\n",
    "                normalize=self.normalize\n",
    "            )\n",
    "        return attack_fn\n",
    "\n",
    "    def create_pgd_attack(self, epsilon: float, num_steps: int = 40) -> callable:\n",
    "        \"\"\"Create PGD attack function.\"\"\"\n",
    "        step_size = epsilon / 4  # Standard choice\n",
    "\n",
    "        pgd_config = PGDConfig(\n",
    "            epsilon=epsilon,\n",
    "            num_steps=num_steps,\n",
    "            step_size=step_size,\n",
    "            random_start=True,\n",
    "            early_stop=False,\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False,\n",
    "            device=str(self.device)\n",
    "        )\n",
    "        attack = PGD(pgd_config)\n",
    "\n",
    "        def attack_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "            return attack.generate(\n",
    "                self.model, x, y,\n",
    "                loss_fn=nn.CrossEntropyLoss(),\n",
    "                normalize=self.normalize\n",
    "            )\n",
    "        return attack_fn\n",
    "\n",
    "    def create_cw_attack(\n",
    "        self,\n",
    "        confidence: float = 0.0,\n",
    "        max_iterations: int = 100,\n",
    "        learning_rate: float = 0.01\n",
    "    ) -> callable:\n",
    "        \"\"\"Create Carlini-Wagner L2 attack function.\"\"\"\n",
    "        cw_config = CWConfig(\n",
    "            confidence=confidence,\n",
    "            learning_rate=learning_rate,\n",
    "            max_iterations=max_iterations,\n",
    "            binary_search_steps=5,  # Reduced for speed\n",
    "            initial_c=1e-3,\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False,\n",
    "            device=str(self.device)\n",
    "        )\n",
    "        attack = CarliniWagner(cw_config)\n",
    "\n",
    "        def attack_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "            return attack.generate(\n",
    "                self.model, x, y,\n",
    "                normalize=self.normalize\n",
    "            )\n",
    "        return attack_fn\n",
    "\n",
    "print(\"‚úÖ AdversarialEvaluator class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32201a3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "19f9cb8c3d1040ff9f2c41a4f4414fcd",
      "333e4781331f42b2a0576b3810a586d3",
      "ccdceb99f64f4f19ac32dda3ef62ab55",
      "374b51c81427443ab76d94a6fc884665",
      "eadb0f62edee4a60959af1048dd2e223",
      "c30fc8faa4d64bf09bc93db24bde2ff9",
      "6649256e45f4465fb3f1c8d10ca57b80",
      "55598a065061401ea274b241b7932f60",
      "ecea458d42a840ffad15638380934ba9",
      "cfdc4a12a8ac45a28fcc447efcd80185",
      "ba5a1c6a5c2344fa994e4fbb62610bce",
      "daa7df57b47649cf933546142c1bcda0",
      "170074aea6d24957aeec537137c10979",
      "6be3d3310d8a4a3cab4924b1ca1dd381",
      "a3bcffc707e046fbb475699494427457",
      "edd216b75e4c4524aafe38db61ed959f",
      "64e22564d5e74431a8069e8a31fcb949",
      "01730d4f416748a799d1d34b7d9a0650",
      "42ef5e07808e40a1ae32aaa656b9a5c7",
      "0745e3ae737c400dbd52fa5ef54b94b2",
      "fe3e1ea484b54d0891c332483c4e655f",
      "8fe54cebd2564477877b6edfe9371832",
      "cf63925fe57943b68d62ca2a3aa76189",
      "ecc5c9f406d9480a83b28945e96a0684",
      "0a69916ea74742968e3ea918e0e68e17",
      "42cb96844c0e4666af44563bf6025e3b",
      "4a0ccef9090940ec9baf46eb6f95ac42",
      "9a27c6ab436a486296bb8225bba5a5aa",
      "0fff45b7098e43318ae8b78e4d806738",
      "cfda9f9a2ca94cdf9cc7ed20779a4396",
      "22745e93201146c8bdda6cac69147c9d",
      "e4a2405e4d4e41fea325c2071dd22b19",
      "efd102a0644043ab8470bb5c5c153010"
     ]
    },
    "id": "32201a3d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764471021214,
     "user_tz": 0,
     "elapsed": 44404,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "67ccc9c7-9b15-4e06-f006-4977eaaf6b35"
   },
   "outputs": [],
   "source": [
    "#@title üì¶ Cell 8: Load Test Dataset and Models\n",
    "#@markdown **Load test data and all seed checkpoints**\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä LOADING TEST DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = ISICTestDataset(\n",
    "    root_dir=paths.test_dir,\n",
    "    max_samples=config.max_test_samples\n",
    ")\n",
    "\n",
    "# Create dataloader - Use num_workers=0 in Colab to avoid multiprocessing warnings\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Set to 0 for Colab compatibility\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Test samples: {len(test_dataset)}\")\n",
    "print(f\"‚úÖ Batches: {len(test_loader)}\")\n",
    "print(f\"‚úÖ Batch size: {config.batch_size}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODELS FOR ALL SEEDS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîß LOADING MODEL CHECKPOINTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = {}\n",
    "normalize = get_normalizer(device)\n",
    "\n",
    "for seed in config.seeds:\n",
    "    # Try multiple checkpoint path patterns\n",
    "    checkpoint_patterns = [\n",
    "        paths.checkpoint_dir / f\"seed_{seed}\" / \"best.pt\",      # Most common: seed_42/best.pt\n",
    "        paths.checkpoint_dir / f\"seed_{seed}\" / \"last.pt\",      # Alternative: seed_42/last.pt\n",
    "        paths.checkpoint_dir / f\"baseline_seed_{seed}.pt\",      # Flat: baseline_seed_42.pt\n",
    "        paths.checkpoint_dir / f\"seed_{seed}_best.pt\",          # Flat alternative: seed_42_best.pt\n",
    "        paths.checkpoint_dir / f\"seed{seed}\" / \"best.pt\",       # No underscore: seed42/best.pt\n",
    "    ]\n",
    "\n",
    "    checkpoint_path = None\n",
    "    for pattern in checkpoint_patterns:\n",
    "        if pattern.exists():\n",
    "            checkpoint_path = pattern\n",
    "            break\n",
    "\n",
    "    if checkpoint_path is None:\n",
    "        print(f\"‚ö†Ô∏è Checkpoint not found for seed {seed}\")\n",
    "        print(f\"   Searched patterns:\")\n",
    "        for p in checkpoint_patterns[:3]:\n",
    "            print(f\"      - {p}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüì• Loading seed {seed}...\")\n",
    "    model = create_model(num_classes=NUM_CLASSES, pretrained=False)\n",
    "    metadata = load_checkpoint(model, checkpoint_path, device)\n",
    "\n",
    "    models[seed] = model\n",
    "\n",
    "    print(f\"   ‚úÖ Loaded: {checkpoint_path}\")\n",
    "    print(f\"   üìà Validation accuracy: {metadata.get('val_acc', 'N/A')}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(models)} models\")\n",
    "\n",
    "# ============================================================================\n",
    "# SANITY CHECK: VERIFY CLEAN ACCURACY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç SANITY CHECK: CLEAN ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "clean_results = {}\n",
    "\n",
    "for seed, model in models.items():\n",
    "    print(f\"\\nüß™ Evaluating seed {seed}...\")\n",
    "    evaluator = AdversarialEvaluator(model, device, normalize)\n",
    "    result = evaluator.evaluate_clean(test_loader, desc=f\"Clean eval (seed {seed})\")\n",
    "    clean_results[seed] = result\n",
    "\n",
    "    print(f\"   ‚úÖ Accuracy: {result['accuracy']*100:.2f}%\")\n",
    "    print(f\"   ‚úÖ Balanced Accuracy: {result['balanced_accuracy']*100:.2f}%\")\n",
    "    print(f\"   ‚úÖ F1 (macro): {result['f1_macro']*100:.2f}%\")\n",
    "    print(f\"   ‚úÖ AUROC: {result['auroc']*100:.2f}%\")\n",
    "\n",
    "# Summary statistics\n",
    "mean_acc = np.mean([r['accuracy'] for r in clean_results.values()])\n",
    "std_acc = np.std([r['accuracy'] for r in clean_results.values()])\n",
    "\n",
    "print(f\"\\nüìä Mean Clean Accuracy: {mean_acc*100:.2f}% ¬± {std_acc*100:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259012ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 891,
     "referenced_widgets": [
      "9cf5683e89a54fe4822026182c400fd6",
      "b837403f41ae480584167a6c6027bd02",
      "a81c23a2bf7f485597f50805f82b516f",
      "e2827e5fd9874766a7fe84636c1c5bbd",
      "c2709616eea54415a541ebd78e08747a",
      "89e6c8c5828e478caadf0e897f30e061",
      "f7352573e48d4f468b98865cf6d4c37f",
      "7f170bfd24da4a08add1d6e3be910502",
      "07c4a292b3ae47ad9ed0df26ae18cac4",
      "6d6a0f9909904c5ba71a05493e0ab757",
      "905bca90014b456ca1e492436eee9fb7",
      "8344dbae2f0c4dd1bfc58e0b0cde7de2",
      "88c163d3ceb74244b5a91a43f2b3122a",
      "c480d00ae7cf47a18427e3504c22e444",
      "d0f221167ff04aa99780c72560412800",
      "c12492c6352244ac83e1071439c75905",
      "e9d1500fd45a40989d3ecd9e370f3436",
      "dd614db45fc2489788c06c6a06aeade9",
      "d9aa832c4fd242aca0aea89d4f62ebee",
      "ecea4229d9d14ce4be18321bae8f7c2e",
      "92827b0d54aa452b808012cf0f59f5d7",
      "6abe83dc7dcf489daf802310b0faee1f",
      "bab43172b71041c284f72f89f38e0f00",
      "3a57335663c648c1ab44ae890bef0d8d",
      "e7cab203287d4ce0a6827c1363f43e81",
      "d19ce8efed044f6995a0628de7fc3345",
      "b8418baf6fd143a4803274c4e776e3c5",
      "853dd3a643494f32b329573fc39f130b",
      "0b3147aaf92f49fb9649c6b0df836a21",
      "e37263d234454f72aabc5d452478665e",
      "5ce5393724a043bc8dcdd45be8e5dfcd",
      "371e4ea7d8c447b6a495d0c2e97bf279",
      "356f23e43d5c43908606112acd6a2760",
      "add2d3f1ea2e472b852c86ce905c6238",
      "e71f356dae454a118f1c31f172b5d0ec",
      "69b35f99f6954d899128de02ce7d2084",
      "9a14b3954ddd43caa1e5980e90c955b4",
      "d8583f4f7e3340f0a2ed5ea3751f515f",
      "3e1a47edb75a46dd94a669d329544804",
      "28d03b9e98ee4e8faa74dbe8710d2273",
      "13621434791a4790937dd4c937a73b32",
      "8e6e8c8937a740c6b8f3617345e8647d",
      "1fa7f8d0da6d40e4828dc1360ec49c4c",
      "9333b09d03f64900b3ad75f5bc4d123c",
      "3a65af025ed64fc6bd6b5247d984bc6e",
      "b30fab5d09304a579bc3ca05f0afa92f",
      "f37f9362350e45fc98967df1386aa1e9",
      "9753bbe401214232a6ab3a541ecdca06",
      "56ffeb17c77945db9349b1b1fac105d0",
      "6497bc78a4e44047b8d202e2618509fc",
      "f6bda9ccbfd248f392fd96e345a08145",
      "5834a4c713fd44488b2df262011f3c88",
      "7aaf40435f0245ed845b7244663230d5",
      "197d7f6080914edbaa2656e74667ecbb",
      "7e9ffb99d5fc4aa18d231364564cfdb2",
      "a757c67190054c9cb535a95f64e04b3c",
      "2f0570f0c939402aad6ba12f088e41e4",
      "2d8dc46652374be3b34af94762fb5ca2",
      "9f5982359c974fa6a618e751d98bbf80",
      "70b581fd21cc445f83ba42bf1a5b9d54",
      "4d48e677065a41c7b243c027d14a5243",
      "b4a039bfbb89411590bc902800c2fdfe",
      "73fe64553ed44e2ba893d640d228d2e5",
      "84c62e13dd3c4b37b06df912c9893f76",
      "527932c2d29849e59c7bcf90b0ee3cc0",
      "4470d87a457b4d2eb15f236d864b5315",
      "21f292fb250741b686afa6000e739a69",
      "760e799872e44c4d8a2f18f796fd321e",
      "aa9c9d3f0ecf454dbb20bc002b8209ad",
      "0ceaffbfd56440188543098e180d5245",
      "46872c7021f942f1a3cd902ac760ebc0",
      "0710eaada6cc4f458a60ffc026e15cbc",
      "c563560fe83d447690da5e51a345d702",
      "638e047c7f0648db843bb5f12cdfc5f2",
      "1d1a57f22a1b4dbfb68703f831719282",
      "847fdaaec126493db6ca6f96a7dd181e",
      "df4df33d0fb34146ae82a73173fa027b"
     ]
    },
    "id": "259012ba",
    "executionInfo": {
     "status": "error",
     "timestamp": 1764473575774,
     "user_tz": 0,
     "elapsed": 1070715,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "453d9ee0-cb84-4efb-c8e3-01c86595644b"
   },
   "outputs": [],
   "source": [
    "#@title üöÄ Cell 9: Run Full Adversarial Evaluation\n",
    "#@markdown **Execute FGSM, PGD, and C&W attacks across all seeds and epsilons**\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ STARTING FULL ADVERSARIAL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚è±Ô∏è  Start time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"üéØ Seeds: {config.seeds}\")\n",
    "print(f\"üéØ Epsilons: {[config.get_epsilon_str(e) for e in config.epsilons]}\")\n",
    "print(f\"üéØ Attacks: FGSM={'‚úì' if config.fgsm_enabled else '‚úó'}, \"\n",
    "      f\"PGD={'‚úì' if config.pgd_enabled else '‚úó'}, \"\n",
    "      f\"C&W={'‚úì' if config.cw_enabled else '‚úó'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Results storage\n",
    "all_results = {\n",
    "    \"clean\": clean_results,\n",
    "    \"fgsm\": {},\n",
    "    \"pgd\": {},\n",
    "    \"cw\": {}\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for seed in config.seeds:\n",
    "    if seed not in models:\n",
    "        continue\n",
    "\n",
    "    model = models[seed]\n",
    "    evaluator = AdversarialEvaluator(model, device, normalize)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üî¨ EVALUATING SEED {seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Initialize seed results\n",
    "    all_results[\"fgsm\"][seed] = {}\n",
    "    all_results[\"pgd\"][seed] = {}\n",
    "    all_results[\"cw\"][seed] = {}\n",
    "\n",
    "    # ========================================================================\n",
    "    # FGSM ATTACKS\n",
    "    # ========================================================================\n",
    "    if config.fgsm_enabled:\n",
    "        print(f\"\\n‚ö° FGSM Attacks (Seed {seed})\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        for eps in config.epsilons:\n",
    "            attack_fn = evaluator.create_fgsm_attack(eps)\n",
    "            result = evaluator.evaluate_attack(\n",
    "                test_loader,\n",
    "                attack_name=\"FGSM\",\n",
    "                epsilon=eps,\n",
    "                attack_fn=attack_fn\n",
    "            )\n",
    "            all_results[\"fgsm\"][seed][eps] = result\n",
    "\n",
    "            print(f\"   Œµ={config.get_epsilon_str(eps):>7}: \"\n",
    "                  f\"Robust Acc = {result['robust_accuracy']*100:5.2f}% \"\n",
    "                  f\"(‚Üì{result['accuracy_drop']*100:5.2f}pp)\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # PGD ATTACKS\n",
    "    # ========================================================================\n",
    "    if config.pgd_enabled:\n",
    "        print(f\"\\nüîÑ PGD-{config.pgd_steps} Attacks (Seed {seed})\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        for eps in config.epsilons:\n",
    "            attack_fn = evaluator.create_pgd_attack(eps, num_steps=config.pgd_steps)\n",
    "            result = evaluator.evaluate_attack(\n",
    "                test_loader,\n",
    "                attack_name=f\"PGD-{config.pgd_steps}\",\n",
    "                epsilon=eps,\n",
    "                attack_fn=attack_fn\n",
    "            )\n",
    "            all_results[\"pgd\"][seed][eps] = result\n",
    "\n",
    "            print(f\"   Œµ={config.get_epsilon_str(eps):>7}: \"\n",
    "                  f\"Robust Acc = {result['robust_accuracy']*100:5.2f}% \"\n",
    "                  f\"(‚Üì{result['accuracy_drop']*100:5.2f}pp)\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # C&W ATTACKS (Only at strongest epsilon for speed)\n",
    "    # ========================================================================\n",
    "    if config.cw_enabled:\n",
    "        print(f\"\\nüéØ C&W L2 Attack (Seed {seed})\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # C&W is epsilon-free (L2 minimization), run once\n",
    "        attack_fn = evaluator.create_cw_attack(\n",
    "            confidence=config.cw_confidence,\n",
    "            max_iterations=config.cw_iterations,\n",
    "            learning_rate=config.cw_learning_rate\n",
    "        )\n",
    "        result = evaluator.evaluate_attack(\n",
    "            test_loader,\n",
    "            attack_name=\"C&W-L2\",\n",
    "            epsilon=0.0,  # C&W minimizes L2 directly\n",
    "            attack_fn=attack_fn,\n",
    "            desc=\"C&W L2 Attack\"\n",
    "        )\n",
    "        all_results[\"cw\"][seed][\"l2\"] = result\n",
    "\n",
    "        print(f\"   Robust Acc = {result['robust_accuracy']*100:5.2f}% \"\n",
    "              f\"(‚Üì{result['accuracy_drop']*100:5.2f}pp)\")\n",
    "        print(f\"   Mean L2 perturbation: {result['mean_l2_dist']:.4f}\")\n",
    "\n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Timing\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ EVALUATION COMPLETE\")\n",
    "print(f\"‚è±Ô∏è  Total time: {elapsed/60:.1f} minutes\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e3c7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b5e3c7f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764469987593,
     "user_tz": 0,
     "elapsed": 179,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "680f2a33-6b2b-45cc-e4bc-f314f3fd3c4e"
   },
   "outputs": [],
   "source": [
    "#@title üìä Cell 10: Results Summary Table\n",
    "#@markdown **Generate comprehensive results summary with statistics**\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ADVERSARIAL ROBUSTNESS RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# DETECT AND HANDLE MULTIPLE DATA STRUCTURES\n",
    "# ============================================================================\n",
    "\n",
    "results_data = []\n",
    "\n",
    "def get_all_results_structure():\n",
    "    \"\"\"\n",
    "    Detect the data structure of all_results.\n",
    "    Returns: 'seed_first', 'attack_first_populated', 'attack_first_empty', or None\n",
    "    \"\"\"\n",
    "    if not all_results:\n",
    "        return None\n",
    "\n",
    "    keys = list(all_results.keys())\n",
    "    first_key = keys[0]\n",
    "\n",
    "    # Check if keys are integers (seed-first structure)\n",
    "    if isinstance(first_key, int):\n",
    "        return \"seed_first\"\n",
    "\n",
    "    # Check if keys are attack type strings\n",
    "    if isinstance(first_key, str) and first_key in ['clean', 'fgsm', 'pgd', 'cw']:\n",
    "        # Check if any attack dict has data\n",
    "        for attack_type in ['fgsm', 'pgd', 'cw']:\n",
    "            if attack_type in all_results and len(all_results[attack_type]) > 0:\n",
    "                return \"attack_first_populated\"\n",
    "        return \"attack_first_empty\"\n",
    "\n",
    "    return None\n",
    "\n",
    "structure = get_all_results_structure()\n",
    "print(f\"üìä Data structure detected: {structure}\")\n",
    "\n",
    "if structure == \"seed_first\":\n",
    "    # ========================================================================\n",
    "    # STRUCTURE: all_results[seed]['fgsm_2/255'] (Original Cell 9 format)\n",
    "    # ========================================================================\n",
    "    print(\"   Using seed-first format (original Cell 9)\")\n",
    "\n",
    "    # Get config\n",
    "    try:\n",
    "        epsilons = CONFIG.get('epsilons', [2/255, 4/255, 8/255])\n",
    "        pgd_steps = CONFIG.get('pgd_steps', 40)\n",
    "    except:\n",
    "        epsilons = [2/255, 4/255, 8/255]\n",
    "        pgd_steps = 40\n",
    "\n",
    "    seeds = [k for k in all_results.keys() if isinstance(k, int)]\n",
    "    print(f\"   Seeds found: {seeds}\")\n",
    "\n",
    "    for seed in seeds:\n",
    "        seed_data = all_results[seed]\n",
    "\n",
    "        # Clean accuracy\n",
    "        clean_info = seed_data.get('clean', {})\n",
    "        clean_acc = clean_info.get('accuracy', 0)\n",
    "\n",
    "        # FGSM results\n",
    "        for eps in epsilons:\n",
    "            eps_str = f\"{int(eps*255)}/255\"\n",
    "            key = f'fgsm_{eps_str}'\n",
    "            if key in seed_data:\n",
    "                r = seed_data[key]\n",
    "                results_data.append({\n",
    "                    \"Seed\": seed,\n",
    "                    \"Attack\": \"FGSM\",\n",
    "                    \"Epsilon\": eps_str,\n",
    "                    \"Epsilon_Val\": eps,\n",
    "                    \"Clean_Acc\": clean_acc,\n",
    "                    \"Robust_Acc\": r.get(\"robust_accuracy\", 0),\n",
    "                    \"Acc_Drop\": r.get(\"accuracy_drop\", 0),\n",
    "                    \"Attack_Success\": 100 - r.get(\"robust_accuracy\", 0),\n",
    "                })\n",
    "\n",
    "        # PGD results\n",
    "        for eps in epsilons:\n",
    "            eps_str = f\"{int(eps*255)}/255\"\n",
    "            key = f'pgd_{eps_str}'\n",
    "            if key in seed_data:\n",
    "                r = seed_data[key]\n",
    "                results_data.append({\n",
    "                    \"Seed\": seed,\n",
    "                    \"Attack\": f\"PGD-{pgd_steps}\",\n",
    "                    \"Epsilon\": eps_str,\n",
    "                    \"Epsilon_Val\": eps,\n",
    "                    \"Clean_Acc\": clean_acc,\n",
    "                    \"Robust_Acc\": r.get(\"robust_accuracy\", 0),\n",
    "                    \"Acc_Drop\": r.get(\"accuracy_drop\", 0),\n",
    "                    \"Attack_Success\": 100 - r.get(\"robust_accuracy\", 0),\n",
    "                })\n",
    "\n",
    "        # C&W results\n",
    "        if 'cw' in seed_data:\n",
    "            r = seed_data['cw']\n",
    "            results_data.append({\n",
    "                \"Seed\": seed,\n",
    "                \"Attack\": \"C&W-L2\",\n",
    "                \"Epsilon\": \"L2\",\n",
    "                \"Epsilon_Val\": 0,\n",
    "                \"Clean_Acc\": clean_acc,\n",
    "                \"Robust_Acc\": r.get(\"robust_accuracy\", 0),\n",
    "                \"Acc_Drop\": r.get(\"accuracy_drop\", 0),\n",
    "                \"Attack_Success\": 100 - r.get(\"robust_accuracy\", 0),\n",
    "            })\n",
    "\n",
    "elif structure == \"attack_first_populated\":\n",
    "    # ========================================================================\n",
    "    # STRUCTURE: all_results[\"fgsm\"][seed][eps] (@title Cell 9 format)\n",
    "    # ========================================================================\n",
    "    print(\"   Using attack-first format (@title Cell 9)\")\n",
    "\n",
    "    # Get seeds from the actual data\n",
    "    fgsm_seeds = list(all_results.get(\"fgsm\", {}).keys())\n",
    "    pgd_seeds = list(all_results.get(\"pgd\", {}).keys())\n",
    "    cw_seeds = list(all_results.get(\"cw\", {}).keys())\n",
    "    clean_seeds = list(all_results.get(\"clean\", {}).keys())\n",
    "\n",
    "    all_seeds = set(fgsm_seeds + pgd_seeds + cw_seeds + clean_seeds)\n",
    "    print(f\"   Seeds found: {sorted(all_seeds)}\")\n",
    "\n",
    "    for seed in all_seeds:\n",
    "        # Clean accuracy\n",
    "        clean_info = all_results.get(\"clean\", {}).get(seed, {})\n",
    "        clean_acc = clean_info.get(\"accuracy\", 0)\n",
    "        if clean_acc <= 1:\n",
    "            clean_acc *= 100\n",
    "\n",
    "        # FGSM results\n",
    "        fgsm_data = all_results.get(\"fgsm\", {}).get(seed, {})\n",
    "        for eps, r in fgsm_data.items():\n",
    "            eps_str = f\"{int(eps*255)}/255\" if isinstance(eps, float) else str(eps)\n",
    "            robust_acc = r.get(\"robust_accuracy\", 0)\n",
    "            if robust_acc <= 1:\n",
    "                robust_acc *= 100\n",
    "            acc_drop = r.get(\"accuracy_drop\", 0)\n",
    "            if acc_drop <= 1:\n",
    "                acc_drop *= 100\n",
    "\n",
    "            results_data.append({\n",
    "                \"Seed\": seed,\n",
    "                \"Attack\": \"FGSM\",\n",
    "                \"Epsilon\": eps_str,\n",
    "                \"Epsilon_Val\": eps if isinstance(eps, float) else 0,\n",
    "                \"Clean_Acc\": clean_acc,\n",
    "                \"Robust_Acc\": robust_acc,\n",
    "                \"Acc_Drop\": acc_drop,\n",
    "                \"Attack_Success\": 100 - robust_acc,\n",
    "            })\n",
    "\n",
    "        # PGD results\n",
    "        pgd_data = all_results.get(\"pgd\", {}).get(seed, {})\n",
    "        for eps, r in pgd_data.items():\n",
    "            eps_str = f\"{int(eps*255)}/255\" if isinstance(eps, float) else str(eps)\n",
    "            robust_acc = r.get(\"robust_accuracy\", 0)\n",
    "            if robust_acc <= 1:\n",
    "                robust_acc *= 100\n",
    "            acc_drop = r.get(\"accuracy_drop\", 0)\n",
    "            if acc_drop <= 1:\n",
    "                acc_drop *= 100\n",
    "\n",
    "            results_data.append({\n",
    "                \"Seed\": seed,\n",
    "                \"Attack\": \"PGD-40\",\n",
    "                \"Epsilon\": eps_str,\n",
    "                \"Epsilon_Val\": eps if isinstance(eps, float) else 0,\n",
    "                \"Clean_Acc\": clean_acc,\n",
    "                \"Robust_Acc\": robust_acc,\n",
    "                \"Acc_Drop\": acc_drop,\n",
    "                \"Attack_Success\": 100 - robust_acc,\n",
    "            })\n",
    "\n",
    "        # C&W results\n",
    "        cw_data = all_results.get(\"cw\", {}).get(seed, {})\n",
    "        if \"l2\" in cw_data:\n",
    "            r = cw_data[\"l2\"]\n",
    "            robust_acc = r.get(\"robust_accuracy\", 0)\n",
    "            if robust_acc <= 1:\n",
    "                robust_acc *= 100\n",
    "            acc_drop = r.get(\"accuracy_drop\", 0)\n",
    "            if acc_drop <= 1:\n",
    "                acc_drop *= 100\n",
    "\n",
    "            results_data.append({\n",
    "                \"Seed\": seed,\n",
    "                \"Attack\": \"C&W-L2\",\n",
    "                \"Epsilon\": \"L2\",\n",
    "                \"Epsilon_Val\": 0,\n",
    "                \"Clean_Acc\": clean_acc,\n",
    "                \"Robust_Acc\": robust_acc,\n",
    "                \"Acc_Drop\": acc_drop,\n",
    "                \"Attack_Success\": 100 - robust_acc,\n",
    "            })\n",
    "\n",
    "elif structure == \"attack_first_empty\":\n",
    "    # ========================================================================\n",
    "    # STRUCTURE EXISTS BUT EMPTY - Evaluation wasn't run\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö†Ô∏è  ADVERSARIAL EVALUATION NOT YET COMPLETED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\"\"\n",
    "The data structure exists but contains no results.\n",
    "This means Cell 9 (Adversarial Evaluation) needs to be run first.\n",
    "\n",
    "Please run one of the following cells before this one:\n",
    "  ‚Ä¢ Original Cell 9 (CELL 9: RUN ADVERSARIAL EVALUATION - ALL ATTACKS)\n",
    "  ‚Ä¢ OR @title Cell 9 (üöÄ Cell 9: Run Full Adversarial Evaluation)\n",
    "\n",
    "Note: The original Cell 9 uses CONFIG dict and is simpler.\n",
    "      The @title Cell 9 requires config object and AdversarialEvaluator class.\n",
    "\"\"\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Could not detect data structure!\")\n",
    "    print(f\"   all_results type: {type(all_results)}\")\n",
    "    print(f\"   all_results: {all_results}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE DATAFRAME AND DISPLAY RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "\n",
    "if len(df_results) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è No results to display. Please run Cell 9 first.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Created results DataFrame with {len(df_results)} rows\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # AGGREGATE STATISTICS\n",
    "    # ========================================================================\n",
    "    print(\"\\nüìà AGGREGATED RESULTS (Mean ¬± Std across seeds)\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    summary_data = []\n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        for eps in df_results[df_results[\"Attack\"] == attack][\"Epsilon\"].unique():\n",
    "            subset = df_results[(df_results[\"Attack\"] == attack) & (df_results[\"Epsilon\"] == eps)]\n",
    "            if len(subset) > 0:\n",
    "                summary_data.append({\n",
    "                    \"Attack\": attack,\n",
    "                    \"Epsilon\": eps,\n",
    "                    \"Clean_Acc\": f\"{subset['Clean_Acc'].mean():.2f} ¬± {subset['Clean_Acc'].std():.2f}\",\n",
    "                    \"Robust_Acc\": f\"{subset['Robust_Acc'].mean():.2f} ¬± {subset['Robust_Acc'].std():.2f}\",\n",
    "                    \"Acc_Drop\": f\"{subset['Acc_Drop'].mean():.2f} ¬± {subset['Acc_Drop'].std():.2f}\",\n",
    "                })\n",
    "\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    print(df_summary.to_string(index=False))\n",
    "\n",
    "    # ========================================================================\n",
    "    # DETAILED PER-SEED TABLE\n",
    "    # ========================================================================\n",
    "    print(\"\\n\\nüìã DETAILED RESULTS (Per Seed)\")\n",
    "    print(\"-\" * 70)\n",
    "    display_cols = [\"Seed\", \"Attack\", \"Epsilon\", \"Clean_Acc\", \"Robust_Acc\", \"Acc_Drop\"]\n",
    "    print(df_results[display_cols].to_string(index=False))\n",
    "\n",
    "    # ========================================================================\n",
    "    # KEY FINDINGS\n",
    "    # ========================================================================\n",
    "    print(\"\\n\\nüî¥ KEY VULNERABILITY FINDINGS\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    clean_mean = df_results[\"Clean_Acc\"].mean()\n",
    "    print(f\"   üìä Clean Accuracy:       {clean_mean:.2f}%\")\n",
    "\n",
    "    fgsm_8 = df_results[(df_results[\"Attack\"] == \"FGSM\") & (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "    if len(fgsm_8) > 0:\n",
    "        print(f\"   ‚ö° FGSM (Œµ=8/255):       {fgsm_8['Robust_Acc'].mean():.2f}% (‚Üì{clean_mean - fgsm_8['Robust_Acc'].mean():.1f}%)\")\n",
    "\n",
    "    pgd_8 = df_results[df_results[\"Attack\"].str.contains(\"PGD\") & (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "    if len(pgd_8) > 0:\n",
    "        print(f\"   üîÑ PGD (Œµ=8/255):        {pgd_8['Robust_Acc'].mean():.2f}% (‚Üì{clean_mean - pgd_8['Robust_Acc'].mean():.1f}%)\")\n",
    "\n",
    "    cw = df_results[df_results[\"Attack\"] == \"C&W-L2\"]\n",
    "    if len(cw) > 0:\n",
    "        print(f\"   üéØ C&W L2:               {cw['Robust_Acc'].mean():.2f}% (‚Üì{clean_mean - cw['Robust_Acc'].mean():.1f}%)\")\n",
    "\n",
    "    print(\"\\n   üí° CONCLUSION: Baseline model is HIGHLY VULNERABLE to adversarial attacks!\")\n",
    "    print(\"   ‚ö†Ô∏è  Adversarial training (Phase 5) is essential for clinical deployment.\")\n",
    "\n",
    "print(\"\\n‚úÖ Results summary complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ebcb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "007ebcb0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764469987599,
     "user_tz": 0,
     "elapsed": 5,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "c399022d-1955-4fa5-897f-96c8077cd741"
   },
   "outputs": [],
   "source": [
    "#@title üìà Cell 11: PhD-Level Visualization - Robustness Degradation Curves\n",
    "#@markdown **Publication-quality robustness vs perturbation strength plots**\n",
    "\n",
    "def create_robustness_curves(df_results: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create interactive robustness degradation curves.\n",
    "\n",
    "    Shows how accuracy degrades with increasing perturbation budget Œµ.\n",
    "    \"\"\"\n",
    "    # Check if DataFrame is empty\n",
    "    if len(df_results) == 0:\n",
    "        print(\"‚ö†Ô∏è No results data available for visualization\")\n",
    "        return go.Figure()\n",
    "\n",
    "    # Get PGD steps from Attack column (e.g., \"PGD-40\" -> 40)\n",
    "    pgd_attacks = df_results[df_results[\"Attack\"].str.startswith(\"PGD\")][\"Attack\"].unique()\n",
    "    pgd_name = pgd_attacks[0] if len(pgd_attacks) > 0 else \"PGD-40\"\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    attacks = [\"FGSM\", pgd_name]\n",
    "    colors = {\"FGSM\": \"#FF6B6B\", pgd_name: \"#4ECDC4\"}\n",
    "    markers = {\"FGSM\": \"circle\", pgd_name: \"square\"}\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\n",
    "            \"<b>Robustness Degradation by Attack Type</b>\",\n",
    "            \"<b>Accuracy Drop Severity</b>\"\n",
    "        ),\n",
    "        horizontal_spacing=0.12\n",
    "    )\n",
    "\n",
    "    # Add clean accuracy reference line\n",
    "    clean_acc = df_results[\"Clean_Acc\"].mean()\n",
    "\n",
    "    for attack in attacks:\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack].copy()\n",
    "        if len(attack_data) == 0:\n",
    "            continue\n",
    "\n",
    "        # Group by epsilon\n",
    "        grouped = attack_data.groupby(\"Epsilon_Val\").agg({\n",
    "            \"Robust_Acc\": [\"mean\", \"std\"],\n",
    "            \"Acc_Drop\": [\"mean\", \"std\"]\n",
    "        }).reset_index()\n",
    "        grouped.columns = [\"Epsilon\", \"Robust_Mean\", \"Robust_Std\", \"Drop_Mean\", \"Drop_Std\"]\n",
    "        grouped = grouped.sort_values(\"Epsilon\")\n",
    "\n",
    "        # Filter out non-numeric epsilons (like C&W L2)\n",
    "        grouped = grouped[grouped[\"Epsilon\"] > 0]\n",
    "\n",
    "        if len(grouped) == 0:\n",
    "            continue\n",
    "\n",
    "        # Convert epsilon to string labels for x-axis\n",
    "        epsilon_labels = [f\"{int(e*255)}/255\" for e in grouped[\"Epsilon\"]]\n",
    "\n",
    "        # Plot 1: Robustness curves with confidence bands\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epsilon_labels,\n",
    "                y=grouped[\"Robust_Mean\"],\n",
    "                mode=\"lines+markers\",\n",
    "                name=attack,\n",
    "                line=dict(color=colors.get(attack, \"#888888\"), width=3),\n",
    "                marker=dict(size=12, symbol=markers.get(attack, \"circle\")),\n",
    "                error_y=dict(\n",
    "                    type=\"data\",\n",
    "                    array=grouped[\"Robust_Std\"].fillna(0),\n",
    "                    visible=True,\n",
    "                    color=colors.get(attack, \"#888888\"),\n",
    "                    thickness=2\n",
    "                ),\n",
    "                legendgroup=attack,\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        # Plot 2: Accuracy drop bars\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=epsilon_labels,\n",
    "                y=grouped[\"Drop_Mean\"],\n",
    "                name=attack,\n",
    "                marker_color=colors.get(attack, \"#888888\"),\n",
    "                error_y=dict(\n",
    "                    type=\"data\",\n",
    "                    array=grouped[\"Drop_Std\"].fillna(0),\n",
    "                    visible=True\n",
    "                ),\n",
    "                legendgroup=attack,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "    # Add clean accuracy reference\n",
    "    fig.add_hline(\n",
    "        y=clean_acc,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"gray\",\n",
    "        annotation_text=f\"Clean: {clean_acc:.1f}%\",\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=500,\n",
    "        width=1100,\n",
    "        title=dict(\n",
    "            text=\"<b>Adversarial Robustness Analysis: Baseline ResNet-50 on ISIC 2018</b>\",\n",
    "            font=dict(size=18),\n",
    "            x=0.5\n",
    "        ),\n",
    "        font=dict(family=\"Arial\", size=12),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.08,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        ),\n",
    "        paper_bgcolor=\"white\",\n",
    "        plot_bgcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    # Axis labels\n",
    "    fig.update_xaxes(title_text=\"Perturbation Budget (Œµ)\", row=1, col=1, gridcolor=\"lightgray\")\n",
    "    fig.update_xaxes(title_text=\"Perturbation Budget (Œµ)\", row=1, col=2, gridcolor=\"lightgray\")\n",
    "    fig.update_yaxes(title_text=\"Robust Accuracy (%)\", row=1, col=1, gridcolor=\"lightgray\", range=[0, 100])\n",
    "    fig.update_yaxes(title_text=\"Accuracy Drop (pp)\", row=1, col=2, gridcolor=\"lightgray\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Create and display\n",
    "if 'df_results' in dir() and len(df_results) > 0:\n",
    "    fig_robustness = create_robustness_curves(df_results)\n",
    "    fig_robustness.show()\n",
    "\n",
    "    # Save figure\n",
    "    save_path = Path(\"results/phase4/figures\") if 'paths' not in dir() else paths.figures_dir\n",
    "    if save_path.exists():\n",
    "        fig_robustness.write_html(save_path / \"robustness_curves.html\")\n",
    "        try:\n",
    "            fig_robustness.write_image(save_path / \"robustness_curves.png\", scale=2)\n",
    "            print(f\"‚úÖ Saved to {save_path / 'robustness_curves.png'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not save PNG (kaleido may not be installed): {e}\")\n",
    "            print(f\"‚úÖ Saved HTML to {save_path / 'robustness_curves.html'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è df_results not found or empty. Please run Cell 10 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e86ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "f14e86ad",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764469987697,
     "user_tz": 0,
     "elapsed": 79,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "8436c181-3091-4dbf-abc9-750838633f4b"
   },
   "outputs": [],
   "source": [
    "#@title üî• Cell 12: PhD-Level Visualization - Attack Comparison Heatmap\n",
    "#@markdown **Visual comparison of attack effectiveness across seeds and epsilon values**\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ATTACK COMPARISON VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_attack_heatmap_from_df(df_results: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create attack comparison heatmap from results DataFrame.\n",
    "    Works with both data structures.\n",
    "    \"\"\"\n",
    "    if len(df_results) == 0:\n",
    "        print(\"‚ö†Ô∏è No results data available\")\n",
    "        return go.Figure()\n",
    "\n",
    "    # Filter to FGSM and PGD attacks only\n",
    "    df_attacks = df_results[df_results[\"Attack\"].isin([\"FGSM\"]) |\n",
    "                            df_results[\"Attack\"].str.startswith(\"PGD\")]\n",
    "\n",
    "    if len(df_attacks) == 0:\n",
    "        print(\"‚ö†Ô∏è No FGSM/PGD results found\")\n",
    "        return go.Figure()\n",
    "\n",
    "    # Create pivot table: rows = epsilon, cols = seed, values = robust accuracy\n",
    "    # First for FGSM\n",
    "    fgsm_data = df_attacks[df_attacks[\"Attack\"] == \"FGSM\"]\n",
    "    pgd_data = df_attacks[df_attacks[\"Attack\"].str.startswith(\"PGD\")]\n",
    "\n",
    "    # Build heatmap matrix\n",
    "    seeds = sorted(df_results[\"Seed\"].unique())\n",
    "    epsilons = sorted([e for e in df_results[\"Epsilon\"].unique() if e not in [\"L2\", \"N/A\"]])\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\"<b>FGSM Attack Accuracy</b>\", \"<b>PGD Attack Accuracy</b>\"),\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "\n",
    "    for idx, (attack_name, attack_df) in enumerate([(\"FGSM\", fgsm_data), (\"PGD\", pgd_data)]):\n",
    "        if len(attack_df) == 0:\n",
    "            continue\n",
    "\n",
    "        # Build matrix\n",
    "        matrix = []\n",
    "        for eps in epsilons:\n",
    "            row = []\n",
    "            for seed in seeds:\n",
    "                val = attack_df[(attack_df[\"Epsilon\"] == eps) &\n",
    "                               (attack_df[\"Seed\"] == seed)][\"Robust_Acc\"]\n",
    "                row.append(val.values[0] if len(val) > 0 else np.nan)\n",
    "            matrix.append(row)\n",
    "\n",
    "        matrix = np.array(matrix)\n",
    "\n",
    "        # Add heatmap trace\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=matrix,\n",
    "                x=[f\"Seed {s}\" for s in seeds],\n",
    "                y=[f\"Œµ={e}\" for e in epsilons],\n",
    "                colorscale=[\n",
    "                    [0, \"#d73027\"],      # Red - low accuracy (vulnerable)\n",
    "                    [0.25, \"#fc8d59\"],   # Orange\n",
    "                    [0.5, \"#fee08b\"],    # Yellow\n",
    "                    [0.75, \"#91cf60\"],   # Light green\n",
    "                    [1, \"#1a9850\"]       # Dark green - robust\n",
    "                ],\n",
    "                colorbar=dict(\n",
    "                    title=\"Accuracy (%)\",\n",
    "                    x=1.02 if idx == 1 else 0.45,\n",
    "                    len=0.9\n",
    "                ),\n",
    "                text=np.round(matrix, 1),\n",
    "                texttemplate=\"%{text:.1f}%\",\n",
    "                textfont=dict(size=12),\n",
    "                showscale=(idx == 1),  # Only show colorbar for second plot\n",
    "                hovertemplate=f\"{attack_name}<br>%{{y}}<br>%{{x}}<br>Acc: %{{z:.1f}}%<extra></extra>\"\n",
    "            ),\n",
    "            row=1, col=idx+1\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=400,\n",
    "        width=1000,\n",
    "        title=dict(\n",
    "            text=\"<b>Adversarial Robustness Heatmap: Baseline ResNet-50</b><br>\"\n",
    "                 \"<sup>üî¥ Low Accuracy (Vulnerable) ‚Üí üü¢ High Accuracy (Robust)</sup>\",\n",
    "            font=dict(size=16),\n",
    "            x=0.5\n",
    "        ),\n",
    "        font=dict(family=\"Arial\", size=12),\n",
    "        paper_bgcolor=\"white\",\n",
    "        plot_bgcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Create and display\n",
    "if 'df_results' in dir() and len(df_results) > 0:\n",
    "    fig_heatmap = create_attack_heatmap_from_df(df_results)\n",
    "    fig_heatmap.show()\n",
    "\n",
    "    # Save\n",
    "    save_path = Path(\"results/phase4/figures\") if 'paths' not in dir() else paths.figures_dir\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fig_heatmap.write_html(save_path / \"attack_heatmap.html\")\n",
    "    try:\n",
    "        fig_heatmap.write_image(save_path / \"attack_heatmap.png\", scale=2)\n",
    "        print(f\"‚úÖ Saved to {save_path / 'attack_heatmap.png'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úÖ Saved HTML to {save_path / 'attack_heatmap.html'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è df_results not found or empty. Please run Cell 10 first.\")\n",
    "\n",
    "# ============================================================================\n",
    "# VULNERABILITY SUMMARY BY ATTACK TYPE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüéØ VULNERABILITY SUMMARY BY ATTACK\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if 'df_results' in dir() and len(df_results) > 0:\n",
    "    clean_mean = df_results[\"Clean_Acc\"].mean()\n",
    "\n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        attack_df = df_results[df_results[\"Attack\"] == attack]\n",
    "\n",
    "        for eps in sorted(attack_df[\"Epsilon\"].unique()):\n",
    "            subset = attack_df[attack_df[\"Epsilon\"] == eps]\n",
    "            robust_mean = subset[\"Robust_Acc\"].mean()\n",
    "            robust_std = subset[\"Robust_Acc\"].std()\n",
    "            drop = clean_mean - robust_mean\n",
    "\n",
    "            status = \"üî¥ CRITICAL\" if robust_mean < 20 else \"üü° MODERATE\" if robust_mean < 50 else \"üü¢ ROBUST\"\n",
    "            print(f\"   {attack:12} Œµ={eps:6}: {robust_mean:5.1f}% ¬± {robust_std:4.1f}% (‚Üì{drop:5.1f}%) {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a849c1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "9a849c1b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764469989209,
     "user_tz": 0,
     "elapsed": 1513,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "dfe5ae92-ce12-499e-8a8c-8e71fa8ca6fd"
   },
   "outputs": [],
   "source": [
    "#@title üéØ Cell 13: PhD-Level Visualization - Robustness Summary Dashboard\n",
    "#@markdown **Comprehensive summary visualization of all attack results**\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ROBUSTNESS SUMMARY DASHBOARD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_summary_dashboard(df_results: pd.DataFrame) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create comprehensive summary dashboard using matplotlib.\n",
    "    Works with any data structure.\n",
    "    \"\"\"\n",
    "    if len(df_results) == 0:\n",
    "        print(\"‚ö†Ô∏è No results data available\")\n",
    "        return None\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.25)\n",
    "\n",
    "    # Style settings\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    colors_attacks = {\"FGSM\": \"#4169E1\", \"PGD\": \"#DC143C\", \"C&W\": \"#9400D3\"}\n",
    "\n",
    "    clean_mean = df_results[\"Clean_Acc\"].mean()\n",
    "    seeds = sorted(df_results[\"Seed\"].unique())\n",
    "\n",
    "    # ============================================================\n",
    "    # Panel 1: Bar chart - Accuracy by Attack Type\n",
    "    # ============================================================\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "    # Group by attack type and get mean accuracy\n",
    "    attack_groups = df_results.groupby(\"Attack\").agg({\n",
    "        \"Robust_Acc\": [\"mean\", \"std\"]\n",
    "    }).reset_index()\n",
    "    attack_groups.columns = [\"Attack\", \"Mean\", \"Std\"]\n",
    "    attack_groups = attack_groups.sort_values(\"Mean\", ascending=True)\n",
    "\n",
    "    # Determine colors based on attack type\n",
    "    bar_colors = []\n",
    "    for attack in attack_groups[\"Attack\"]:\n",
    "        if \"FGSM\" in attack:\n",
    "            bar_colors.append(colors_attacks[\"FGSM\"])\n",
    "        elif \"PGD\" in attack:\n",
    "            bar_colors.append(colors_attacks[\"PGD\"])\n",
    "        else:\n",
    "            bar_colors.append(colors_attacks[\"C&W\"])\n",
    "\n",
    "    bars = ax1.barh(attack_groups[\"Attack\"], attack_groups[\"Mean\"],\n",
    "                    xerr=attack_groups[\"Std\"], capsize=5,\n",
    "                    color=bar_colors, edgecolor=\"white\", linewidth=1.5, alpha=0.85)\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, mean_val in zip(bars, attack_groups[\"Mean\"]):\n",
    "        ax1.text(bar.get_width() + 2, bar.get_y() + bar.get_height()/2,\n",
    "                 f'{mean_val:.1f}%', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "    ax1.axvline(x=clean_mean, color='green', linestyle='--', linewidth=2,\n",
    "                label=f'Clean: {clean_mean:.1f}%')\n",
    "    ax1.set_xlabel(\"Robust Accuracy (%)\", fontsize=12, fontweight='bold')\n",
    "    ax1.set_title(\"A) Robust Accuracy by Attack Type\", fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlim(0, 100)\n",
    "    ax1.legend(loc='upper right')\n",
    "\n",
    "    # ============================================================\n",
    "    # Panel 2: Line chart - Accuracy Degradation by Epsilon\n",
    "    # ============================================================\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    # Get FGSM and PGD results only (exclude C&W which has no epsilon)\n",
    "    fgsm_df = df_results[df_results[\"Attack\"] == \"FGSM\"]\n",
    "    pgd_df = df_results[df_results[\"Attack\"].str.startswith(\"PGD\")]\n",
    "\n",
    "    for attack_type, attack_df, color in [(\"FGSM\", fgsm_df, colors_attacks[\"FGSM\"]),\n",
    "                                           (\"PGD\", pgd_df, colors_attacks[\"PGD\"])]:\n",
    "        if len(attack_df) == 0:\n",
    "            continue\n",
    "\n",
    "        # Group by epsilon\n",
    "        grouped = attack_df.groupby(\"Epsilon_Val\").agg({\n",
    "            \"Robust_Acc\": [\"mean\", \"std\"]\n",
    "        }).reset_index()\n",
    "        grouped.columns = [\"Epsilon\", \"Mean\", \"Std\"]\n",
    "        grouped = grouped[grouped[\"Epsilon\"] > 0].sort_values(\"Epsilon\")\n",
    "\n",
    "        if len(grouped) > 0:\n",
    "            eps_labels = [f\"{int(e*255)}/255\" for e in grouped[\"Epsilon\"]]\n",
    "            ax2.errorbar(eps_labels, grouped[\"Mean\"], yerr=grouped[\"Std\"],\n",
    "                        marker='o', markersize=10, linewidth=2.5, capsize=5,\n",
    "                        label=attack_type, color=color)\n",
    "\n",
    "    ax2.axhline(y=clean_mean, color='green', linestyle='--', linewidth=2,\n",
    "                label=f'Clean: {clean_mean:.1f}%')\n",
    "    ax2.set_xlabel(\"Perturbation Budget (Œµ)\", fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel(\"Robust Accuracy (%)\", fontsize=12, fontweight='bold')\n",
    "    ax2.set_title(\"B) Accuracy Degradation with Œµ\", fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # ============================================================\n",
    "    # Panel 3: Grouped bar - Cross-Seed Comparison\n",
    "    # ============================================================\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "    x = np.arange(len(seeds))\n",
    "    width = 0.25\n",
    "\n",
    "    # Get strongest attack results for comparison\n",
    "    for i, (attack_label, attack_filter) in enumerate([\n",
    "        (\"FGSM Œµ=8/255\", (df_results[\"Attack\"] == \"FGSM\") & (df_results[\"Epsilon\"] == \"8/255\")),\n",
    "        (\"PGD Œµ=8/255\", df_results[\"Attack\"].str.startswith(\"PGD\") & (df_results[\"Epsilon\"] == \"8/255\")),\n",
    "        (\"C&W L2\", df_results[\"Attack\"] == \"C&W-L2\")\n",
    "    ]):\n",
    "        subset = df_results[attack_filter]\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "\n",
    "        values = [subset[subset[\"Seed\"] == s][\"Robust_Acc\"].values[0]\n",
    "                  if len(subset[subset[\"Seed\"] == s]) > 0 else 0 for s in seeds]\n",
    "\n",
    "        color = colors_attacks[\"FGSM\"] if \"FGSM\" in attack_label else (\n",
    "                colors_attacks[\"PGD\"] if \"PGD\" in attack_label else colors_attacks[\"C&W\"])\n",
    "\n",
    "        bars = ax3.bar(x + i*width, values, width, label=attack_label,\n",
    "                      color=color, edgecolor='white', linewidth=1.5)\n",
    "\n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2, height + 1,\n",
    "                    f'{height:.0f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "    ax3.set_xlabel(\"Random Seed\", fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel(\"Robust Accuracy (%)\", fontsize=12, fontweight='bold')\n",
    "    ax3.set_title(\"C) Cross-Seed Robustness Comparison\", fontsize=14, fontweight='bold')\n",
    "    ax3.set_xticks(x + width)\n",
    "    ax3.set_xticklabels([f\"Seed {s}\" for s in seeds])\n",
    "    ax3.legend(loc='upper right', fontsize=9)\n",
    "    ax3.set_ylim(0, 100)\n",
    "\n",
    "    # ============================================================\n",
    "    # Panel 4: Summary Statistics Box\n",
    "    # ============================================================\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.axis('off')\n",
    "\n",
    "    # Calculate key statistics\n",
    "    fgsm_8 = df_results[(df_results[\"Attack\"] == \"FGSM\") & (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "    pgd_8 = df_results[df_results[\"Attack\"].str.startswith(\"PGD\") & (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "    cw = df_results[df_results[\"Attack\"] == \"C&W-L2\"]\n",
    "\n",
    "    fgsm_robust = fgsm_8[\"Robust_Acc\"].mean() if len(fgsm_8) > 0 else 0\n",
    "    pgd_robust = pgd_8[\"Robust_Acc\"].mean() if len(pgd_8) > 0 else 0\n",
    "    cw_robust = cw[\"Robust_Acc\"].mean() if len(cw) > 0 else 0\n",
    "\n",
    "    summary_text = f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë         KEY FINDINGS: ADVERSARIAL ROBUSTNESS                 ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                              ‚ïë\n",
    "‚ïë   üìä BASELINE PERFORMANCE                                    ‚ïë\n",
    "‚ïë   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚ïë\n",
    "‚ïë   Clean Accuracy:          {clean_mean:>6.2f}%                         ‚ïë\n",
    "‚ïë   Number of Seeds:         {len(seeds):>6}                             ‚ïë\n",
    "‚ïë                                                              ‚ïë\n",
    "‚ïë   ‚ö†Ô∏è  VULNERABILITY ASSESSMENT (Œµ=8/255)                     ‚ïë\n",
    "‚ïë   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚ïë\n",
    "‚ïë   FGSM Attack:             {fgsm_robust:>6.2f}%  (‚Üì{clean_mean-fgsm_robust:>5.1f}%)           ‚ïë\n",
    "‚ïë   PGD-40 Attack:           {pgd_robust:>6.2f}%  (‚Üì{clean_mean-pgd_robust:>5.1f}%)           ‚ïë\n",
    "‚ïë   C&W L2 Attack:           {cw_robust:>6.2f}%  (‚Üì{clean_mean-cw_robust:>5.1f}%)           ‚ïë\n",
    "‚ïë                                                              ‚ïë\n",
    "‚ïë   üí° IMPLICATIONS                                            ‚ïë\n",
    "‚ïë   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚ïë\n",
    "‚ïë   ‚Ä¢ Model is HIGHLY VULNERABLE to adversarial attacks        ‚ïë\n",
    "‚ïë   ‚Ä¢ Iterative attacks (PGD) are more effective than FGSM     ‚ïë\n",
    "‚ïë   ‚Ä¢ Adversarial training essential for clinical use          ‚ïë\n",
    "‚ïë                                                              ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\"\n",
    "\n",
    "    ax4.text(0.5, 0.5, summary_text, transform=ax4.transAxes, fontsize=10,\n",
    "             fontfamily='monospace', verticalalignment='center', horizontalalignment='center',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', facecolor='#f8f9fa', edgecolor='#dee2e6', linewidth=2))\n",
    "\n",
    "    plt.suptitle(\"Phase 4: Adversarial Robustness Evaluation ‚Äî Baseline ResNet-50 on ISIC 2018\",\n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Create and display\n",
    "if 'df_results' in dir() and len(df_results) > 0:\n",
    "    fig_dashboard = create_summary_dashboard(df_results)\n",
    "    if fig_dashboard:\n",
    "        plt.show()\n",
    "\n",
    "        # Save\n",
    "        save_path = Path(\"results/phase4/figures\") if 'paths' not in dir() else paths.figures_dir\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        fig_dashboard.savefig(save_path / \"robustness_dashboard.png\",\n",
    "                             dpi=300, bbox_inches='tight', facecolor='white')\n",
    "        print(f\"‚úÖ Saved dashboard to {save_path / 'robustness_dashboard.png'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è df_results not found or empty. Please run Cell 10 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52942c38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 897
    },
    "id": "52942c38",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764469989300,
     "user_tz": 0,
     "elapsed": 89,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "bdeb35e2-a2ac-4e6a-b952-aa0396d6c186"
   },
   "outputs": [],
   "source": [
    "#@title üï∏Ô∏è Cell 14: PhD-Level Visualization - Radar Chart & Attack Effectiveness\n",
    "#@markdown **Multi-dimensional attack comparison using polar/radar charts**\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ATTACK EFFECTIVENESS RADAR CHART\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_radar_chart_from_df(df_results: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create radar chart comparing attack effectiveness using DataFrame results.\n",
    "    Works with both data structures.\n",
    "    \"\"\"\n",
    "    if len(df_results) == 0:\n",
    "        print(\"‚ö†Ô∏è No results data available\")\n",
    "        return go.Figure()\n",
    "\n",
    "    clean_mean = df_results[\"Clean_Acc\"].mean()\n",
    "\n",
    "    # Get attack categories and their strongest epsilon results\n",
    "    attack_metrics = {}\n",
    "\n",
    "    # FGSM at Œµ=8/255\n",
    "    fgsm_8 = df_results[(df_results[\"Attack\"] == \"FGSM\") & (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "    if len(fgsm_8) > 0:\n",
    "        attack_metrics[\"FGSM Œµ=8/255\"] = {\n",
    "            \"Accuracy Drop\": (clean_mean - fgsm_8[\"Robust_Acc\"].mean()),\n",
    "            \"Attack Success\": (100 - fgsm_8[\"Robust_Acc\"].mean()),\n",
    "            \"Consistency\": (100 - fgsm_8[\"Robust_Acc\"].std() * 5),  # Lower std = more consistent\n",
    "            \"Speed\": 95,  # Single-step attack\n",
    "            \"Robustness Impact\": fgsm_8[\"Acc_Drop\"].mean() if \"Acc_Drop\" in fgsm_8.columns else clean_mean - fgsm_8[\"Robust_Acc\"].mean()\n",
    "        }\n",
    "\n",
    "    # PGD at Œµ=8/255\n",
    "    pgd_8 = df_results[df_results[\"Attack\"].str.startswith(\"PGD\") & (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "    if len(pgd_8) > 0:\n",
    "        attack_metrics[\"PGD Œµ=8/255\"] = {\n",
    "            \"Accuracy Drop\": (clean_mean - pgd_8[\"Robust_Acc\"].mean()),\n",
    "            \"Attack Success\": (100 - pgd_8[\"Robust_Acc\"].mean()),\n",
    "            \"Consistency\": (100 - pgd_8[\"Robust_Acc\"].std() * 5),\n",
    "            \"Speed\": 40,  # Multi-step attack, slower\n",
    "            \"Robustness Impact\": pgd_8[\"Acc_Drop\"].mean() if \"Acc_Drop\" in pgd_8.columns else clean_mean - pgd_8[\"Robust_Acc\"].mean()\n",
    "        }\n",
    "\n",
    "    # C&W\n",
    "    cw = df_results[df_results[\"Attack\"] == \"C&W-L2\"]\n",
    "    if len(cw) > 0:\n",
    "        attack_metrics[\"C&W L2\"] = {\n",
    "            \"Accuracy Drop\": (clean_mean - cw[\"Robust_Acc\"].mean()),\n",
    "            \"Attack Success\": (100 - cw[\"Robust_Acc\"].mean()),\n",
    "            \"Consistency\": (100 - cw[\"Robust_Acc\"].std() * 5),\n",
    "            \"Speed\": 10,  # Optimization-based, very slow\n",
    "            \"Robustness Impact\": cw[\"Acc_Drop\"].mean() if \"Acc_Drop\" in cw.columns else clean_mean - cw[\"Robust_Acc\"].mean()\n",
    "        }\n",
    "\n",
    "    if len(attack_metrics) == 0:\n",
    "        print(\"‚ö†Ô∏è No attack metrics available\")\n",
    "        return go.Figure()\n",
    "\n",
    "    # Create radar chart\n",
    "    dimensions = list(list(attack_metrics.values())[0].keys())\n",
    "    colors = {\"FGSM Œµ=8/255\": \"#4169E1\", \"PGD Œµ=8/255\": \"#DC143C\", \"C&W L2\": \"#9400D3\"}\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for attack_name, metrics in attack_metrics.items():\n",
    "        values = [min(max(metrics[dim], 0), 100) for dim in dimensions]  # Clip to 0-100\n",
    "        values.append(values[0])  # Close the polygon\n",
    "\n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=dimensions + [dimensions[0]],\n",
    "            fill='toself',\n",
    "            fillcolor=colors.get(attack_name, \"#888888\"),\n",
    "            opacity=0.3,\n",
    "            line=dict(color=colors.get(attack_name, \"#888888\"), width=2),\n",
    "            name=attack_name\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 100],\n",
    "                ticksuffix=\"%\"\n",
    "            )\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        title=dict(\n",
    "            text=\"<b>Attack Effectiveness Comparison</b><br>\"\n",
    "                 \"<sup>Higher values = More effective attack / More vulnerable model</sup>\",\n",
    "            font=dict(size=16),\n",
    "            x=0.5\n",
    "        ),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=-0.2,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        ),\n",
    "        height=600,\n",
    "        width=700,\n",
    "        font=dict(family=\"Arial\", size=12),\n",
    "        paper_bgcolor=\"white\"\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Create and display\n",
    "if 'df_results' in dir() and len(df_results) > 0:\n",
    "    fig_radar = create_radar_chart_from_df(df_results)\n",
    "    fig_radar.show()\n",
    "\n",
    "    # Save\n",
    "    save_path = Path(\"results/phase4/figures\") if 'paths' not in dir() else paths.figures_dir\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fig_radar.write_html(save_path / \"attack_radar.html\")\n",
    "    try:\n",
    "        fig_radar.write_image(save_path / \"attack_radar.png\", scale=2)\n",
    "        print(f\"‚úÖ Saved radar chart to {save_path / 'attack_radar.png'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úÖ Saved HTML to {save_path / 'attack_radar.html'}\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # ATTACK EFFECTIVENESS TABLE\n",
    "    # ============================================================================\n",
    "    print(\"\\nüìä ATTACK EFFECTIVENESS SUMMARY\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    clean_mean = df_results[\"Clean_Acc\"].mean()\n",
    "\n",
    "    table_data = []\n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        subset = df_results[df_results[\"Attack\"] == attack]\n",
    "        for eps in sorted(subset[\"Epsilon\"].unique()):\n",
    "            eps_subset = subset[subset[\"Epsilon\"] == eps]\n",
    "            robust = eps_subset[\"Robust_Acc\"].mean()\n",
    "            success = 100 - robust\n",
    "\n",
    "            table_data.append({\n",
    "                \"Attack\": attack,\n",
    "                \"Epsilon\": eps,\n",
    "                \"Robust Acc\": f\"{robust:.1f}%\",\n",
    "                \"Success Rate\": f\"{success:.1f}%\",\n",
    "                \"Rating\": \"‚≠ê‚≠ê‚≠ê\" if success > 80 else \"‚≠ê‚≠ê\" if success > 50 else \"‚≠ê\"\n",
    "            })\n",
    "\n",
    "    effect_df = pd.DataFrame(table_data)\n",
    "    print(effect_df.to_string(index=False))\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è df_results not found or empty. Please run Cell 10 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc0f6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1x_GLD1VMK0yIzlY9_dZTgmcFyy9kKXtU"
    },
    "id": "46cc0f6d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764470002857,
     "user_tz": 0,
     "elapsed": 13558,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "160e61d6-be02-4697-c233-2471ea5b06af"
   },
   "outputs": [],
   "source": [
    "#@title üñºÔ∏è Cell 15: PhD-Level Visualization - Adversarial Example Gallery\n",
    "#@markdown **Visualize clean images, perturbations, and adversarial examples**\n",
    "\n",
    "def visualize_adversarial_examples(\n",
    "    all_results: dict,\n",
    "    attack_key: str = \"pgd\",\n",
    "    epsilon: float = 8/255,\n",
    "    num_examples: int = 5,\n",
    "    amplification: float = 10.0\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create publication-quality adversarial example visualization.\n",
    "\n",
    "    Shows:\n",
    "    - Clean image\n",
    "    - Perturbation (amplified for visibility)\n",
    "    - Adversarial image\n",
    "    - Prediction change\n",
    "    \"\"\"\n",
    "    # Get saved examples from first seed\n",
    "    first_seed = config.seeds[0]\n",
    "\n",
    "    if attack_key == \"cw\":\n",
    "        examples = all_results[attack_key][first_seed][\"l2\"].get(\"saved_examples\", [])\n",
    "    else:\n",
    "        examples = all_results[attack_key][first_seed][epsilon].get(\"saved_examples\", [])\n",
    "\n",
    "    if not examples:\n",
    "        print(\"‚ö†Ô∏è No saved examples available\")\n",
    "        return None\n",
    "\n",
    "    # Select successful attacks (prediction changed)\n",
    "    successful = [e for e in examples if e[\"clean_pred\"] != e[\"adv_pred\"]]\n",
    "    if len(successful) < num_examples:\n",
    "        successful = examples  # Use all if not enough successful\n",
    "\n",
    "    num_examples = min(num_examples, len(successful))\n",
    "\n",
    "    fig, axes = plt.subplots(num_examples, 4, figsize=(16, 4 * num_examples))\n",
    "    if num_examples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for idx, example in enumerate(successful[:num_examples]):\n",
    "        # Get tensors\n",
    "        clean = example[\"clean\"].numpy().transpose(1, 2, 0)  # CHW -> HWC\n",
    "        adv = example[\"adversarial\"].numpy().transpose(1, 2, 0)\n",
    "        perturbation = example[\"perturbation\"].numpy().transpose(1, 2, 0)\n",
    "\n",
    "        true_label = CLASS_NAMES[example[\"true_label\"]]\n",
    "        clean_pred = CLASS_NAMES[example[\"clean_pred\"]]\n",
    "        adv_pred = CLASS_NAMES[example[\"adv_pred\"]]\n",
    "\n",
    "        # Ensure valid range\n",
    "        clean = np.clip(clean, 0, 1)\n",
    "        adv = np.clip(adv, 0, 1)\n",
    "\n",
    "        # Amplify perturbation for visibility\n",
    "        pert_amplified = perturbation * amplification + 0.5\n",
    "        pert_amplified = np.clip(pert_amplified, 0, 1)\n",
    "\n",
    "        # L2 and Linf norms\n",
    "        l2_norm = np.sqrt(np.sum(perturbation ** 2))\n",
    "        linf_norm = np.max(np.abs(perturbation))\n",
    "\n",
    "        # Plot clean image\n",
    "        axes[idx, 0].imshow(clean)\n",
    "        axes[idx, 0].set_title(f\"Clean Image\\nTrue: {true_label}\\nPred: {clean_pred}\", fontsize=11)\n",
    "        axes[idx, 0].axis(\"off\")\n",
    "        if example[\"clean_pred\"] == example[\"true_label\"]:\n",
    "            axes[idx, 0].spines[:].set_visible(True)\n",
    "            for spine in axes[idx, 0].spines.values():\n",
    "                spine.set_edgecolor('green')\n",
    "                spine.set_linewidth(3)\n",
    "\n",
    "        # Plot perturbation\n",
    "        axes[idx, 1].imshow(pert_amplified)\n",
    "        axes[idx, 1].set_title(f\"Perturbation (√ó{amplification:.0f})\\nL‚àû: {linf_norm*255:.2f}/255\\nL2: {l2_norm:.4f}\", fontsize=11)\n",
    "        axes[idx, 1].axis(\"off\")\n",
    "\n",
    "        # Plot adversarial image\n",
    "        axes[idx, 2].imshow(adv)\n",
    "        axes[idx, 2].set_title(f\"Adversarial Image\\nPred: {adv_pred}\", fontsize=11)\n",
    "        axes[idx, 2].axis(\"off\")\n",
    "        if example[\"adv_pred\"] != example[\"true_label\"]:\n",
    "            for spine in axes[idx, 2].spines.values():\n",
    "                spine.set_visible(True)\n",
    "                spine.set_edgecolor('red')\n",
    "                spine.set_linewidth(3)\n",
    "\n",
    "        # Plot difference heatmap\n",
    "        diff = np.mean(np.abs(adv - clean), axis=2)  # Average across channels\n",
    "        im = axes[idx, 3].imshow(diff, cmap=\"hot\", vmin=0, vmax=epsilon * 2)\n",
    "        axes[idx, 3].set_title(f\"Absolute Difference\\n({clean_pred} ‚Üí {adv_pred})\", fontsize=11)\n",
    "        axes[idx, 3].axis(\"off\")\n",
    "        plt.colorbar(im, ax=axes[idx, 3], fraction=0.046, pad=0.04)\n",
    "\n",
    "    attack_label = \"C&W L2\" if attack_key == \"cw\" else f\"{'PGD' if attack_key == 'pgd' else 'FGSM'}-{config.get_epsilon_str(epsilon)}\"\n",
    "    plt.suptitle(\n",
    "        f\"Adversarial Example Gallery: {attack_label} Attack\\n\"\n",
    "        f\"Green border = correct, Red border = misclassified\",\n",
    "        fontsize=16, fontweight=\"bold\", y=1.02\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Create visualization for PGD attack\n",
    "if config.pgd_enabled and config.seeds[0] in all_results.get(\"pgd\", {}):\n",
    "    fig_examples = visualize_adversarial_examples(\n",
    "        all_results,\n",
    "        attack_key=\"pgd\",\n",
    "        epsilon=8/255,\n",
    "        num_examples=5,\n",
    "        amplification=10.0\n",
    "    )\n",
    "    if fig_examples:\n",
    "        plt.show()\n",
    "\n",
    "        # Save\n",
    "        if paths.figures_dir.exists():\n",
    "            fig_examples.savefig(paths.figures_dir / \"adversarial_examples_pgd.png\",\n",
    "                                 dpi=200, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "            print(f\"‚úÖ Saved to {paths.figures_dir / 'adversarial_examples_pgd.png'}\")\n",
    "\n",
    "# Also show FGSM examples if available\n",
    "if config.fgsm_enabled and config.seeds[0] in all_results.get(\"fgsm\", {}):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üì∏ FGSM Attack Examples\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    fig_fgsm = visualize_adversarial_examples(\n",
    "        all_results,\n",
    "        attack_key=\"fgsm\",\n",
    "        epsilon=8/255,\n",
    "        num_examples=3,\n",
    "        amplification=10.0\n",
    "    )\n",
    "    if fig_fgsm:\n",
    "        plt.show()\n",
    "\n",
    "        if paths.figures_dir.exists():\n",
    "            fig_fgsm.savefig(paths.figures_dir / \"adversarial_examples_fgsm.png\",\n",
    "                            dpi=200, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "            print(f\"‚úÖ Saved to {paths.figures_dir / 'adversarial_examples_fgsm.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bbbb6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "82bbbb6b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764472407785,
     "user_tz": 0,
     "elapsed": 163,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "cd740341-7e1d-4f64-dcef-655271f92124"
   },
   "outputs": [],
   "source": [
    "#@title üìä Cell 16: PhD-Level Analysis - Statistical Significance & Seed Consistency\n",
    "#@markdown **Bootstrap confidence intervals and cross-seed consistency analysis**\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def statistical_analysis(df_results: pd.DataFrame, config: EvaluationConfig) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform rigorous statistical analysis of adversarial evaluation results.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"üìä STATISTICAL ANALYSIS OF ADVERSARIAL ROBUSTNESS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. Cross-seed consistency (Coefficient of Variation)\n",
    "    print(\"\\n1Ô∏è‚É£ CROSS-SEED CONSISTENCY (Coefficient of Variation)\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        for eps in df_results[df_results[\"Attack\"] == attack][\"Epsilon\"].unique():\n",
    "            subset = df_results[(df_results[\"Attack\"] == attack) & (df_results[\"Epsilon\"] == eps)]\n",
    "            if len(subset) >= 2:\n",
    "                mean_acc = subset[\"Robust_Acc\"].mean()\n",
    "                std_acc = subset[\"Robust_Acc\"].std()\n",
    "                cv = (std_acc / mean_acc) * 100 if mean_acc > 0 else 0\n",
    "\n",
    "                consistency = \"‚úÖ High\" if cv < 5 else \"‚ö†Ô∏è Medium\" if cv < 10 else \"‚ùå Low\"\n",
    "                print(f\"   {attack:15} {eps:>7}: CV = {cv:5.2f}% {consistency}\")\n",
    "\n",
    "    # 2. Bootstrap 95% Confidence Intervals\n",
    "    print(\"\\n2Ô∏è‚É£ BOOTSTRAP 95% CONFIDENCE INTERVALS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    def bootstrap_ci(data, n_bootstrap=1000, ci=0.95):\n",
    "        \"\"\"Calculate bootstrap confidence interval.\"\"\"\n",
    "        if len(data) < 2:\n",
    "            return data.mean(), data.mean(), data.mean()\n",
    "\n",
    "        bootstraps = []\n",
    "        for _ in range(n_bootstrap):\n",
    "            sample = np.random.choice(data, size=len(data), replace=True)\n",
    "            bootstraps.append(sample.mean())\n",
    "\n",
    "        lower = np.percentile(bootstraps, (1 - ci) / 2 * 100)\n",
    "        upper = np.percentile(bootstraps, (1 + ci) / 2 * 100)\n",
    "        return data.mean(), lower, upper\n",
    "\n",
    "    ci_results = []\n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack]\n",
    "\n",
    "        if attack == \"C&W-L2\":\n",
    "            subset = attack_data\n",
    "            mean, lower, upper = bootstrap_ci(subset[\"Robust_Acc\"].values)\n",
    "            ci_results.append({\n",
    "                \"Attack\": attack,\n",
    "                \"Epsilon\": \"N/A\",\n",
    "                \"Mean\": mean,\n",
    "                \"CI_Lower\": lower,\n",
    "                \"CI_Upper\": upper\n",
    "            })\n",
    "            print(f\"   {attack:15} {'N/A':>7}: {mean:5.2f}% [{lower:5.2f}%, {upper:5.2f}%]\")\n",
    "        else:\n",
    "            for eps in attack_data[\"Epsilon\"].unique():\n",
    "                subset = attack_data[attack_data[\"Epsilon\"] == eps]\n",
    "                mean, lower, upper = bootstrap_ci(subset[\"Robust_Acc\"].values)\n",
    "                ci_results.append({\n",
    "                    \"Attack\": attack,\n",
    "                    \"Epsilon\": eps,\n",
    "                    \"Mean\": mean,\n",
    "                    \"CI_Lower\": lower,\n",
    "                    \"CI_Upper\": upper\n",
    "                })\n",
    "                print(f\"   {attack:15} {eps:>7}: {mean:5.2f}% [{lower:5.2f}%, {upper:5.2f}%]\")\n",
    "\n",
    "    results[\"confidence_intervals\"] = ci_results\n",
    "\n",
    "    # 3. Attack Comparison (Paired t-test: FGSM vs PGD)\n",
    "    print(\"\\n3Ô∏è‚É£ ATTACK COMPARISON (Paired t-test at Œµ=8/255)\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    if config.fgsm_enabled and config.pgd_enabled:\n",
    "        eps_8 = \"8/255\"\n",
    "        fgsm_acc = df_results[(df_results[\"Attack\"] == \"FGSM\") & (df_results[\"Epsilon\"] == eps_8)][\"Robust_Acc\"].values\n",
    "        pgd_acc = df_results[(df_results[\"Attack\"] == f\"PGD-{config.pgd_steps}\") & (df_results[\"Epsilon\"] == eps_8)][\"Robust_Acc\"].values\n",
    "\n",
    "        if len(fgsm_acc) >= 2 and len(pgd_acc) >= 2 and len(fgsm_acc) == len(pgd_acc):\n",
    "            t_stat, p_value = stats.ttest_rel(fgsm_acc, pgd_acc)\n",
    "\n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "\n",
    "            print(f\"   FGSM mean: {fgsm_acc.mean():.2f}%\")\n",
    "            print(f\"   PGD mean:  {pgd_acc.mean():.2f}%\")\n",
    "            print(f\"   Difference: {(fgsm_acc.mean() - pgd_acc.mean()):.2f}pp\")\n",
    "            print(f\"   t-statistic: {t_stat:.3f}\")\n",
    "            print(f\"   p-value: {p_value:.4f} {significance}\")\n",
    "\n",
    "            results[\"fgsm_vs_pgd\"] = {\n",
    "                \"t_statistic\": t_stat,\n",
    "                \"p_value\": p_value,\n",
    "                \"significant\": p_value < 0.05\n",
    "            }\n",
    "\n",
    "    # 4. Effect Size (Cohen's d)\n",
    "    print(\"\\n4Ô∏è‚É£ EFFECT SIZE (Cohen's d): Clean vs Adversarial\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack]\n",
    "        clean_acc = attack_data[\"Clean_Acc\"].values\n",
    "        robust_acc = attack_data[\"Robust_Acc\"].values\n",
    "\n",
    "        # Cohen's d\n",
    "        pooled_std = np.sqrt((clean_acc.std()**2 + robust_acc.std()**2) / 2)\n",
    "        if pooled_std > 0:\n",
    "            cohens_d = (clean_acc.mean() - robust_acc.mean()) / pooled_std\n",
    "        else:\n",
    "            cohens_d = 0\n",
    "\n",
    "        effect = \"Negligible\" if abs(cohens_d) < 0.2 else \"Small\" if abs(cohens_d) < 0.5 else \"Medium\" if abs(cohens_d) < 0.8 else \"Large\"\n",
    "        print(f\"   {attack:15}: d = {cohens_d:6.2f} ({effect})\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run statistical analysis\n",
    "stat_results = statistical_analysis(df_results, config)\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION: SEED CONSISTENCY BOX PLOT\n",
    "# ============================================================================\n",
    "\n",
    "def create_seed_comparison_boxplot(df_results: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"Create box plot comparing robust accuracy across seeds.\"\"\"\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    colors = {\"FGSM\": \"#FF6B6B\", f\"PGD-{config.pgd_steps}\": \"#4ECDC4\", \"C&W-L2\": \"#9B59B6\"}\n",
    "\n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack]\n",
    "\n",
    "        fig.add_trace(go.Box(\n",
    "            y=attack_data[\"Robust_Acc\"],\n",
    "            x=[attack] * len(attack_data),\n",
    "            name=attack,\n",
    "            marker_color=colors.get(attack, \"#888888\"),\n",
    "            boxpoints=\"all\",\n",
    "            jitter=0.3,\n",
    "            pointpos=-1.8,\n",
    "            hovertemplate=\"Seed: %{text}<br>Robust Acc: %{y:.2f}%<extra></extra>\",\n",
    "            text=attack_data[\"Seed\"].astype(str)\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"<b>Cross-Seed Consistency: Robust Accuracy Distribution</b>\",\n",
    "            font=dict(size=16),\n",
    "            x=0.5\n",
    "        ),\n",
    "        xaxis_title=\"Attack Type\",\n",
    "        yaxis_title=\"Robust Accuracy (%)\",\n",
    "        yaxis=dict(range=[0, 100]),\n",
    "        showlegend=False,\n",
    "        height=500,\n",
    "        width=800\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig_boxplot = create_seed_comparison_boxplot(df_results)\n",
    "fig_boxplot.show()\n",
    "\n",
    "# Save with graceful handling of missing kaleido\n",
    "if paths.figures_dir.exists():\n",
    "    fig_boxplot.write_html(paths.figures_dir / \"seed_consistency_boxplot.html\")\n",
    "    try:\n",
    "        fig_boxplot.write_image(paths.figures_dir / \"seed_consistency_boxplot.png\", scale=2)\n",
    "        print(f\"‚úÖ Saved to {paths.figures_dir / 'seed_consistency_boxplot.png'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è PNG export skipped (install kaleido for PNG support): pip install kaleido\")\n",
    "        print(f\"‚úÖ Saved HTML to {paths.figures_dir / 'seed_consistency_boxplot.html'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0bf83c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a0bf83c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764473585131,
     "user_tz": 0,
     "elapsed": 24,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "54a214ba-5bab-492c-f35b-f54dccd9192f"
   },
   "outputs": [],
   "source": [
    "#@title üíæ Cell 17: Save Results & Export for Dissertation\n",
    "#@markdown **Export all results in multiple formats for dissertation use**\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üíæ SAVING RESULTS & EXPORTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS AS CSV\n",
    "# ============================================================================\n",
    "\n",
    "# Detailed results CSV\n",
    "csv_path = paths.results_dir / \"adversarial_robustness_results.csv\"\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"‚úÖ Detailed results: {csv_path}\")\n",
    "\n",
    "# Summary statistics CSV\n",
    "summary_path = paths.results_dir / \"adversarial_robustness_summary.csv\"\n",
    "df_summary.to_csv(summary_path, index=False)\n",
    "print(f\"‚úÖ Summary statistics: {summary_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE AS JSON (for programmatic access)\n",
    "# ============================================================================\n",
    "\n",
    "json_results = {\n",
    "    \"metadata\": {\n",
    "        \"evaluation_date\": datetime.now().isoformat(),\n",
    "        \"seeds\": config.seeds,\n",
    "        \"epsilons\": [float(e) for e in config.epsilons],\n",
    "        \"attacks\": {\n",
    "            \"fgsm\": config.fgsm_enabled,\n",
    "            \"pgd\": config.pgd_enabled,\n",
    "            \"pgd_steps\": config.pgd_steps,\n",
    "            \"cw\": config.cw_enabled,\n",
    "            \"cw_iterations\": config.cw_iterations\n",
    "        },\n",
    "        \"dataset\": \"ISIC 2018\",\n",
    "        \"model\": \"ResNet-50\",\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    },\n",
    "    \"clean_accuracy\": {\n",
    "        str(seed): {\n",
    "            \"accuracy\": float(clean_results[seed][\"accuracy\"]),\n",
    "            \"balanced_accuracy\": float(clean_results[seed][\"balanced_accuracy\"]),\n",
    "            \"f1_macro\": float(clean_results[seed][\"f1_macro\"]),\n",
    "            \"auroc\": float(clean_results[seed][\"auroc\"])\n",
    "        }\n",
    "        for seed in config.seeds if seed in clean_results\n",
    "    },\n",
    "    \"adversarial_results\": df_results.to_dict(orient=\"records\")\n",
    "}\n",
    "\n",
    "json_path = paths.results_dir / \"adversarial_robustness_results.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(json_results, f, indent=2, default=str)\n",
    "print(f\"‚úÖ JSON results: {json_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATE LATEX TABLE FOR DISSERTATION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_latex_table(df_results: pd.DataFrame, config: EvaluationConfig) -> str:\n",
    "    \"\"\"Generate LaTeX table for dissertation.\"\"\"\n",
    "\n",
    "    lines = [\n",
    "        \"\\\\begin{table}[htbp]\",\n",
    "        \"\\\\centering\",\n",
    "        \"\\\\caption{Adversarial Robustness Evaluation: Baseline ResNet-50 on ISIC 2018}\",\n",
    "        \"\\\\label{tab:adversarial_robustness}\",\n",
    "        \"\\\\begin{tabular}{llcccc}\",\n",
    "        \"\\\\toprule\",\n",
    "        \"Attack & $\\\\epsilon$ & Clean Acc (\\\\%) & Robust Acc (\\\\%) & Acc Drop (pp) & ASR (\\\\%) \\\\\\\\\",\n",
    "        \"\\\\midrule\"\n",
    "    ]\n",
    "\n",
    "    # Group and average across seeds\n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack]\n",
    "\n",
    "        for eps in attack_data[\"Epsilon\"].unique():\n",
    "            subset = attack_data[attack_data[\"Epsilon\"] == eps]\n",
    "\n",
    "            clean = f\"{subset['Clean_Acc'].mean():.1f} $\\\\pm$ {subset['Clean_Acc'].std():.1f}\"\n",
    "            robust = f\"{subset['Robust_Acc'].mean():.1f} $\\\\pm$ {subset['Robust_Acc'].std():.1f}\"\n",
    "            drop = f\"{subset['Acc_Drop'].mean():.1f} $\\\\pm$ {subset['Acc_Drop'].std():.1f}\"\n",
    "            asr = f\"{subset['Attack_Success'].mean():.1f} $\\\\pm$ {subset['Attack_Success'].std():.1f}\"\n",
    "\n",
    "            lines.append(f\"{attack} & {eps} & {clean} & {robust} & {drop} & {asr} \\\\\\\\\")\n",
    "\n",
    "    lines.extend([\n",
    "        \"\\\\bottomrule\",\n",
    "        \"\\\\end{tabular}\",\n",
    "        \"\\\\begin{tablenotes}\",\n",
    "        \"\\\\small\",\n",
    "        \"\\\\item Note: Values are mean $\\\\pm$ std across seeds (42, 123, 456). ASR = Attack Success Rate.\",\n",
    "        \"\\\\end{tablenotes}\",\n",
    "        \"\\\\end{table}\"\n",
    "    ])\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "latex_table = generate_latex_table(df_results, config)\n",
    "\n",
    "# Save LaTeX table\n",
    "latex_path = paths.results_dir / \"adversarial_robustness_table.tex\"\n",
    "with open(latex_path, \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "print(f\"‚úÖ LaTeX table: {latex_path}\")\n",
    "\n",
    "# Print LaTeX table\n",
    "print(\"\\nüìÑ LATEX TABLE FOR DISSERTATION:\")\n",
    "print(\"-\" * 60)\n",
    "print(latex_table)\n",
    "\n",
    "# ============================================================================\n",
    "# LIST ALL SAVED FILES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìÅ ALL SAVED FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if paths.results_dir.exists():\n",
    "    for f in sorted(paths.results_dir.rglob(\"*\")):\n",
    "        if f.is_file():\n",
    "            size_kb = f.stat().st_size / 1024\n",
    "            print(f\"   {f.relative_to(paths.results_dir)} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50fa15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd50fa15",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764473592313,
     "user_tz": 0,
     "elapsed": 229,
     "user": {
      "displayName": "Viraj Jain",
      "userId": "18222087507710878736"
     }
    },
    "outputId": "b4602fe3-f429-4803-b949-cb7e34a8c0fd"
   },
   "outputs": [],
   "source": [
    "#@title üìã Cell 18: Executive Summary & Key Findings\n",
    "#@markdown **Final summary of adversarial robustness evaluation**\n",
    "\n",
    "def print_executive_summary(df_results: pd.DataFrame, clean_results: dict, config: EvaluationConfig):\n",
    "    \"\"\"Generate executive summary of evaluation results.\"\"\"\n",
    "\n",
    "    print(\"‚ïî\" + \"‚ïê\"*68 + \"‚ïó\")\n",
    "    print(\"‚ïë\" + \" \"*20 + \"EXECUTIVE SUMMARY\" + \" \"*31 + \"‚ïë\")\n",
    "    print(\"‚ïë\" + \" \"*10 + \"Phase 4: Adversarial Robustness Evaluation\" + \" \"*15 + \"‚ïë\")\n",
    "    print(\"‚ïö\" + \"‚ïê\"*68 + \"‚ïù\")\n",
    "\n",
    "    # Clean accuracy\n",
    "    mean_clean = np.mean([r[\"accuracy\"] for r in clean_results.values()]) * 100\n",
    "    std_clean = np.std([r[\"accuracy\"] for r in clean_results.values()]) * 100\n",
    "\n",
    "    print(f\"\\nüìä BASELINE CLEAN PERFORMANCE\")\n",
    "    print(f\"   ‚Ä¢ Clean Accuracy: {mean_clean:.2f}% ¬± {std_clean:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Model: ResNet-50 (ImageNet pretrained)\")\n",
    "    print(f\"   ‚Ä¢ Dataset: ISIC 2018 (7 classes)\")\n",
    "    print(f\"   ‚Ä¢ Seeds evaluated: {config.seeds}\")\n",
    "\n",
    "    # Adversarial results summary\n",
    "    print(f\"\\nüõ°Ô∏è ADVERSARIAL ROBUSTNESS FINDINGS\")\n",
    "\n",
    "    # FGSM\n",
    "    if config.fgsm_enabled:\n",
    "        fgsm_8 = df_results[(df_results[\"Attack\"] == \"FGSM\") & (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "        if len(fgsm_8) > 0:\n",
    "            mean_robust = fgsm_8[\"Robust_Acc\"].mean()\n",
    "            mean_drop = fgsm_8[\"Acc_Drop\"].mean()\n",
    "            print(f\"\\n   FGSM (Œµ=8/255):\")\n",
    "            print(f\"   ‚îú‚îÄ Robust Accuracy: {mean_robust:.2f}%\")\n",
    "            print(f\"   ‚îú‚îÄ Accuracy Drop: {mean_drop:.2f}pp\")\n",
    "            print(f\"   ‚îî‚îÄ Interpretation: {'Moderate vulnerability' if mean_drop < 40 else 'High vulnerability'}\")\n",
    "\n",
    "    # PGD\n",
    "    if config.pgd_enabled:\n",
    "        pgd_8 = df_results[(df_results[\"Attack\"] == f\"PGD-{config.pgd_steps}\") & (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "        if len(pgd_8) > 0:\n",
    "            mean_robust = pgd_8[\"Robust_Acc\"].mean()\n",
    "            mean_drop = pgd_8[\"Acc_Drop\"].mean()\n",
    "            print(f\"\\n   PGD-{config.pgd_steps} (Œµ=8/255):\")\n",
    "            print(f\"   ‚îú‚îÄ Robust Accuracy: {mean_robust:.2f}%\")\n",
    "            print(f\"   ‚îú‚îÄ Accuracy Drop: {mean_drop:.2f}pp\")\n",
    "            print(f\"   ‚îî‚îÄ Interpretation: {'Severe vulnerability' if mean_drop > 60 else 'High vulnerability' if mean_drop > 40 else 'Moderate'}\")\n",
    "\n",
    "    # C&W\n",
    "    if config.cw_enabled:\n",
    "        cw = df_results[df_results[\"Attack\"] == \"C&W-L2\"]\n",
    "        if len(cw) > 0:\n",
    "            mean_robust = cw[\"Robust_Acc\"].mean()\n",
    "            mean_drop = cw[\"Acc_Drop\"].mean()\n",
    "            print(f\"\\n   C&W L2:\")\n",
    "            print(f\"   ‚îú‚îÄ Robust Accuracy: {mean_robust:.2f}%\")\n",
    "            print(f\"   ‚îú‚îÄ Accuracy Drop: {mean_drop:.2f}pp\")\n",
    "            print(f\"   ‚îî‚îÄ Interpretation: Strongest attack, minimal perturbation\")\n",
    "\n",
    "    # Key insights\n",
    "    print(f\"\\nüîë KEY INSIGHTS\")\n",
    "    print(f\"   1. Standard CNNs are highly vulnerable to adversarial attacks\")\n",
    "    print(f\"   2. PGD attacks are stronger than FGSM (multi-step > single-step)\")\n",
    "    print(f\"   3. Some classes (e.g., MEL, BCC) may be more vulnerable than others\")\n",
    "    print(f\"   4. Adversarial training is needed for robust medical AI deployment\")\n",
    "\n",
    "    # Research implications\n",
    "    print(f\"\\nüìö RESEARCH IMPLICATIONS\")\n",
    "    print(f\"   ‚Ä¢ These results motivate Phase 5: Adversarial Training\")\n",
    "    print(f\"   ‚Ä¢ Tri-objective optimization will balance accuracy, robustness, and explainability\")\n",
    "    print(f\"   ‚Ä¢ Medical AI systems require robustness validation before clinical use\")\n",
    "\n",
    "    # Files generated\n",
    "    print(f\"\\nüìÅ GENERATED OUTPUTS\")\n",
    "    print(f\"   ‚Ä¢ Results CSV: adversarial_robustness_results.csv\")\n",
    "    print(f\"   ‚Ä¢ Summary CSV: adversarial_robustness_summary.csv\")\n",
    "    print(f\"   ‚Ä¢ JSON results: adversarial_robustness_results.json\")\n",
    "    print(f\"   ‚Ä¢ LaTeX table: adversarial_robustness_table.tex\")\n",
    "    print(f\"   ‚Ä¢ Figures: robustness_curves.png, vulnerability_heatmap.png, etc.\")\n",
    "\n",
    "    print(\"\\n\" + \"‚ïê\"*70)\n",
    "    print(f\"‚úÖ Phase 4 Adversarial Robustness Evaluation COMPLETE\")\n",
    "    print(f\"‚è±Ô∏è  Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"‚ïê\"*70)\n",
    "\n",
    "# Print executive summary\n",
    "print_executive_summary(df_results, clean_results, config)\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL MEMORY CLEANUP\n",
    "# ============================================================================\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nüßπ Memory cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}