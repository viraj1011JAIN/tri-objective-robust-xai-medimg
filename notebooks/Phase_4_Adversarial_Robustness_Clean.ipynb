{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca3c4bc",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è Phase 4: Adversarial Robustness Evaluation\n",
    "\n",
    "## Tri-Objective Robust XAI for Medical Imaging\n",
    "\n",
    "**Author:** Viraj Pankaj Jain  \n",
    "**Institution:** University of Glasgow  \n",
    "**Date:** November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook evaluates the **adversarial robustness** of baseline ResNet-50 models trained on ISIC 2018 dermoscopy images. We systematically assess vulnerability to:\n",
    "\n",
    "| Attack | Type | Strength | Use Case |\n",
    "|--------|------|----------|----------|\n",
    "| **FGSM** | Gradient-based | Fast, single-step | Real-time threat assessment |\n",
    "| **PGD** | Iterative | Strong, multi-step | Reliable robustness benchmark |\n",
    "| **C&W** | Optimization | Strongest, minimal perturbation | Worst-case security analysis |\n",
    "\n",
    "## üéØ Research Questions Addressed\n",
    "\n",
    "- **RQ1:** How vulnerable are standard CNNs to adversarial attacks in medical imaging?\n",
    "- **RQ2:** How does attack strength (Œµ) affect model accuracy degradation?\n",
    "- **RQ3:** Which skin lesion classes are most vulnerable to adversarial perturbations?\n",
    "\n",
    "## üìä Evaluation Protocol\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  ADVERSARIAL EVALUATION PIPELINE                                ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  1. Load trained baseline models (Seeds: 42, 123, 456)          ‚îÇ\n",
    "‚îÇ  2. Evaluate clean accuracy (sanity check)                      ‚îÇ\n",
    "‚îÇ  3. Generate adversarial examples at Œµ ‚àà {2/255, 4/255, 8/255}  ‚îÇ\n",
    "‚îÇ  4. Measure robust accuracy under each attack                   ‚îÇ\n",
    "‚îÇ  5. Analyze per-class vulnerability                             ‚îÇ\n",
    "‚îÇ  6. Visualize perturbations and decision boundaries             ‚îÇ\n",
    "‚îÇ  7. Statistical analysis across seeds                           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## ‚ö° Hardware Requirements\n",
    "\n",
    "- **Recommended:** NVIDIA A100 (40GB) - Full evaluation ~15 minutes\n",
    "- **Minimum:** NVIDIA T4 (16GB) - Full evaluation ~45 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 4: ADVERSARIAL ROBUSTNESS EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Mount Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Google Colab detected, Drive mounted\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚úÖ Local environment detected\")\n",
    "\n",
    "# Clone/update repository\n",
    "if IN_COLAB:\n",
    "    REPO_PATH = Path('/content/tri-objective-robust-xai-medimg')\n",
    "    if not REPO_PATH.exists():\n",
    "        !git clone https://github.com/viraj1011JAIN/tri-objective-robust-xai-medimg.git {REPO_PATH}\n",
    "        print(\"‚úÖ Repository cloned\")\n",
    "    else:\n",
    "        os.chdir(REPO_PATH)\n",
    "        !git pull origin main\n",
    "        print(\"‚úÖ Repository updated\")\n",
    "\n",
    "    os.chdir(REPO_PATH)\n",
    "    sys.path.insert(0, str(REPO_PATH))\n",
    "    PROJECT_ROOT = REPO_PATH\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"üìÅ Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: INSTALL DEPENDENCIES\n",
    "# ============================================================================\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q timm albumentations scikit-learn pandas matplotlib seaborn tqdm mlflow\n",
    "!pip install -q plotly kaleido scipy statsmodels\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a790d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: IMPORTS\n",
    "# ============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Albumentations for transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score\n",
    ")\n",
    "from scipy import stats\n",
    "\n",
    "# Project imports - Attacks\n",
    "from src.attacks.fgsm import FGSM, FGSMConfig\n",
    "from src.attacks.pgd import PGD, PGDConfig\n",
    "from src.attacks.cw import CarliniWagner, CWConfig\n",
    "\n",
    "# Project imports - Data & Model\n",
    "from src.datasets.isic import ISICDataset\n",
    "from src.models.build import build_model\n",
    "from src.utils.reproducibility import set_global_seed\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    # Enable TF32 for A100\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: CONFIGURATION\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths (Google Drive)\n",
    "    'data_root': Path('/content/drive/MyDrive/data/data/isic_2018'),\n",
    "    'checkpoint_dir': Path('/content/drive/MyDrive/checkpoints/baseline'),\n",
    "    'results_dir': Path('/content/drive/MyDrive/results/phase4'),\n",
    "\n",
    "    # Model\n",
    "    'model_name': 'resnet50',\n",
    "    'num_classes': 7,\n",
    "\n",
    "    # Evaluation settings\n",
    "    'batch_size': 64,\n",
    "    'num_workers': 4,\n",
    "    'image_size': 224,\n",
    "\n",
    "    # Seeds to evaluate\n",
    "    'seeds': [42, 123, 456],\n",
    "\n",
    "    # Attack configurations\n",
    "    'epsilons': [2/255, 4/255, 8/255],\n",
    "    'pgd_steps': 40,\n",
    "    'cw_iterations': 100,\n",
    "\n",
    "    # Class names\n",
    "    'class_names': ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'],\n",
    "}\n",
    "\n",
    "# ImageNet normalization\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Create output directories\n",
    "CONFIG['results_dir'].mkdir(parents=True, exist_ok=True)\n",
    "(CONFIG['results_dir'] / 'figures').mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìä Model: {CONFIG['model_name']}\")\n",
    "print(f\"üìä Seeds: {CONFIG['seeds']}\")\n",
    "print(f\"üìä Epsilons: {[f'{int(e*255)}/255' for e in CONFIG['epsilons']]}\")\n",
    "print(f\"üìä Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"üìÅ Data: {CONFIG['data_root']}\")\n",
    "print(f\"üìÅ Checkpoints: {CONFIG['checkpoint_dir']}\")\n",
    "print(f\"üìÅ Results: {CONFIG['results_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: DATA PREPARATION\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load and fix metadata\n",
    "metadata_path = CONFIG['data_root'] / 'metadata.csv'\n",
    "print(f\"üìÑ Loading metadata: {metadata_path}\")\n",
    "\n",
    "df = pd.read_csv(metadata_path)\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "\n",
    "# Fix path separators\n",
    "if 'image_path' in df.columns:\n",
    "    df['image_path'] = df['image_path'].str.replace('\\\\', '/', regex=False)\n",
    "    print(\"   ‚úÖ Fixed path separators\")\n",
    "\n",
    "# Save fixed metadata\n",
    "fixed_path = CONFIG['data_root'] / 'metadata_fixed.csv'\n",
    "df.to_csv(fixed_path, index=False)\n",
    "\n",
    "# Show test split info\n",
    "test_df = df[df['split'] == 'test']\n",
    "print(f\"\\nüìä Test samples: {len(test_df)}\")\n",
    "print(f\"üìä Test class distribution:\")\n",
    "print(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: CREATE TEST DATASET\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"CREATING TEST DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test transforms - NO normalization (attacks need [0,1] range)\n",
    "test_transforms = A.Compose([\n",
    "    A.Resize(CONFIG['image_size'], CONFIG['image_size']),\n",
    "    A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),  # Keep in [0, 1]\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = ISICDataset(\n",
    "    root=str(CONFIG['data_root']),\n",
    "    split='test',\n",
    "    transforms=test_transforms,\n",
    "    csv_path=str(fixed_path),\n",
    "    image_column='image_path',\n",
    "    label_column='label'\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Test samples: {len(test_dataset)}\")\n",
    "print(f\"‚úÖ Batches: {len(test_loader)}\")\n",
    "print(f\"‚úÖ Classes: {CONFIG['class_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd9df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_normalizer(device):\n",
    "    \"\"\"Create ImageNet normalization function.\"\"\"\n",
    "    mean = torch.tensor(IMAGENET_MEAN).view(1, 3, 1, 1).to(device)\n",
    "    std = torch.tensor(IMAGENET_STD).view(1, 3, 1, 1).to(device)\n",
    "    \n",
    "    def normalize(x):\n",
    "        return (x - mean) / std\n",
    "    return normalize\n",
    "\n",
    "def evaluate_clean(model, dataloader, device, normalize_fn):\n",
    "    \"\"\"Evaluate model on clean data.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Clean Eval', leave=False):\n",
    "            if len(batch) == 3:\n",
    "                images, labels, _ = batch\n",
    "            else:\n",
    "                images, labels = batch\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Normalize and predict\n",
    "            outputs = model(normalize_fn(images))\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds) * 100,\n",
    "        'balanced_accuracy': balanced_accuracy_score(all_labels, all_preds) * 100,\n",
    "        'f1_macro': f1_score(all_labels, all_preds, average='macro') * 100,\n",
    "        'auroc': roc_auc_score(all_labels, all_probs, multi_class='ovr') * 100,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'probs': all_probs\n",
    "    }\n",
    "\n",
    "def evaluate_attack(model, dataloader, device, normalize_fn, attack_fn, desc='Attack'):\n",
    "    \"\"\"Evaluate model under adversarial attack.\"\"\"\n",
    "    model.eval()\n",
    "    all_clean_preds, all_adv_preds, all_labels = [], [], []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=desc, leave=False):\n",
    "        if len(batch) == 3:\n",
    "            images, labels, _ = batch\n",
    "        else:\n",
    "            images, labels = batch\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Clean predictions\n",
    "        with torch.no_grad():\n",
    "            clean_preds = model(normalize_fn(images)).argmax(dim=1)\n",
    "        \n",
    "        # Generate adversarial examples\n",
    "        x_adv = attack_fn(images, labels)\n",
    "        \n",
    "        # Adversarial predictions\n",
    "        with torch.no_grad():\n",
    "            adv_preds = model(normalize_fn(x_adv)).argmax(dim=1)\n",
    "        \n",
    "        all_clean_preds.extend(clean_preds.cpu().numpy())\n",
    "        all_adv_preds.extend(adv_preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_clean_preds = np.array(all_clean_preds)\n",
    "    all_adv_preds = np.array(all_adv_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    clean_acc = accuracy_score(all_labels, all_clean_preds) * 100\n",
    "    robust_acc = accuracy_score(all_labels, all_adv_preds) * 100\n",
    "    \n",
    "    # Per-class robust accuracy\n",
    "    cm = confusion_matrix(all_labels, all_adv_preds)\n",
    "    per_class_acc = (cm.diagonal() / cm.sum(axis=1)) * 100\n",
    "    \n",
    "    return {\n",
    "        'clean_accuracy': clean_acc,\n",
    "        'robust_accuracy': robust_acc,\n",
    "        'accuracy_drop': clean_acc - robust_acc,\n",
    "        'per_class_robust_acc': dict(zip(CONFIG['class_names'], per_class_acc)),\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3148d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: LOAD MODELS AND VERIFY CLEAN ACCURACY\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MODELS & VERIFYING CLEAN ACCURACY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "normalize = get_normalizer(device)\n",
    "models = {}\n",
    "clean_results = {}\n",
    "\n",
    "for seed in CONFIG['seeds']:\n",
    "    print(f\"\\nüì• Loading seed {seed}...\")\n",
    "    \n",
    "    # Find checkpoint\n",
    "    checkpoint_path = CONFIG['checkpoint_dir'] / f'seed_{seed}' / 'best.pt'\n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"   ‚ùå Checkpoint not found: {checkpoint_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Load model\n",
    "    model = build_model(\n",
    "        architecture=CONFIG['model_name'],\n",
    "        num_classes=CONFIG['num_classes'],\n",
    "        pretrained=False\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    models[seed] = model\n",
    "    \n",
    "    # Verify clean accuracy\n",
    "    result = evaluate_clean(model, test_loader, device, normalize)\n",
    "    clean_results[seed] = result\n",
    "    \n",
    "    print(f\"   ‚úÖ Clean Accuracy: {result['accuracy']:.2f}%\")\n",
    "    print(f\"   ‚úÖ AUROC: {result['auroc']:.2f}%\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìä CLEAN ACCURACY SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "accs = [clean_results[s]['accuracy'] for s in clean_results]\n",
    "print(f\"Mean: {np.mean(accs):.2f}% ¬± {np.std(accs):.2f}%\")\n",
    "for seed in clean_results:\n",
    "    print(f\"   Seed {seed}: {clean_results[seed]['accuracy']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6691f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: RUN ADVERSARIAL EVALUATION - ALL ATTACKS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"ADVERSARIAL EVALUATION - ALL ATTACKS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"‚è±Ô∏è  Start time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for seed in CONFIG['seeds']:\n",
    "    if seed not in models:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SEED {seed}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    model = models[seed]\n",
    "    seed_results = {'clean': clean_results[seed]}\n",
    "    \n",
    "    # ==================== FGSM ====================\n",
    "    print(\"\\nüî• FGSM Attacks:\")\n",
    "    for eps in CONFIG['epsilons']:\n",
    "        eps_str = f\"{int(eps*255)}/255\"\n",
    "        \n",
    "        # Create FGSM attack\n",
    "        fgsm_config = FGSMConfig(\n",
    "            epsilon=eps,\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False\n",
    "        )\n",
    "        fgsm = FGSM(fgsm_config)\n",
    "        \n",
    "        def fgsm_attack_fn(x, y):\n",
    "            return fgsm.generate(model, x, y, loss_fn=nn.CrossEntropyLoss(), normalize=normalize)\n",
    "        \n",
    "        result = evaluate_attack(model, test_loader, device, normalize, fgsm_attack_fn, f\"FGSM Œµ={eps_str}\")\n",
    "        seed_results[f'fgsm_{eps_str}'] = result\n",
    "        print(f\"   Œµ={eps_str}: {result['robust_accuracy']:.2f}% (drop: {result['accuracy_drop']:.2f}%)\")\n",
    "    \n",
    "    # ==================== PGD ====================\n",
    "    print(\"\\nüî• PGD Attacks:\")\n",
    "    for eps in CONFIG['epsilons']:\n",
    "        eps_str = f\"{int(eps*255)}/255\"\n",
    "        step_size = eps / 4\n",
    "        \n",
    "        # Create PGD attack\n",
    "        pgd_config = PGDConfig(\n",
    "            epsilon=eps,\n",
    "            num_steps=CONFIG['pgd_steps'],\n",
    "            step_size=step_size,\n",
    "            random_start=True,\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False\n",
    "        )\n",
    "        pgd = PGD(pgd_config)\n",
    "        \n",
    "        def pgd_attack_fn(x, y):\n",
    "            return pgd.generate(model, x, y, loss_fn=nn.CrossEntropyLoss(), normalize=normalize)\n",
    "        \n",
    "        result = evaluate_attack(model, test_loader, device, normalize, pgd_attack_fn, f\"PGD Œµ={eps_str}\")\n",
    "        seed_results[f'pgd_{eps_str}'] = result\n",
    "        print(f\"   Œµ={eps_str}: {result['robust_accuracy']:.2f}% (drop: {result['accuracy_drop']:.2f}%)\")\n",
    "    \n",
    "    # ==================== C&W ====================\n",
    "    print(\"\\nüî• Carlini-Wagner Attack:\")\n",
    "    cw_config = CWConfig(\n",
    "        confidence=0.0,\n",
    "        learning_rate=0.01,\n",
    "        max_iterations=CONFIG['cw_iterations'],\n",
    "        binary_search_steps=5,\n",
    "        clip_min=0.0,\n",
    "        clip_max=1.0,\n",
    "        targeted=False\n",
    "    )\n",
    "    cw = CarliniWagner(cw_config)\n",
    "    \n",
    "    def cw_attack_fn(x, y):\n",
    "        return cw.generate(model, x, y, normalize=normalize)\n",
    "    \n",
    "    result = evaluate_attack(model, test_loader, device, normalize, cw_attack_fn, \"C&W L2\")\n",
    "    seed_results['cw'] = result\n",
    "    print(f\"   C&W: {result['robust_accuracy']:.2f}% (drop: {result['accuracy_drop']:.2f}%)\")\n",
    "    \n",
    "    all_results[seed] = seed_results\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  End time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(\"‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: AGGREGATE RESULTS TABLE\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"AGGREGATED RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build results table\n",
    "rows = []\n",
    "attacks = ['clean'] + [f'fgsm_{int(e*255)}/255' for e in CONFIG['epsilons']] + \\\n",
    "          [f'pgd_{int(e*255)}/255' for e in CONFIG['epsilons']] + ['cw']\n",
    "\n",
    "for attack in attacks:\n",
    "    if attack == 'clean':\n",
    "        accs = [all_results[s]['clean']['accuracy'] for s in all_results]\n",
    "        attack_name = 'Clean (No Attack)'\n",
    "    elif attack == 'cw':\n",
    "        accs = [all_results[s]['cw']['robust_accuracy'] for s in all_results]\n",
    "        attack_name = 'C&W L2'\n",
    "    else:\n",
    "        accs = [all_results[s][attack]['robust_accuracy'] for s in all_results]\n",
    "        attack_name = attack.upper().replace('_', ' Œµ=')\n",
    "    \n",
    "    rows.append({\n",
    "        'Attack': attack_name,\n",
    "        'Mean Accuracy (%)': f\"{np.mean(accs):.2f}\",\n",
    "        'Std (%)': f\"{np.std(accs):.2f}\",\n",
    "        'Seed 42': f\"{accs[0]:.2f}\" if len(accs) > 0 else 'N/A',\n",
    "        'Seed 123': f\"{accs[1]:.2f}\" if len(accs) > 1 else 'N/A',\n",
    "        'Seed 456': f\"{accs[2]:.2f}\" if len(accs) > 2 else 'N/A',\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(CONFIG['results_dir'] / 'adversarial_results.csv', index=False)\n",
    "print(f\"\\n‚úÖ Results saved to: {CONFIG['results_dir'] / 'adversarial_results.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: VISUALIZATION - ROBUSTNESS DEGRADATION CURVES\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"VISUALIZATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Prepare data for plotting\n",
    "eps_vals = [0] + [e*255 for e in CONFIG['epsilons']]  # 0, 2, 4, 8\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "\n",
    "# Plot FGSM\n",
    "ax = axes[0]\n",
    "for i, seed in enumerate(CONFIG['seeds']):\n",
    "    if seed not in all_results:\n",
    "        continue\n",
    "    accs = [all_results[seed]['clean']['accuracy']]\n",
    "    for eps in CONFIG['epsilons']:\n",
    "        accs.append(all_results[seed][f'fgsm_{int(eps*255)}/255']['robust_accuracy'])\n",
    "    ax.plot(eps_vals, accs, 'o-', color=colors[i], label=f'Seed {seed}', linewidth=2, markersize=8)\n",
    "\n",
    "ax.set_xlabel('Perturbation Œµ (√ó255)', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('FGSM Attack Robustness', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(eps_vals)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# Plot PGD\n",
    "ax = axes[1]\n",
    "for i, seed in enumerate(CONFIG['seeds']):\n",
    "    if seed not in all_results:\n",
    "        continue\n",
    "    accs = [all_results[seed]['clean']['accuracy']]\n",
    "    for eps in CONFIG['epsilons']:\n",
    "        accs.append(all_results[seed][f'pgd_{int(eps*255)}/255']['robust_accuracy'])\n",
    "    ax.plot(eps_vals, accs, 'o-', color=colors[i], label=f'Seed {seed}', linewidth=2, markersize=8)\n",
    "\n",
    "ax.set_xlabel('Perturbation Œµ (√ó255)', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('PGD Attack Robustness (40 steps)', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(eps_vals)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "plt.suptitle('Adversarial Robustness: Baseline ResNet-50 on ISIC 2018', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['results_dir'] / 'figures' / 'robustness_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: robustness_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: PER-CLASS VULNERABILITY HEATMAP\n",
    "# ============================================================================\n",
    "\n",
    "# Aggregate per-class accuracy across seeds for PGD Œµ=8/255\n",
    "class_accs = {}\n",
    "for cls in CONFIG['class_names']:\n",
    "    class_accs[cls] = []\n",
    "    for seed in all_results:\n",
    "        class_accs[cls].append(all_results[seed]['pgd_8/255']['per_class_robust_acc'][cls])\n",
    "\n",
    "# Create heatmap data\n",
    "heatmap_data = []\n",
    "for attack_key in ['fgsm_8/255', 'pgd_8/255', 'cw']:\n",
    "    row = []\n",
    "    for cls in CONFIG['class_names']:\n",
    "        accs = [all_results[s][attack_key]['per_class_robust_acc'][cls] for s in all_results]\n",
    "        row.append(np.mean(accs))\n",
    "    heatmap_data.append(row)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "im = ax.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=100)\n",
    "\n",
    "ax.set_xticks(range(len(CONFIG['class_names'])))\n",
    "ax.set_xticklabels(CONFIG['class_names'], fontsize=11)\n",
    "ax.set_yticks(range(3))\n",
    "ax.set_yticklabels(['FGSM Œµ=8/255', 'PGD Œµ=8/255', 'C&W L2'], fontsize=11)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(3):\n",
    "    for j in range(len(CONFIG['class_names'])):\n",
    "        text = ax.text(j, i, f\"{heatmap_data[i][j]:.1f}%\",\n",
    "                       ha='center', va='center', color='black', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_title('Per-Class Robust Accuracy Under Strong Attacks\\n(Mean across 3 seeds)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, label='Accuracy (%)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['results_dir'] / 'figures' / 'class_vulnerability_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: class_vulnerability_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fadd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: ATTACK COMPARISON BAR CHART\n",
    "# ============================================================================\n",
    "\n",
    "# Prepare data\n",
    "attack_names = ['Clean', 'FGSM\\nŒµ=2/255', 'FGSM\\nŒµ=4/255', 'FGSM\\nŒµ=8/255',\n",
    "                'PGD\\nŒµ=2/255', 'PGD\\nŒµ=4/255', 'PGD\\nŒµ=8/255', 'C&W']\n",
    "attack_keys = ['clean', 'fgsm_2/255', 'fgsm_4/255', 'fgsm_8/255',\n",
    "               'pgd_2/255', 'pgd_4/255', 'pgd_8/255', 'cw']\n",
    "\n",
    "# Calculate mean and std\n",
    "means = []\n",
    "stds = []\n",
    "for key in attack_keys:\n",
    "    if key == 'clean':\n",
    "        accs = [all_results[s]['clean']['accuracy'] for s in all_results]\n",
    "    else:\n",
    "        accs = [all_results[s][key]['robust_accuracy'] for s in all_results]\n",
    "    means.append(np.mean(accs))\n",
    "    stds.append(np.std(accs))\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = ['#2ecc71'] + ['#3498db']*3 + ['#e74c3c']*3 + ['#9b59b6']\n",
    "bars = ax.bar(attack_names, means, yerr=stds, capsize=5, color=colors, edgecolor='black', alpha=0.8)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean in zip(bars, means):\n",
    "    ax.annotate(f'{mean:.1f}%', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 5), textcoords='offset points', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Baseline Model Robustness Comparison\\n(Mean ¬± Std across 3 seeds)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.axhline(y=means[0], color='gray', linestyle='--', alpha=0.5, label='Clean Accuracy')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['results_dir'] / 'figures' / 'attack_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: attack_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 14: SAVE ALL RESULTS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Prepare JSON-serializable results\n",
    "export_results = {}\n",
    "for seed in all_results:\n",
    "    export_results[str(seed)] = {}\n",
    "    for attack_key, attack_result in all_results[seed].items():\n",
    "        export_results[str(seed)][attack_key] = {\n",
    "            k: v.tolist() if isinstance(v, np.ndarray) else v\n",
    "            for k, v in attack_result.items()\n",
    "            if k not in ['predictions', 'labels', 'probs', 'confusion_matrix']\n",
    "        }\n",
    "\n",
    "# Save to JSON\n",
    "results_file = CONFIG['results_dir'] / 'adversarial_results.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(export_results, f, indent=2)\n",
    "print(f\"‚úÖ Results saved to: {results_file}\")\n",
    "\n",
    "# List all saved files\n",
    "print(f\"\\nüìÅ Saved files in {CONFIG['results_dir']}:\")\n",
    "for f in sorted(CONFIG['results_dir'].glob('**/*')):\n",
    "    if f.is_file():\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"   üìÑ {f.name} ({size_kb:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 15: FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 4 COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate key metrics\n",
    "clean_mean = np.mean([all_results[s]['clean']['accuracy'] for s in all_results])\n",
    "fgsm8_mean = np.mean([all_results[s]['fgsm_8/255']['robust_accuracy'] for s in all_results])\n",
    "pgd8_mean = np.mean([all_results[s]['pgd_8/255']['robust_accuracy'] for s in all_results])\n",
    "cw_mean = np.mean([all_results[s]['cw']['robust_accuracy'] for s in all_results])\n",
    "\n",
    "print(f\"\\nüìä BASELINE MODEL ROBUSTNESS (Mean across seeds):\")\n",
    "print(f\"-\" * 50)\n",
    "print(f\"   Clean Accuracy:       {clean_mean:.2f}%\")\n",
    "print(f\"   FGSM (Œµ=8/255):       {fgsm8_mean:.2f}%  (drop: {clean_mean - fgsm8_mean:.2f}%)\")\n",
    "print(f\"   PGD-40 (Œµ=8/255):     {pgd8_mean:.2f}%  (drop: {clean_mean - pgd8_mean:.2f}%)\")\n",
    "print(f\"   C&W L2:               {cw_mean:.2f}%  (drop: {clean_mean - cw_mean:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüîë KEY FINDINGS:\")\n",
    "print(f\"-\" * 50)\n",
    "print(f\"   ‚Ä¢ Standard CNNs are highly vulnerable to adversarial attacks\")\n",
    "print(f\"   ‚Ä¢ PGD causes {clean_mean - pgd8_mean:.1f}% accuracy drop at Œµ=8/255\")\n",
    "print(f\"   ‚Ä¢ Adversarial training (Phase 5) is needed for robustness\")\n",
    "\n",
    "print(f\"\\nüéØ NEXT STEPS:\")\n",
    "print(f\"-\" * 50)\n",
    "print(f\"   1. Run Phase 5: Tri-objective robust training\")\n",
    "print(f\"   2. Run Phase 6: Explainability analysis (Grad-CAM, SHAP)\")\n",
    "print(f\"   3. Compare baseline vs robust models\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ PHASE 4 ADVERSARIAL EVALUATION COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üîß Cell 2: Environment Setup & Dependencies\n",
    "#@markdown **Run this cell first to install all required packages**\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages for adversarial evaluation.\"\"\"\n",
    "    packages = [\n",
    "        \"torch>=2.0.0\",\n",
    "        \"torchvision>=0.15.0\",\n",
    "        \"timm>=0.9.0\",\n",
    "        \"albumentations>=1.3.0\",\n",
    "        \"scikit-learn>=1.3.0\",\n",
    "        \"pandas>=2.0.0\",\n",
    "        \"numpy>=1.24.0\",\n",
    "        \"matplotlib>=3.7.0\",\n",
    "        \"seaborn>=0.12.0\",\n",
    "        \"plotly>=5.15.0\",\n",
    "        \"kaleido\",  # For plotly static export\n",
    "        \"tqdm>=4.65.0\",\n",
    "        \"mlflow>=2.5.0\",\n",
    "        \"scipy>=1.11.0\",\n",
    "    ]\n",
    "    \n",
    "    print(\"üì¶ Installing packages...\")\n",
    "    for pkg in packages:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "    print(\"‚úÖ All packages installed!\")\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üåê Running in Google Colab\")\n",
    "    install_packages()\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Running locally\")\n",
    "\n",
    "# Core imports\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 8),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.dpi': 100,\n",
    "})\n",
    "\n",
    "# GPU Configuration\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üñ•Ô∏è  HARDWARE CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úÖ GPU: {gpu_name}\")\n",
    "    print(f\"‚úÖ VRAM: {gpu_mem:.1f} GB\")\n",
    "    \n",
    "    # Enable optimizations for A100/Ampere GPUs\n",
    "    if \"A100\" in gpu_name or torch.cuda.get_device_capability()[0] >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"‚úÖ TF32 enabled for Ampere GPU\")\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"‚úÖ cuDNN benchmark enabled\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è No GPU found, using CPU (will be slow)\")\n",
    "\n",
    "print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úÖ Device: {device}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üóÇÔ∏è Cell 3: Mount Google Drive & Configure Paths\n",
    "#@markdown **Configure data and checkpoint paths**\n",
    "\n",
    "# Mount Google Drive (Colab only)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    print(\"‚úÖ Google Drive mounted\")\n",
    "\n",
    "# ============================================================================\n",
    "# PATH CONFIGURATION - Adjust these paths as needed\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class PathConfig:\n",
    "    \"\"\"Central path configuration for the evaluation pipeline.\"\"\"\n",
    "    \n",
    "    # Base paths\n",
    "    drive_base: Path = Path(\"/content/drive/MyDrive\")\n",
    "    \n",
    "    # Data paths\n",
    "    data_root: Path = field(default=None)\n",
    "    train_dir: Path = field(default=None)\n",
    "    val_dir: Path = field(default=None)\n",
    "    test_dir: Path = field(default=None)\n",
    "    \n",
    "    # Checkpoint paths\n",
    "    checkpoint_dir: Path = field(default=None)\n",
    "    \n",
    "    # Output paths\n",
    "    results_dir: Path = field(default=None)\n",
    "    figures_dir: Path = field(default=None)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize derived paths.\"\"\"\n",
    "        if self.data_root is None:\n",
    "            self.data_root = self.drive_base / \"data\" / \"data\" / \"isic_2018\"\n",
    "        if self.train_dir is None:\n",
    "            self.train_dir = self.data_root / \"train\"\n",
    "        if self.val_dir is None:\n",
    "            self.val_dir = self.data_root / \"val\"\n",
    "        if self.test_dir is None:\n",
    "            self.test_dir = self.data_root / \"test\"\n",
    "        if self.checkpoint_dir is None:\n",
    "            self.checkpoint_dir = self.drive_base / \"checkpoints\" / \"baseline\"\n",
    "        if self.results_dir is None:\n",
    "            self.results_dir = self.drive_base / \"results\" / \"phase4_adversarial\"\n",
    "        if self.figures_dir is None:\n",
    "            self.figures_dir = self.results_dir / \"figures\"\n",
    "    \n",
    "    def validate(self) -> bool:\n",
    "        \"\"\"Validate that required paths exist.\"\"\"\n",
    "        required = [self.data_root, self.test_dir, self.checkpoint_dir]\n",
    "        missing = [p for p in required if not p.exists()]\n",
    "        \n",
    "        if missing:\n",
    "            print(\"‚ùå Missing paths:\")\n",
    "            for p in missing:\n",
    "                print(f\"   - {p}\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def create_output_dirs(self):\n",
    "        \"\"\"Create output directories if they don't exist.\"\"\"\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"‚úÖ Results directory: {self.results_dir}\")\n",
    "        print(f\"‚úÖ Figures directory: {self.figures_dir}\")\n",
    "\n",
    "# Initialize paths\n",
    "paths = PathConfig()\n",
    "\n",
    "# Validate paths\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìÅ PATH VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if paths.validate():\n",
    "    print(f\"‚úÖ Data root: {paths.data_root}\")\n",
    "    print(f\"‚úÖ Test directory: {paths.test_dir}\")\n",
    "    print(f\"‚úÖ Checkpoint directory: {paths.checkpoint_dir}\")\n",
    "    paths.create_output_dirs()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Please update PathConfig with correct paths!\")\n",
    "    \n",
    "# List available checkpoints\n",
    "print(\"\\nüì¶ Available Checkpoints:\")\n",
    "if paths.checkpoint_dir.exists():\n",
    "    checkpoints = list(paths.checkpoint_dir.glob(\"*.pt\"))\n",
    "    for ckpt in sorted(checkpoints):\n",
    "        size_mb = ckpt.stat().st_size / 1e6\n",
    "        print(f\"   - {ckpt.name} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è No checkpoints found!\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e498f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üì• Cell 4: Clone Repository & Import Attack Classes\n",
    "#@markdown **Clone the project repository and import custom modules**\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Clone repository (Colab only)\n",
    "REPO_URL = \"https://github.com/viraj1011JAIN/tri-objective-robust-xai-medimg.git\"\n",
    "REPO_DIR = \"/content/tri-objective-robust-xai-medimg\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists(REPO_DIR):\n",
    "        print(f\"üì• Cloning repository...\")\n",
    "        os.system(f\"git clone {REPO_URL} {REPO_DIR}\")\n",
    "        print(\"‚úÖ Repository cloned!\")\n",
    "    else:\n",
    "        print(\"üìÅ Repository already exists, pulling latest...\")\n",
    "        os.system(f\"cd {REPO_DIR} && git pull\")\n",
    "    \n",
    "    # Add to Python path\n",
    "    if REPO_DIR not in sys.path:\n",
    "        sys.path.insert(0, REPO_DIR)\n",
    "    print(f\"‚úÖ Added {REPO_DIR} to Python path\")\n",
    "else:\n",
    "    # Local development - find project root\n",
    "    current_dir = Path.cwd()\n",
    "    if \"notebooks\" in str(current_dir):\n",
    "        project_root = current_dir.parent\n",
    "    else:\n",
    "        project_root = current_dir\n",
    "    \n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "    print(f\"‚úÖ Using local project: {project_root}\")\n",
    "\n",
    "# Import custom attack classes\n",
    "print(\"\\nüîß Importing attack modules...\")\n",
    "\n",
    "try:\n",
    "    from src.attacks.fgsm import FGSM, FGSMConfig, fgsm_attack\n",
    "    from src.attacks.pgd import PGD, PGDConfig, pgd_attack\n",
    "    from src.attacks.cw import CarliniWagner, CWConfig, cw_attack\n",
    "    from src.attacks.base import AttackConfig, AttackResult\n",
    "    print(\"‚úÖ FGSM attack imported\")\n",
    "    print(\"‚úÖ PGD attack imported\")\n",
    "    print(\"‚úÖ Carlini-Wagner attack imported\")\n",
    "    print(\"‚úÖ Base attack classes imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"‚ö†Ô∏è Please ensure the repository is properly cloned\")\n",
    "    raise\n",
    "\n",
    "# Import dataset utilities\n",
    "try:\n",
    "    from src.datasets.isic import ISICDataset\n",
    "    print(\"‚úÖ ISICDataset imported\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è ISICDataset not found, will use custom implementation\")\n",
    "    ISICDataset = None\n",
    "\n",
    "# Import model utilities\n",
    "import timm\n",
    "print(f\"‚úÖ timm version: {timm.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ALL MODULES IMPORTED SUCCESSFULLY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf885ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üìä Cell 5: Dataset & Model Loading Utilities\n",
    "#@markdown **Define dataset wrapper and model loading functions**\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ============================================================================\n",
    "# ISIC 2018 CLASS INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"AKIEC\",  # Actinic Keratoses\n",
    "    \"BCC\",    # Basal Cell Carcinoma\n",
    "    \"BKL\",    # Benign Keratosis\n",
    "    \"DF\",     # Dermatofibroma\n",
    "    \"MEL\",    # Melanoma\n",
    "    \"NV\",     # Melanocytic Nevi\n",
    "    \"VASC\"    # Vascular Lesions\n",
    "]\n",
    "\n",
    "CLASS_DESCRIPTIONS = {\n",
    "    \"AKIEC\": \"Actinic Keratoses (pre-cancerous)\",\n",
    "    \"BCC\": \"Basal Cell Carcinoma (cancerous)\",\n",
    "    \"BKL\": \"Benign Keratosis (non-cancerous)\",\n",
    "    \"DF\": \"Dermatofibroma (benign)\",\n",
    "    \"MEL\": \"Melanoma (malignant, dangerous)\",\n",
    "    \"NV\": \"Melanocytic Nevi (common moles)\",\n",
    "    \"VASC\": \"Vascular Lesions (blood vessel)\"\n",
    "}\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# ImageNet normalization (used by pretrained models)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class ISICTestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ISIC 2018 Test Dataset for adversarial evaluation.\n",
    "    \n",
    "    Returns unnormalized images in [0, 1] range for adversarial attacks.\n",
    "    Normalization is applied separately during model inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: Path,\n",
    "        transform: Optional[A.Compose] = None,\n",
    "        max_samples: Optional[int] = None\n",
    "    ):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform or self._default_transform()\n",
    "        \n",
    "        # Collect samples\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {name: idx for idx, name in enumerate(CLASS_NAMES)}\n",
    "        \n",
    "        for class_name in CLASS_NAMES:\n",
    "            class_dir = self.root_dir / class_name\n",
    "            if class_dir.exists():\n",
    "                images = list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\"))\n",
    "                for img_path in images:\n",
    "                    self.samples.append((img_path, self.class_to_idx[class_name]))\n",
    "        \n",
    "        # Limit samples if specified\n",
    "        if max_samples and len(self.samples) > max_samples:\n",
    "            # Stratified sampling\n",
    "            from collections import defaultdict\n",
    "            by_class = defaultdict(list)\n",
    "            for path, label in self.samples:\n",
    "                by_class[label].append((path, label))\n",
    "            \n",
    "            per_class = max_samples // NUM_CLASSES\n",
    "            self.samples = []\n",
    "            for label, items in by_class.items():\n",
    "                self.samples.extend(items[:per_class])\n",
    "        \n",
    "        print(f\"üìä Loaded {len(self.samples)} test samples from {self.root_dir}\")\n",
    "        \n",
    "        # Print class distribution\n",
    "        class_counts = {}\n",
    "        for _, label in self.samples:\n",
    "            class_counts[label] = class_counts.get(label, 0) + 1\n",
    "        for idx, name in enumerate(CLASS_NAMES):\n",
    "            print(f\"   {name}: {class_counts.get(idx, 0)} samples\")\n",
    "    \n",
    "    def _default_transform(self) -> A.Compose:\n",
    "        \"\"\"Default test transform: resize and convert to tensor.\"\"\"\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),  # Keep in [0, 1]\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, dict]:\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed[\"image\"]\n",
    "        \n",
    "        # Metadata\n",
    "        metadata = {\n",
    "            \"image_path\": str(img_path),\n",
    "            \"class_name\": CLASS_NAMES[label]\n",
    "        }\n",
    "        \n",
    "        return image.float(), label, metadata\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def create_model(num_classes: int = NUM_CLASSES, pretrained: bool = False) -> nn.Module:\n",
    "    \"\"\"Create ResNet-50 model for ISIC classification.\"\"\"\n",
    "    model = timm.create_model(\n",
    "        \"resnet50\",\n",
    "        pretrained=pretrained,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def load_checkpoint(model: nn.Module, checkpoint_path: Path, device: torch.device) -> dict:\n",
    "    \"\"\"\n",
    "    Load model checkpoint and return metadata.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        checkpoint_path: Path to checkpoint file\n",
    "        device: Target device\n",
    "    \n",
    "    Returns:\n",
    "        Checkpoint metadata dictionary\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    \n",
    "    # Handle different checkpoint formats\n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        metadata = {\n",
    "            \"epoch\": checkpoint.get(\"epoch\", \"unknown\"),\n",
    "            \"val_acc\": checkpoint.get(\"val_acc\", checkpoint.get(\"best_val_acc\", \"unknown\")),\n",
    "            \"seed\": checkpoint.get(\"seed\", \"unknown\")\n",
    "        }\n",
    "    elif \"state_dict\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        metadata = {\"epoch\": \"unknown\", \"val_acc\": \"unknown\", \"seed\": \"unknown\"}\n",
    "    else:\n",
    "        # Direct state dict\n",
    "        model.load_state_dict(checkpoint)\n",
    "        metadata = {\"epoch\": \"unknown\", \"val_acc\": \"unknown\", \"seed\": \"unknown\"}\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def get_normalizer(device: torch.device):\n",
    "    \"\"\"Get ImageNet normalization function.\"\"\"\n",
    "    mean = torch.tensor(IMAGENET_MEAN).view(1, 3, 1, 1).to(device)\n",
    "    std = torch.tensor(IMAGENET_STD).view(1, 3, 1, 1).to(device)\n",
    "    \n",
    "    def normalize(x: torch.Tensor) -> torch.Tensor:\n",
    "        return (x - mean) / std\n",
    "    \n",
    "    return normalize\n",
    "\n",
    "print(\"‚úÖ Dataset and model utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ab186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ‚öôÔ∏è Cell 6: Evaluation Configuration\n",
    "#@markdown **Configure attack parameters and evaluation settings**\n",
    "\n",
    "@dataclass\n",
    "class EvaluationConfig:\n",
    "    \"\"\"Configuration for adversarial evaluation.\"\"\"\n",
    "    \n",
    "    # Seeds to evaluate\n",
    "    seeds: List[int] = field(default_factory=lambda: [42, 123, 456])\n",
    "    \n",
    "    # Epsilon values (perturbation budgets)\n",
    "    epsilons: List[float] = field(default_factory=lambda: [2/255, 4/255, 8/255])\n",
    "    \n",
    "    # Attack configurations\n",
    "    fgsm_enabled: bool = True\n",
    "    pgd_enabled: bool = True\n",
    "    pgd_steps: int = 40\n",
    "    pgd_step_size: Optional[float] = None  # Auto: epsilon/4\n",
    "    \n",
    "    cw_enabled: bool = True\n",
    "    cw_iterations: int = 100  # Reduced for speed (default 1000)\n",
    "    cw_confidence: float = 0.0\n",
    "    cw_learning_rate: float = 0.01\n",
    "    \n",
    "    # Evaluation settings\n",
    "    batch_size: int = 64  # Increase for A100\n",
    "    num_workers: int = 4\n",
    "    max_test_samples: Optional[int] = None  # None = all samples\n",
    "    \n",
    "    # Output settings\n",
    "    save_adversarial_examples: bool = True\n",
    "    num_examples_to_save: int = 50\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Adjust settings based on hardware.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            \n",
    "            # Optimize batch size for GPU memory\n",
    "            if gpu_mem >= 35:  # A100 40GB\n",
    "                self.batch_size = 128\n",
    "                self.num_workers = 8\n",
    "                self.cw_iterations = 200\n",
    "                print(f\"‚ö° A100 optimizations: batch={self.batch_size}, C&W iters={self.cw_iterations}\")\n",
    "            elif gpu_mem >= 14:  # T4/V100\n",
    "                self.batch_size = 64\n",
    "                self.num_workers = 4\n",
    "                self.cw_iterations = 100\n",
    "                print(f\"‚ö° T4/V100 settings: batch={self.batch_size}\")\n",
    "            else:  # Smaller GPU\n",
    "                self.batch_size = 32\n",
    "                self.num_workers = 2\n",
    "                self.cw_iterations = 50\n",
    "                print(f\"‚ö†Ô∏è Limited GPU: batch={self.batch_size}\")\n",
    "    \n",
    "    def get_epsilon_str(self, eps: float) -> str:\n",
    "        \"\"\"Convert epsilon to readable string.\"\"\"\n",
    "        return f\"{int(eps * 255)}/255\"\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Get configuration summary.\"\"\"\n",
    "        lines = [\n",
    "            \"=\"*60,\n",
    "            \"üìã EVALUATION CONFIGURATION\",\n",
    "            \"=\"*60,\n",
    "            f\"Seeds: {self.seeds}\",\n",
    "            f\"Epsilons: {[self.get_epsilon_str(e) for e in self.epsilons]}\",\n",
    "            \"\",\n",
    "            \"Attacks:\",\n",
    "            f\"  FGSM: {'‚úì' if self.fgsm_enabled else '‚úó'}\",\n",
    "            f\"  PGD:  {'‚úì' if self.pgd_enabled else '‚úó'} (steps={self.pgd_steps})\",\n",
    "            f\"  C&W:  {'‚úì' if self.cw_enabled else '‚úó'} (iters={self.cw_iterations})\",\n",
    "            \"\",\n",
    "            f\"Batch size: {self.batch_size}\",\n",
    "            f\"Max samples: {self.max_test_samples or 'all'}\",\n",
    "            \"=\"*60\n",
    "        ]\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# Initialize configuration\n",
    "config = EvaluationConfig()\n",
    "print(config.summary())\n",
    "\n",
    "# Epsilon display helper\n",
    "EPSILON_LABELS = {\n",
    "    2/255: \"Œµ=2/255 (weak)\",\n",
    "    4/255: \"Œµ=4/255 (medium)\",\n",
    "    8/255: \"Œµ=8/255 (strong)\"\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Configuration ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7777fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üéØ Cell 7: Adversarial Attack Engine\n",
    "#@markdown **Core attack generation and evaluation functions**\n",
    "\n",
    "class AdversarialEvaluator:\n",
    "    \"\"\"\n",
    "    Unified adversarial evaluation engine.\n",
    "    \n",
    "    Supports FGSM, PGD, and Carlini-Wagner attacks with batch processing\n",
    "    and detailed metrics collection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        device: torch.device,\n",
    "        normalize_fn: callable\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.normalize = normalize_fn\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = {}\n",
    "    \n",
    "    def evaluate_clean(\n",
    "        self,\n",
    "        dataloader: DataLoader,\n",
    "        desc: str = \"Clean Evaluation\"\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate model on clean (unperturbed) data.\"\"\"\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=desc, leave=False):\n",
    "                # Handle 3-tuple (image, label, metadata)\n",
    "                images, labels = batch[0], batch[1]\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                # Forward pass with normalization\n",
    "                logits = self.model(self.normalize(images))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_probs = np.array(all_probs)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "        f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        # Per-class accuracy\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "        \n",
    "        # AUROC (one-vs-rest)\n",
    "        try:\n",
    "            auroc = roc_auc_score(all_labels, all_probs, multi_class='ovr')\n",
    "        except ValueError:\n",
    "            auroc = 0.0\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"balanced_accuracy\": balanced_acc,\n",
    "            \"f1_macro\": f1_macro,\n",
    "            \"f1_weighted\": f1_weighted,\n",
    "            \"auroc\": auroc,\n",
    "            \"per_class_accuracy\": dict(zip(CLASS_NAMES, per_class_acc)),\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"predictions\": all_preds,\n",
    "            \"labels\": all_labels,\n",
    "            \"probabilities\": all_probs\n",
    "        }\n",
    "    \n",
    "    def evaluate_attack(\n",
    "        self,\n",
    "        dataloader: DataLoader,\n",
    "        attack_name: str,\n",
    "        epsilon: float,\n",
    "        attack_fn: callable,\n",
    "        desc: str = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate model under adversarial attack.\n",
    "        \n",
    "        Args:\n",
    "            dataloader: Test data loader\n",
    "            attack_name: Name of attack (FGSM, PGD, CW)\n",
    "            epsilon: Perturbation budget\n",
    "            attack_fn: Function that generates adversarial examples\n",
    "            desc: Progress bar description\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with attack results and metrics\n",
    "        \"\"\"\n",
    "        if desc is None:\n",
    "            desc = f\"{attack_name} Œµ={config.get_epsilon_str(epsilon)}\"\n",
    "        \n",
    "        all_preds_clean = []\n",
    "        all_preds_adv = []\n",
    "        all_labels = []\n",
    "        all_l2_dists = []\n",
    "        all_linf_dists = []\n",
    "        successful_attacks = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Store some examples for visualization\n",
    "        saved_examples = []\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=desc, leave=False):\n",
    "            images, labels = batch[0], batch[1]\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            # Clean predictions\n",
    "            with torch.no_grad():\n",
    "                clean_logits = self.model(self.normalize(images))\n",
    "                clean_preds = clean_logits.argmax(dim=1)\n",
    "            \n",
    "            # Generate adversarial examples\n",
    "            try:\n",
    "                x_adv = attack_fn(images, labels)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Attack failed on batch: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Adversarial predictions\n",
    "            with torch.no_grad():\n",
    "                adv_logits = self.model(self.normalize(x_adv))\n",
    "                adv_preds = adv_logits.argmax(dim=1)\n",
    "            \n",
    "            # Calculate perturbation norms\n",
    "            delta = (x_adv - images).view(images.size(0), -1)\n",
    "            l2_dist = torch.norm(delta, p=2, dim=1)\n",
    "            linf_dist = torch.norm(delta, p=float('inf'), dim=1)\n",
    "            \n",
    "            # Track successful attacks (correctly classified ‚Üí misclassified)\n",
    "            was_correct = (clean_preds == labels)\n",
    "            is_wrong = (adv_preds != labels)\n",
    "            successful = was_correct & is_wrong\n",
    "            \n",
    "            successful_attacks += successful.sum().item()\n",
    "            total_samples += was_correct.sum().item()  # Only count correctly classified\n",
    "            \n",
    "            # Store results\n",
    "            all_preds_clean.extend(clean_preds.cpu().numpy())\n",
    "            all_preds_adv.extend(adv_preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_l2_dists.extend(l2_dist.cpu().numpy())\n",
    "            all_linf_dists.extend(linf_dist.cpu().numpy())\n",
    "            \n",
    "            # Save examples for visualization\n",
    "            if len(saved_examples) < config.num_examples_to_save:\n",
    "                for i in range(min(5, images.size(0))):\n",
    "                    if len(saved_examples) >= config.num_examples_to_save:\n",
    "                        break\n",
    "                    saved_examples.append({\n",
    "                        \"clean\": images[i].cpu(),\n",
    "                        \"adversarial\": x_adv[i].cpu(),\n",
    "                        \"perturbation\": (x_adv[i] - images[i]).cpu(),\n",
    "                        \"true_label\": labels[i].item(),\n",
    "                        \"clean_pred\": clean_preds[i].item(),\n",
    "                        \"adv_pred\": adv_preds[i].item()\n",
    "                    })\n",
    "        \n",
    "        # Convert to arrays\n",
    "        all_preds_clean = np.array(all_preds_clean)\n",
    "        all_preds_adv = np.array(all_preds_adv)\n",
    "        all_labels = np.array(all_labels)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        clean_acc = accuracy_score(all_labels, all_preds_clean)\n",
    "        robust_acc = accuracy_score(all_labels, all_preds_adv)\n",
    "        attack_success_rate = successful_attacks / max(total_samples, 1)\n",
    "        \n",
    "        # Per-class robust accuracy\n",
    "        cm_adv = confusion_matrix(all_labels, all_preds_adv)\n",
    "        per_class_robust_acc = cm_adv.diagonal() / cm_adv.sum(axis=1)\n",
    "        \n",
    "        return {\n",
    "            \"attack_name\": attack_name,\n",
    "            \"epsilon\": epsilon,\n",
    "            \"epsilon_str\": config.get_epsilon_str(epsilon),\n",
    "            \"clean_accuracy\": clean_acc,\n",
    "            \"robust_accuracy\": robust_acc,\n",
    "            \"accuracy_drop\": clean_acc - robust_acc,\n",
    "            \"attack_success_rate\": attack_success_rate,\n",
    "            \"mean_l2_dist\": np.mean(all_l2_dists),\n",
    "            \"mean_linf_dist\": np.mean(all_linf_dists),\n",
    "            \"per_class_robust_accuracy\": dict(zip(CLASS_NAMES, per_class_robust_acc)),\n",
    "            \"confusion_matrix\": cm_adv,\n",
    "            \"saved_examples\": saved_examples,\n",
    "            \"predictions_clean\": all_preds_clean,\n",
    "            \"predictions_adv\": all_preds_adv,\n",
    "            \"labels\": all_labels\n",
    "        }\n",
    "    \n",
    "    def create_fgsm_attack(self, epsilon: float) -> callable:\n",
    "        \"\"\"Create FGSM attack function.\"\"\"\n",
    "        fgsm_config = FGSMConfig(\n",
    "            epsilon=epsilon,\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False,\n",
    "            device=str(self.device)\n",
    "        )\n",
    "        attack = FGSM(fgsm_config)\n",
    "        \n",
    "        def attack_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "            return attack.generate(\n",
    "                self.model, x, y,\n",
    "                loss_fn=nn.CrossEntropyLoss(),\n",
    "                normalize=self.normalize\n",
    "            )\n",
    "        return attack_fn\n",
    "    \n",
    "    def create_pgd_attack(self, epsilon: float, num_steps: int = 40) -> callable:\n",
    "        \"\"\"Create PGD attack function.\"\"\"\n",
    "        step_size = epsilon / 4  # Standard choice\n",
    "        \n",
    "        pgd_config = PGDConfig(\n",
    "            epsilon=epsilon,\n",
    "            num_steps=num_steps,\n",
    "            step_size=step_size,\n",
    "            random_start=True,\n",
    "            early_stop=False,\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False,\n",
    "            device=str(self.device)\n",
    "        )\n",
    "        attack = PGD(pgd_config)\n",
    "        \n",
    "        def attack_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "            return attack.generate(\n",
    "                self.model, x, y,\n",
    "                loss_fn=nn.CrossEntropyLoss(),\n",
    "                normalize=self.normalize\n",
    "            )\n",
    "        return attack_fn\n",
    "    \n",
    "    def create_cw_attack(\n",
    "        self,\n",
    "        confidence: float = 0.0,\n",
    "        max_iterations: int = 100,\n",
    "        learning_rate: float = 0.01\n",
    "    ) -> callable:\n",
    "        \"\"\"Create Carlini-Wagner L2 attack function.\"\"\"\n",
    "        cw_config = CWConfig(\n",
    "            confidence=confidence,\n",
    "            learning_rate=learning_rate,\n",
    "            max_iterations=max_iterations,\n",
    "            binary_search_steps=5,  # Reduced for speed\n",
    "            initial_c=1e-3,\n",
    "            clip_min=0.0,\n",
    "            clip_max=1.0,\n",
    "            targeted=False,\n",
    "            device=str(self.device)\n",
    "        )\n",
    "        attack = CarliniWagner(cw_config)\n",
    "        \n",
    "        def attack_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "            return attack.generate(\n",
    "                self.model, x, y,\n",
    "                normalize=self.normalize\n",
    "            )\n",
    "        return attack_fn\n",
    "\n",
    "print(\"‚úÖ AdversarialEvaluator class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32201a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üì¶ Cell 8: Load Test Dataset and Models\n",
    "#@markdown **Load test data and all seed checkpoints**\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä LOADING TEST DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = ISICTestDataset(\n",
    "    root_dir=paths.test_dir,\n",
    "    max_samples=config.max_test_samples\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Test samples: {len(test_dataset)}\")\n",
    "print(f\"‚úÖ Batches: {len(test_loader)}\")\n",
    "print(f\"‚úÖ Batch size: {config.batch_size}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODELS FOR ALL SEEDS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîß LOADING MODEL CHECKPOINTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = {}\n",
    "normalize = get_normalizer(device)\n",
    "\n",
    "for seed in config.seeds:\n",
    "    checkpoint_path = paths.checkpoint_dir / f\"baseline_seed_{seed}.pt\"\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        # Try alternative naming\n",
    "        alt_path = paths.checkpoint_dir / f\"seed_{seed}_best.pt\"\n",
    "        if alt_path.exists():\n",
    "            checkpoint_path = alt_path\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Checkpoint not found for seed {seed}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nüì• Loading seed {seed}...\")\n",
    "    model = create_model(num_classes=NUM_CLASSES, pretrained=False)\n",
    "    metadata = load_checkpoint(model, checkpoint_path, device)\n",
    "    \n",
    "    models[seed] = model\n",
    "    \n",
    "    print(f\"   ‚úÖ Loaded: {checkpoint_path.name}\")\n",
    "    print(f\"   üìà Validation accuracy: {metadata.get('val_acc', 'N/A')}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(models)} models\")\n",
    "\n",
    "# ============================================================================\n",
    "# SANITY CHECK: VERIFY CLEAN ACCURACY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç SANITY CHECK: CLEAN ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "clean_results = {}\n",
    "\n",
    "for seed, model in models.items():\n",
    "    print(f\"\\nüß™ Evaluating seed {seed}...\")\n",
    "    evaluator = AdversarialEvaluator(model, device, normalize)\n",
    "    result = evaluator.evaluate_clean(test_loader, desc=f\"Clean eval (seed {seed})\")\n",
    "    clean_results[seed] = result\n",
    "    \n",
    "    print(f\"   ‚úÖ Accuracy: {result['accuracy']*100:.2f}%\")\n",
    "    print(f\"   ‚úÖ Balanced Accuracy: {result['balanced_accuracy']*100:.2f}%\")\n",
    "    print(f\"   ‚úÖ F1 (macro): {result['f1_macro']*100:.2f}%\")\n",
    "    print(f\"   ‚úÖ AUROC: {result['auroc']*100:.2f}%\")\n",
    "\n",
    "# Summary statistics\n",
    "mean_acc = np.mean([r['accuracy'] for r in clean_results.values()])\n",
    "std_acc = np.std([r['accuracy'] for r in clean_results.values()])\n",
    "\n",
    "print(f\"\\nüìä Mean Clean Accuracy: {mean_acc*100:.2f}% ¬± {std_acc*100:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259012ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üöÄ Cell 9: Run Full Adversarial Evaluation\n",
    "#@markdown **Execute FGSM, PGD, and C&W attacks across all seeds and epsilons**\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ STARTING FULL ADVERSARIAL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚è±Ô∏è  Start time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"üéØ Seeds: {config.seeds}\")\n",
    "print(f\"üéØ Epsilons: {[config.get_epsilon_str(e) for e in config.epsilons]}\")\n",
    "print(f\"üéØ Attacks: FGSM={'‚úì' if config.fgsm_enabled else '‚úó'}, \"\n",
    "      f\"PGD={'‚úì' if config.pgd_enabled else '‚úó'}, \"\n",
    "      f\"C&W={'‚úì' if config.cw_enabled else '‚úó'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Results storage\n",
    "all_results = {\n",
    "    \"clean\": clean_results,\n",
    "    \"fgsm\": {},\n",
    "    \"pgd\": {},\n",
    "    \"cw\": {}\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for seed in config.seeds:\n",
    "    if seed not in models:\n",
    "        continue\n",
    "    \n",
    "    model = models[seed]\n",
    "    evaluator = AdversarialEvaluator(model, device, normalize)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üî¨ EVALUATING SEED {seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Initialize seed results\n",
    "    all_results[\"fgsm\"][seed] = {}\n",
    "    all_results[\"pgd\"][seed] = {}\n",
    "    all_results[\"cw\"][seed] = {}\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FGSM ATTACKS\n",
    "    # ========================================================================\n",
    "    if config.fgsm_enabled:\n",
    "        print(f\"\\n‚ö° FGSM Attacks (Seed {seed})\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for eps in config.epsilons:\n",
    "            attack_fn = evaluator.create_fgsm_attack(eps)\n",
    "            result = evaluator.evaluate_attack(\n",
    "                test_loader,\n",
    "                attack_name=\"FGSM\",\n",
    "                epsilon=eps,\n",
    "                attack_fn=attack_fn\n",
    "            )\n",
    "            all_results[\"fgsm\"][seed][eps] = result\n",
    "            \n",
    "            print(f\"   Œµ={config.get_epsilon_str(eps):>7}: \"\n",
    "                  f\"Robust Acc = {result['robust_accuracy']*100:5.2f}% \"\n",
    "                  f\"(‚Üì{result['accuracy_drop']*100:5.2f}pp)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PGD ATTACKS\n",
    "    # ========================================================================\n",
    "    if config.pgd_enabled:\n",
    "        print(f\"\\nüîÑ PGD-{config.pgd_steps} Attacks (Seed {seed})\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for eps in config.epsilons:\n",
    "            attack_fn = evaluator.create_pgd_attack(eps, num_steps=config.pgd_steps)\n",
    "            result = evaluator.evaluate_attack(\n",
    "                test_loader,\n",
    "                attack_name=f\"PGD-{config.pgd_steps}\",\n",
    "                epsilon=eps,\n",
    "                attack_fn=attack_fn\n",
    "            )\n",
    "            all_results[\"pgd\"][seed][eps] = result\n",
    "            \n",
    "            print(f\"   Œµ={config.get_epsilon_str(eps):>7}: \"\n",
    "                  f\"Robust Acc = {result['robust_accuracy']*100:5.2f}% \"\n",
    "                  f\"(‚Üì{result['accuracy_drop']*100:5.2f}pp)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # C&W ATTACKS (Only at strongest epsilon for speed)\n",
    "    # ========================================================================\n",
    "    if config.cw_enabled:\n",
    "        print(f\"\\nüéØ C&W L2 Attack (Seed {seed})\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # C&W is epsilon-free (L2 minimization), run once\n",
    "        attack_fn = evaluator.create_cw_attack(\n",
    "            confidence=config.cw_confidence,\n",
    "            max_iterations=config.cw_iterations,\n",
    "            learning_rate=config.cw_learning_rate\n",
    "        )\n",
    "        result = evaluator.evaluate_attack(\n",
    "            test_loader,\n",
    "            attack_name=\"C&W-L2\",\n",
    "            epsilon=0.0,  # C&W minimizes L2 directly\n",
    "            attack_fn=attack_fn,\n",
    "            desc=\"C&W L2 Attack\"\n",
    "        )\n",
    "        all_results[\"cw\"][seed][\"l2\"] = result\n",
    "        \n",
    "        print(f\"   Robust Acc = {result['robust_accuracy']*100:5.2f}% \"\n",
    "              f\"(‚Üì{result['accuracy_drop']*100:5.2f}pp)\")\n",
    "        print(f\"   Mean L2 perturbation: {result['mean_l2_dist']:.4f}\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Timing\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ EVALUATION COMPLETE\")\n",
    "print(f\"‚è±Ô∏è  Total time: {elapsed/60:.1f} minutes\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üìä Cell 10: Results Summary Table\n",
    "#@markdown **Generate comprehensive results summary with statistics**\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ADVERSARIAL ROBUSTNESS RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# BUILD RESULTS DATAFRAME\n",
    "# ============================================================================\n",
    "\n",
    "results_data = []\n",
    "\n",
    "for seed in config.seeds:\n",
    "    if seed not in clean_results:\n",
    "        continue\n",
    "    \n",
    "    # Clean accuracy\n",
    "    clean_acc = clean_results[seed][\"accuracy\"]\n",
    "    \n",
    "    # FGSM results\n",
    "    if config.fgsm_enabled and seed in all_results[\"fgsm\"]:\n",
    "        for eps in config.epsilons:\n",
    "            if eps in all_results[\"fgsm\"][seed]:\n",
    "                r = all_results[\"fgsm\"][seed][eps]\n",
    "                results_data.append({\n",
    "                    \"Seed\": seed,\n",
    "                    \"Attack\": \"FGSM\",\n",
    "                    \"Epsilon\": config.get_epsilon_str(eps),\n",
    "                    \"Epsilon_Val\": eps,\n",
    "                    \"Clean_Acc\": clean_acc * 100,\n",
    "                    \"Robust_Acc\": r[\"robust_accuracy\"] * 100,\n",
    "                    \"Acc_Drop\": r[\"accuracy_drop\"] * 100,\n",
    "                    \"Attack_Success\": r[\"attack_success_rate\"] * 100,\n",
    "                    \"Mean_Linf\": r[\"mean_linf_dist\"]\n",
    "                })\n",
    "    \n",
    "    # PGD results\n",
    "    if config.pgd_enabled and seed in all_results[\"pgd\"]:\n",
    "        for eps in config.epsilons:\n",
    "            if eps in all_results[\"pgd\"][seed]:\n",
    "                r = all_results[\"pgd\"][seed][eps]\n",
    "                results_data.append({\n",
    "                    \"Seed\": seed,\n",
    "                    \"Attack\": f\"PGD-{config.pgd_steps}\",\n",
    "                    \"Epsilon\": config.get_epsilon_str(eps),\n",
    "                    \"Epsilon_Val\": eps,\n",
    "                    \"Clean_Acc\": clean_acc * 100,\n",
    "                    \"Robust_Acc\": r[\"robust_accuracy\"] * 100,\n",
    "                    \"Acc_Drop\": r[\"accuracy_drop\"] * 100,\n",
    "                    \"Attack_Success\": r[\"attack_success_rate\"] * 100,\n",
    "                    \"Mean_Linf\": r[\"mean_linf_dist\"]\n",
    "                })\n",
    "    \n",
    "    # C&W results\n",
    "    if config.cw_enabled and seed in all_results[\"cw\"]:\n",
    "        if \"l2\" in all_results[\"cw\"][seed]:\n",
    "            r = all_results[\"cw\"][seed][\"l2\"]\n",
    "            results_data.append({\n",
    "                \"Seed\": seed,\n",
    "                \"Attack\": \"C&W-L2\",\n",
    "                \"Epsilon\": \"N/A\",\n",
    "                \"Epsilon_Val\": 0,\n",
    "                \"Clean_Acc\": clean_acc * 100,\n",
    "                \"Robust_Acc\": r[\"robust_accuracy\"] * 100,\n",
    "                \"Acc_Drop\": r[\"accuracy_drop\"] * 100,\n",
    "                \"Attack_Success\": r[\"attack_success_rate\"] * 100,\n",
    "                \"Mean_L2\": r[\"mean_l2_dist\"]\n",
    "            })\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "\n",
    "# ============================================================================\n",
    "# AGGREGATE STATISTICS (Mean ¬± Std across seeds)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìà AGGREGATED RESULTS (Mean ¬± Std across seeds)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "# Group by Attack and Epsilon\n",
    "for attack in df_results[\"Attack\"].unique():\n",
    "    for eps in df_results[df_results[\"Attack\"] == attack][\"Epsilon\"].unique():\n",
    "        subset = df_results[(df_results[\"Attack\"] == attack) & (df_results[\"Epsilon\"] == eps)]\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            summary_data.append({\n",
    "                \"Attack\": attack,\n",
    "                \"Epsilon\": eps,\n",
    "                \"Clean_Acc\": f\"{subset['Clean_Acc'].mean():.2f} ¬± {subset['Clean_Acc'].std():.2f}\",\n",
    "                \"Robust_Acc\": f\"{subset['Robust_Acc'].mean():.2f} ¬± {subset['Robust_Acc'].std():.2f}\",\n",
    "                \"Acc_Drop\": f\"{subset['Acc_Drop'].mean():.2f} ¬± {subset['Acc_Drop'].std():.2f}\",\n",
    "                \"Attack_Success\": f\"{subset['Attack_Success'].mean():.2f} ¬± {subset['Attack_Success'].std():.2f}\"\n",
    "            })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# DETAILED PER-SEED TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\nüìã DETAILED RESULTS (Per Seed)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "display_cols = [\"Seed\", \"Attack\", \"Epsilon\", \"Clean_Acc\", \"Robust_Acc\", \"Acc_Drop\", \"Attack_Success\"]\n",
    "print(df_results[display_cols].to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# KEY FINDINGS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\nüîë KEY FINDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Best/worst robust accuracy under PGD-8/255\n",
    "if config.pgd_enabled:\n",
    "    pgd_8 = df_results[(df_results[\"Attack\"] == f\"PGD-{config.pgd_steps}\") & \n",
    "                       (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "    if len(pgd_8) > 0:\n",
    "        mean_robust = pgd_8[\"Robust_Acc\"].mean()\n",
    "        mean_drop = pgd_8[\"Acc_Drop\"].mean()\n",
    "        print(f\"‚Ä¢ PGD-{config.pgd_steps} (Œµ=8/255): {mean_robust:.2f}% robust accuracy \"\n",
    "              f\"(‚Üì{mean_drop:.2f}pp from clean)\")\n",
    "\n",
    "# FGSM vs PGD comparison\n",
    "if config.fgsm_enabled and config.pgd_enabled:\n",
    "    fgsm_8 = df_results[(df_results[\"Attack\"] == \"FGSM\") & (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "    pgd_8 = df_results[(df_results[\"Attack\"] == f\"PGD-{config.pgd_steps}\") & \n",
    "                       (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "    if len(fgsm_8) > 0 and len(pgd_8) > 0:\n",
    "        fgsm_robust = fgsm_8[\"Robust_Acc\"].mean()\n",
    "        pgd_robust = pgd_8[\"Robust_Acc\"].mean()\n",
    "        print(f\"‚Ä¢ FGSM vs PGD gap at Œµ=8/255: {abs(fgsm_robust - pgd_robust):.2f}pp\")\n",
    "\n",
    "# C&W results\n",
    "if config.cw_enabled:\n",
    "    cw_results = df_results[df_results[\"Attack\"] == \"C&W-L2\"]\n",
    "    if len(cw_results) > 0:\n",
    "        mean_cw_robust = cw_results[\"Robust_Acc\"].mean()\n",
    "        print(f\"‚Ä¢ C&W L2 attack: {mean_cw_robust:.2f}% robust accuracy\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ebcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üìà Cell 11: PhD-Level Visualization - Robustness Degradation Curves\n",
    "#@markdown **Publication-quality robustness vs perturbation strength plots**\n",
    "\n",
    "def create_robustness_curves(df_results: pd.DataFrame, config: EvaluationConfig) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create interactive robustness degradation curves.\n",
    "    \n",
    "    Shows how accuracy degrades with increasing perturbation budget Œµ.\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    attacks = [\"FGSM\", f\"PGD-{config.pgd_steps}\"]\n",
    "    colors = {\"FGSM\": \"#FF6B6B\", f\"PGD-{config.pgd_steps}\": \"#4ECDC4\"}\n",
    "    markers = {\"FGSM\": \"circle\", f\"PGD-{config.pgd_steps}\": \"square\"}\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\n",
    "            \"<b>Robustness Degradation by Attack Type</b>\",\n",
    "            \"<b>Accuracy Drop Severity</b>\"\n",
    "        ),\n",
    "        horizontal_spacing=0.12\n",
    "    )\n",
    "    \n",
    "    # Add clean accuracy reference line\n",
    "    clean_acc = df_results[\"Clean_Acc\"].mean()\n",
    "    \n",
    "    for attack in attacks:\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack].copy()\n",
    "        if len(attack_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Group by epsilon\n",
    "        grouped = attack_data.groupby(\"Epsilon_Val\").agg({\n",
    "            \"Robust_Acc\": [\"mean\", \"std\"],\n",
    "            \"Acc_Drop\": [\"mean\", \"std\"]\n",
    "        }).reset_index()\n",
    "        grouped.columns = [\"Epsilon\", \"Robust_Mean\", \"Robust_Std\", \"Drop_Mean\", \"Drop_Std\"]\n",
    "        grouped = grouped.sort_values(\"Epsilon\")\n",
    "        \n",
    "        # Convert epsilon to string labels for x-axis\n",
    "        epsilon_labels = [f\"{int(e*255)}/255\" for e in grouped[\"Epsilon\"]]\n",
    "        \n",
    "        # Plot 1: Robustness curves with confidence bands\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=epsilon_labels,\n",
    "                y=grouped[\"Robust_Mean\"],\n",
    "                mode=\"lines+markers\",\n",
    "                name=attack,\n",
    "                line=dict(color=colors[attack], width=3),\n",
    "                marker=dict(size=12, symbol=markers[attack]),\n",
    "                error_y=dict(\n",
    "                    type=\"data\",\n",
    "                    array=grouped[\"Robust_Std\"],\n",
    "                    visible=True,\n",
    "                    color=colors[attack],\n",
    "                    thickness=2\n",
    "                ),\n",
    "                legendgroup=attack,\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Plot 2: Accuracy drop bars\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=epsilon_labels,\n",
    "                y=grouped[\"Drop_Mean\"],\n",
    "                name=attack,\n",
    "                marker_color=colors[attack],\n",
    "                error_y=dict(\n",
    "                    type=\"data\",\n",
    "                    array=grouped[\"Drop_Std\"],\n",
    "                    visible=True\n",
    "                ),\n",
    "                legendgroup=attack,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Add clean accuracy reference\n",
    "    fig.add_hline(\n",
    "        y=clean_acc, \n",
    "        line_dash=\"dash\", \n",
    "        line_color=\"gray\",\n",
    "        annotation_text=f\"Clean: {clean_acc:.1f}%\",\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=500,\n",
    "        width=1100,\n",
    "        title=dict(\n",
    "            text=\"<b>Adversarial Robustness Analysis: Baseline ResNet-50 on ISIC 2018</b>\",\n",
    "            font=dict(size=18),\n",
    "            x=0.5\n",
    "        ),\n",
    "        font=dict(family=\"Arial\", size=12),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.08,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        ),\n",
    "        paper_bgcolor=\"white\",\n",
    "        plot_bgcolor=\"white\"\n",
    "    )\n",
    "    \n",
    "    # Axis labels\n",
    "    fig.update_xaxes(title_text=\"Perturbation Budget (Œµ)\", row=1, col=1, gridcolor=\"lightgray\")\n",
    "    fig.update_xaxes(title_text=\"Perturbation Budget (Œµ)\", row=1, col=2, gridcolor=\"lightgray\")\n",
    "    fig.update_yaxes(title_text=\"Robust Accuracy (%)\", row=1, col=1, gridcolor=\"lightgray\", range=[0, 100])\n",
    "    fig.update_yaxes(title_text=\"Accuracy Drop (pp)\", row=1, col=2, gridcolor=\"lightgray\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display\n",
    "fig_robustness = create_robustness_curves(df_results, config)\n",
    "fig_robustness.show()\n",
    "\n",
    "# Save figure\n",
    "if paths.figures_dir.exists():\n",
    "    fig_robustness.write_html(paths.figures_dir / \"robustness_curves.html\")\n",
    "    fig_robustness.write_image(paths.figures_dir / \"robustness_curves.png\", scale=2)\n",
    "    print(f\"‚úÖ Saved to {paths.figures_dir / 'robustness_curves.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üî• Cell 12: PhD-Level Visualization - Per-Class Vulnerability Heatmap\n",
    "#@markdown **Detailed heatmap showing which skin lesion classes are most vulnerable**\n",
    "\n",
    "def create_vulnerability_heatmap(all_results: dict, clean_results: dict, config: EvaluationConfig) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create per-class vulnerability heatmap across attacks and epsilons.\n",
    "    \"\"\"\n",
    "    # Build vulnerability matrix\n",
    "    attack_configs = []\n",
    "    \n",
    "    # Add FGSM configs\n",
    "    if config.fgsm_enabled:\n",
    "        for eps in config.epsilons:\n",
    "            attack_configs.append((\"FGSM\", eps, f\"FGSM Œµ={config.get_epsilon_str(eps)}\"))\n",
    "    \n",
    "    # Add PGD configs\n",
    "    if config.pgd_enabled:\n",
    "        for eps in config.epsilons:\n",
    "            attack_configs.append((f\"PGD-{config.pgd_steps}\", eps, f\"PGD-{config.pgd_steps} Œµ={config.get_epsilon_str(eps)}\"))\n",
    "    \n",
    "    # Add C&W\n",
    "    if config.cw_enabled:\n",
    "        attack_configs.append((\"C&W-L2\", \"l2\", \"C&W L2\"))\n",
    "    \n",
    "    # Calculate mean per-class robust accuracy across seeds\n",
    "    vulnerability_matrix = []\n",
    "    \n",
    "    for class_name in CLASS_NAMES:\n",
    "        row = []\n",
    "        for attack_type, eps_key, label in attack_configs:\n",
    "            # Get attack dict key\n",
    "            attack_key = \"fgsm\" if attack_type == \"FGSM\" else (\"pgd\" if \"PGD\" in attack_type else \"cw\")\n",
    "            \n",
    "            accuracies = []\n",
    "            for seed in config.seeds:\n",
    "                if seed not in all_results[attack_key]:\n",
    "                    continue\n",
    "                if eps_key not in all_results[attack_key][seed]:\n",
    "                    continue\n",
    "                \n",
    "                result = all_results[attack_key][seed][eps_key]\n",
    "                if class_name in result.get(\"per_class_robust_accuracy\", {}):\n",
    "                    accuracies.append(result[\"per_class_robust_accuracy\"][class_name] * 100)\n",
    "            \n",
    "            if accuracies:\n",
    "                row.append(np.mean(accuracies))\n",
    "            else:\n",
    "                row.append(np.nan)\n",
    "        \n",
    "        vulnerability_matrix.append(row)\n",
    "    \n",
    "    vulnerability_matrix = np.array(vulnerability_matrix)\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=vulnerability_matrix,\n",
    "        x=[c[2] for c in attack_configs],\n",
    "        y=[f\"{name}\\n({CLASS_DESCRIPTIONS[name].split(' ')[0]})\" for name in CLASS_NAMES],\n",
    "        colorscale=[\n",
    "            [0, \"#d73027\"],      # Red - low accuracy (vulnerable)\n",
    "            [0.25, \"#fc8d59\"],   # Orange\n",
    "            [0.5, \"#fee08b\"],    # Yellow\n",
    "            [0.75, \"#91cf60\"],   # Light green\n",
    "            [1, \"#1a9850\"]       # Dark green - high accuracy (robust)\n",
    "        ],\n",
    "        colorbar=dict(\n",
    "            title=\"Robust<br>Accuracy (%)\",\n",
    "            titleside=\"right\",\n",
    "            ticksuffix=\"%\"\n",
    "        ),\n",
    "        text=np.round(vulnerability_matrix, 1),\n",
    "        texttemplate=\"%{text:.1f}%\",\n",
    "        textfont=dict(size=11, color=\"black\"),\n",
    "        hoverongaps=False,\n",
    "        hovertemplate=\"<b>%{y}</b><br>Attack: %{x}<br>Robust Acc: %{z:.2f}%<extra></extra>\"\n",
    "    ))\n",
    "    \n",
    "    # Add clean accuracy comparison as annotation column\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"<b>Per-Class Adversarial Vulnerability Analysis</b><br>\"\n",
    "                 \"<sup>Red = Vulnerable (low accuracy) | Green = Robust (high accuracy)</sup>\",\n",
    "            font=dict(size=16),\n",
    "            x=0.5\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Attack Configuration\",\n",
    "            tickangle=45,\n",
    "            side=\"bottom\"\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Skin Lesion Class\",\n",
    "            autorange=\"reversed\"\n",
    "        ),\n",
    "        height=550,\n",
    "        width=1000,\n",
    "        font=dict(family=\"Arial\", size=12),\n",
    "        paper_bgcolor=\"white\",\n",
    "        plot_bgcolor=\"white\"\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display\n",
    "fig_heatmap = create_vulnerability_heatmap(all_results, clean_results, config)\n",
    "fig_heatmap.show()\n",
    "\n",
    "# Save\n",
    "if paths.figures_dir.exists():\n",
    "    fig_heatmap.write_html(paths.figures_dir / \"vulnerability_heatmap.html\")\n",
    "    fig_heatmap.write_image(paths.figures_dir / \"vulnerability_heatmap.png\", scale=2)\n",
    "    print(f\"‚úÖ Saved to {paths.figures_dir / 'vulnerability_heatmap.png'}\")\n",
    "\n",
    "# ============================================================================\n",
    "# IDENTIFY MOST VULNERABLE CLASSES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüéØ CLASS VULNERABILITY RANKING (Under PGD-40 Œµ=8/255)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Get PGD-40 at 8/255 per-class results\n",
    "eps_8 = 8/255\n",
    "class_vuln = {}\n",
    "\n",
    "for seed in config.seeds:\n",
    "    if seed not in all_results[\"pgd\"]:\n",
    "        continue\n",
    "    if eps_8 not in all_results[\"pgd\"][seed]:\n",
    "        continue\n",
    "    \n",
    "    result = all_results[\"pgd\"][seed][eps_8]\n",
    "    for class_name, acc in result.get(\"per_class_robust_accuracy\", {}).items():\n",
    "        if class_name not in class_vuln:\n",
    "            class_vuln[class_name] = []\n",
    "        class_vuln[class_name].append(acc * 100)\n",
    "\n",
    "# Sort by vulnerability (lowest accuracy = most vulnerable)\n",
    "sorted_vuln = sorted(\n",
    "    [(name, np.mean(accs), np.std(accs)) for name, accs in class_vuln.items()],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "\n",
    "for rank, (name, mean_acc, std_acc) in enumerate(sorted_vuln, 1):\n",
    "    status = \"üî¥ CRITICAL\" if mean_acc < 20 else \"üü° MODERATE\" if mean_acc < 50 else \"üü¢ ROBUST\"\n",
    "    print(f\"{rank}. {name:6} ({CLASS_DESCRIPTIONS[name]:30}): {mean_acc:5.1f}% ¬± {std_acc:4.1f}% {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a849c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üéØ Cell 13: PhD-Level Visualization - Clean vs Adversarial Confusion Matrices\n",
    "#@markdown **Side-by-side confusion matrices showing attack impact on predictions**\n",
    "\n",
    "def create_confusion_matrix_comparison(\n",
    "    clean_results: dict, \n",
    "    all_results: dict, \n",
    "    seed: int,\n",
    "    attack_key: str = \"pgd\",\n",
    "    epsilon: float = 8/255\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create side-by-side confusion matrices for clean vs adversarial.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Get confusion matrices\n",
    "    cm_clean = clean_results[seed][\"confusion_matrix\"]\n",
    "    \n",
    "    # Get adversarial CM\n",
    "    if attack_key == \"cw\":\n",
    "        cm_adv = all_results[attack_key][seed][\"l2\"][\"confusion_matrix\"]\n",
    "        attack_label = \"C&W L2\"\n",
    "    else:\n",
    "        cm_adv = all_results[attack_key][seed][epsilon][\"confusion_matrix\"]\n",
    "        eps_str = f\"{int(epsilon*255)}/255\"\n",
    "        attack_label = f\"PGD-{config.pgd_steps} (Œµ={eps_str})\" if attack_key == \"pgd\" else f\"FGSM (Œµ={eps_str})\"\n",
    "    \n",
    "    # Normalize confusion matrices\n",
    "    cm_clean_norm = cm_clean.astype(float) / cm_clean.sum(axis=1, keepdims=True)\n",
    "    cm_adv_norm = cm_adv.astype(float) / cm_adv.sum(axis=1, keepdims=True)\n",
    "    cm_diff = cm_adv_norm - cm_clean_norm\n",
    "    \n",
    "    # Plot 1: Clean CM\n",
    "    im1 = axes[0].imshow(cm_clean_norm, cmap=\"Blues\", vmin=0, vmax=1)\n",
    "    axes[0].set_title(f\"Clean Predictions\\n(Seed {seed})\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[0].set_xlabel(\"Predicted\")\n",
    "    axes[0].set_ylabel(\"True Label\")\n",
    "    axes[0].set_xticks(range(len(CLASS_NAMES)))\n",
    "    axes[0].set_yticks(range(len(CLASS_NAMES)))\n",
    "    axes[0].set_xticklabels(CLASS_NAMES, rotation=45, ha=\"right\")\n",
    "    axes[0].set_yticklabels(CLASS_NAMES)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(CLASS_NAMES)):\n",
    "        for j in range(len(CLASS_NAMES)):\n",
    "            val = cm_clean_norm[i, j]\n",
    "            color = \"white\" if val > 0.5 else \"black\"\n",
    "            axes[0].text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
    "    \n",
    "    plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Plot 2: Adversarial CM\n",
    "    im2 = axes[1].imshow(cm_adv_norm, cmap=\"Reds\", vmin=0, vmax=1)\n",
    "    axes[1].set_title(f\"Under {attack_label}\\n(Seed {seed})\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[1].set_xlabel(\"Predicted\")\n",
    "    axes[1].set_ylabel(\"True Label\")\n",
    "    axes[1].set_xticks(range(len(CLASS_NAMES)))\n",
    "    axes[1].set_yticks(range(len(CLASS_NAMES)))\n",
    "    axes[1].set_xticklabels(CLASS_NAMES, rotation=45, ha=\"right\")\n",
    "    axes[1].set_yticklabels(CLASS_NAMES)\n",
    "    \n",
    "    for i in range(len(CLASS_NAMES)):\n",
    "        for j in range(len(CLASS_NAMES)):\n",
    "            val = cm_adv_norm[i, j]\n",
    "            color = \"white\" if val > 0.5 else \"black\"\n",
    "            axes[1].text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
    "    \n",
    "    plt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Plot 3: Difference (shows where predictions shifted)\n",
    "    im3 = axes[2].imshow(cm_diff, cmap=\"RdBu_r\", vmin=-0.5, vmax=0.5)\n",
    "    axes[2].set_title(f\"Prediction Shift\\n(Adversarial - Clean)\", fontsize=14, fontweight=\"bold\")\n",
    "    axes[2].set_xlabel(\"Predicted\")\n",
    "    axes[2].set_ylabel(\"True Label\")\n",
    "    axes[2].set_xticks(range(len(CLASS_NAMES)))\n",
    "    axes[2].set_yticks(range(len(CLASS_NAMES)))\n",
    "    axes[2].set_xticklabels(CLASS_NAMES, rotation=45, ha=\"right\")\n",
    "    axes[2].set_yticklabels(CLASS_NAMES)\n",
    "    \n",
    "    for i in range(len(CLASS_NAMES)):\n",
    "        for j in range(len(CLASS_NAMES)):\n",
    "            val = cm_diff[i, j]\n",
    "            color = \"white\" if abs(val) > 0.25 else \"black\"\n",
    "            axes[2].text(j, i, f\"{val:+.2f}\", ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
    "    \n",
    "    cbar = plt.colorbar(im3, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(\"Change in probability\", rotation=270, labelpad=15)\n",
    "    \n",
    "    plt.suptitle(\n",
    "        f\"Confusion Matrix Analysis: Impact of {attack_label} Attack\",\n",
    "        fontsize=16, fontweight=\"bold\", y=1.02\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create confusion matrix for first available seed\n",
    "first_seed = config.seeds[0]\n",
    "if first_seed in clean_results and first_seed in all_results.get(\"pgd\", {}):\n",
    "    fig_cm = create_confusion_matrix_comparison(\n",
    "        clean_results, all_results, \n",
    "        seed=first_seed,\n",
    "        attack_key=\"pgd\",\n",
    "        epsilon=8/255\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Save\n",
    "    if paths.figures_dir.exists():\n",
    "        fig_cm.savefig(paths.figures_dir / \"confusion_matrix_comparison.png\", \n",
    "                       dpi=200, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "        print(f\"‚úÖ Saved to {paths.figures_dir / 'confusion_matrix_comparison.png'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Results not available for confusion matrix visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52942c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üï∏Ô∏è Cell 14: PhD-Level Visualization - Radar Chart & Attack Effectiveness\n",
    "#@markdown **Multi-dimensional attack comparison using radar charts**\n",
    "\n",
    "def create_radar_chart(all_results: dict, clean_results: dict, config: EvaluationConfig) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create radar chart comparing attack effectiveness across multiple dimensions.\n",
    "    \"\"\"\n",
    "    # Calculate mean metrics across seeds for strongest epsilon\n",
    "    eps_8 = 8/255\n",
    "    \n",
    "    metrics = {\n",
    "        \"FGSM\": {},\n",
    "        f\"PGD-{config.pgd_steps}\": {},\n",
    "        \"C&W-L2\": {}\n",
    "    }\n",
    "    \n",
    "    dimensions = [\n",
    "        \"Attack Success Rate\",\n",
    "        \"Accuracy Drop\",\n",
    "        \"Per-Class Variance\",\n",
    "        \"Mean Perturbation\",\n",
    "        \"Speed (inverse)\"\n",
    "    ]\n",
    "    \n",
    "    # Calculate FGSM metrics\n",
    "    if config.fgsm_enabled:\n",
    "        fgsm_results = [all_results[\"fgsm\"][s][eps_8] for s in config.seeds if s in all_results[\"fgsm\"] and eps_8 in all_results[\"fgsm\"][s]]\n",
    "        if fgsm_results:\n",
    "            metrics[\"FGSM\"] = {\n",
    "                \"Attack Success Rate\": np.mean([r[\"attack_success_rate\"] for r in fgsm_results]) * 100,\n",
    "                \"Accuracy Drop\": np.mean([r[\"accuracy_drop\"] for r in fgsm_results]) * 100,\n",
    "                \"Per-Class Variance\": np.mean([np.std(list(r[\"per_class_robust_accuracy\"].values())) for r in fgsm_results]) * 100,\n",
    "                \"Mean Perturbation\": np.mean([r[\"mean_linf_dist\"] for r in fgsm_results]) * 255,  # Scale to 0-8\n",
    "                \"Speed (inverse)\": 95  # FGSM is single-step, very fast\n",
    "            }\n",
    "    \n",
    "    # Calculate PGD metrics\n",
    "    if config.pgd_enabled:\n",
    "        pgd_results = [all_results[\"pgd\"][s][eps_8] for s in config.seeds if s in all_results[\"pgd\"] and eps_8 in all_results[\"pgd\"][s]]\n",
    "        if pgd_results:\n",
    "            metrics[f\"PGD-{config.pgd_steps}\"] = {\n",
    "                \"Attack Success Rate\": np.mean([r[\"attack_success_rate\"] for r in pgd_results]) * 100,\n",
    "                \"Accuracy Drop\": np.mean([r[\"accuracy_drop\"] for r in pgd_results]) * 100,\n",
    "                \"Per-Class Variance\": np.mean([np.std(list(r[\"per_class_robust_accuracy\"].values())) for r in pgd_results]) * 100,\n",
    "                \"Mean Perturbation\": np.mean([r[\"mean_linf_dist\"] for r in pgd_results]) * 255,\n",
    "                \"Speed (inverse)\": 40  # PGD-40 is 40x slower than FGSM\n",
    "            }\n",
    "    \n",
    "    # Calculate C&W metrics\n",
    "    if config.cw_enabled:\n",
    "        cw_results = [all_results[\"cw\"][s][\"l2\"] for s in config.seeds if s in all_results[\"cw\"] and \"l2\" in all_results[\"cw\"][s]]\n",
    "        if cw_results:\n",
    "            metrics[\"C&W-L2\"] = {\n",
    "                \"Attack Success Rate\": np.mean([r[\"attack_success_rate\"] for r in cw_results]) * 100,\n",
    "                \"Accuracy Drop\": np.mean([r[\"accuracy_drop\"] for r in cw_results]) * 100,\n",
    "                \"Per-Class Variance\": np.mean([np.std(list(r[\"per_class_robust_accuracy\"].values())) for r in cw_results]) * 100,\n",
    "                \"Mean Perturbation\": np.mean([r[\"mean_l2_dist\"] for r in cw_results]) * 10,  # Scale L2 for visibility\n",
    "                \"Speed (inverse)\": 10  # C&W is slowest\n",
    "            }\n",
    "    \n",
    "    # Create radar chart\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    colors = {\"FGSM\": \"#FF6B6B\", f\"PGD-{config.pgd_steps}\": \"#4ECDC4\", \"C&W-L2\": \"#9B59B6\"}\n",
    "    \n",
    "    for attack_name, data in metrics.items():\n",
    "        if not data:\n",
    "            continue\n",
    "        \n",
    "        values = [data.get(dim, 0) for dim in dimensions]\n",
    "        values.append(values[0])  # Close the polygon\n",
    "        \n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=dimensions + [dimensions[0]],\n",
    "            fill='toself',\n",
    "            name=attack_name,\n",
    "            line=dict(color=colors[attack_name], width=2),\n",
    "            fillcolor=colors[attack_name],\n",
    "            opacity=0.3\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(\n",
    "                visible=True,\n",
    "                range=[0, 100],\n",
    "                ticksuffix=\"%\"\n",
    "            ),\n",
    "            angularaxis=dict(\n",
    "                tickfont=dict(size=12)\n",
    "            )\n",
    "        ),\n",
    "        title=dict(\n",
    "            text=\"<b>Attack Effectiveness Comparison (Radar Chart)</b><br>\"\n",
    "                 \"<sup>Higher values = more effective/impactful attack</sup>\",\n",
    "            font=dict(size=16),\n",
    "            x=0.5\n",
    "        ),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=-0.2,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        ),\n",
    "        height=600,\n",
    "        width=700\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create radar chart\n",
    "fig_radar = create_radar_chart(all_results, clean_results, config)\n",
    "fig_radar.show()\n",
    "\n",
    "# Save\n",
    "if paths.figures_dir.exists():\n",
    "    fig_radar.write_html(paths.figures_dir / \"attack_radar_chart.html\")\n",
    "    fig_radar.write_image(paths.figures_dir / \"attack_radar_chart.png\", scale=2)\n",
    "    print(f\"‚úÖ Saved to {paths.figures_dir / 'attack_radar_chart.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc0f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üñºÔ∏è Cell 15: PhD-Level Visualization - Adversarial Example Gallery\n",
    "#@markdown **Visualize clean images, perturbations, and adversarial examples**\n",
    "\n",
    "def visualize_adversarial_examples(\n",
    "    all_results: dict,\n",
    "    attack_key: str = \"pgd\",\n",
    "    epsilon: float = 8/255,\n",
    "    num_examples: int = 5,\n",
    "    amplification: float = 10.0\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create publication-quality adversarial example visualization.\n",
    "    \n",
    "    Shows:\n",
    "    - Clean image\n",
    "    - Perturbation (amplified for visibility)\n",
    "    - Adversarial image\n",
    "    - Prediction change\n",
    "    \"\"\"\n",
    "    # Get saved examples from first seed\n",
    "    first_seed = config.seeds[0]\n",
    "    \n",
    "    if attack_key == \"cw\":\n",
    "        examples = all_results[attack_key][first_seed][\"l2\"].get(\"saved_examples\", [])\n",
    "    else:\n",
    "        examples = all_results[attack_key][first_seed][epsilon].get(\"saved_examples\", [])\n",
    "    \n",
    "    if not examples:\n",
    "        print(\"‚ö†Ô∏è No saved examples available\")\n",
    "        return None\n",
    "    \n",
    "    # Select successful attacks (prediction changed)\n",
    "    successful = [e for e in examples if e[\"clean_pred\"] != e[\"adv_pred\"]]\n",
    "    if len(successful) < num_examples:\n",
    "        successful = examples  # Use all if not enough successful\n",
    "    \n",
    "    num_examples = min(num_examples, len(successful))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_examples, 4, figsize=(16, 4 * num_examples))\n",
    "    if num_examples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, example in enumerate(successful[:num_examples]):\n",
    "        # Get tensors\n",
    "        clean = example[\"clean\"].numpy().transpose(1, 2, 0)  # CHW -> HWC\n",
    "        adv = example[\"adversarial\"].numpy().transpose(1, 2, 0)\n",
    "        perturbation = example[\"perturbation\"].numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        true_label = CLASS_NAMES[example[\"true_label\"]]\n",
    "        clean_pred = CLASS_NAMES[example[\"clean_pred\"]]\n",
    "        adv_pred = CLASS_NAMES[example[\"adv_pred\"]]\n",
    "        \n",
    "        # Ensure valid range\n",
    "        clean = np.clip(clean, 0, 1)\n",
    "        adv = np.clip(adv, 0, 1)\n",
    "        \n",
    "        # Amplify perturbation for visibility\n",
    "        pert_amplified = perturbation * amplification + 0.5\n",
    "        pert_amplified = np.clip(pert_amplified, 0, 1)\n",
    "        \n",
    "        # L2 and Linf norms\n",
    "        l2_norm = np.sqrt(np.sum(perturbation ** 2))\n",
    "        linf_norm = np.max(np.abs(perturbation))\n",
    "        \n",
    "        # Plot clean image\n",
    "        axes[idx, 0].imshow(clean)\n",
    "        axes[idx, 0].set_title(f\"Clean Image\\nTrue: {true_label}\\nPred: {clean_pred}\", fontsize=11)\n",
    "        axes[idx, 0].axis(\"off\")\n",
    "        if example[\"clean_pred\"] == example[\"true_label\"]:\n",
    "            axes[idx, 0].spines[:].set_visible(True)\n",
    "            for spine in axes[idx, 0].spines.values():\n",
    "                spine.set_edgecolor('green')\n",
    "                spine.set_linewidth(3)\n",
    "        \n",
    "        # Plot perturbation\n",
    "        axes[idx, 1].imshow(pert_amplified)\n",
    "        axes[idx, 1].set_title(f\"Perturbation (√ó{amplification:.0f})\\nL‚àû: {linf_norm*255:.2f}/255\\nL2: {l2_norm:.4f}\", fontsize=11)\n",
    "        axes[idx, 1].axis(\"off\")\n",
    "        \n",
    "        # Plot adversarial image\n",
    "        axes[idx, 2].imshow(adv)\n",
    "        axes[idx, 2].set_title(f\"Adversarial Image\\nPred: {adv_pred}\", fontsize=11)\n",
    "        axes[idx, 2].axis(\"off\")\n",
    "        if example[\"adv_pred\"] != example[\"true_label\"]:\n",
    "            for spine in axes[idx, 2].spines.values():\n",
    "                spine.set_visible(True)\n",
    "                spine.set_edgecolor('red')\n",
    "                spine.set_linewidth(3)\n",
    "        \n",
    "        # Plot difference heatmap\n",
    "        diff = np.mean(np.abs(adv - clean), axis=2)  # Average across channels\n",
    "        im = axes[idx, 3].imshow(diff, cmap=\"hot\", vmin=0, vmax=epsilon * 2)\n",
    "        axes[idx, 3].set_title(f\"Absolute Difference\\n({clean_pred} ‚Üí {adv_pred})\", fontsize=11)\n",
    "        axes[idx, 3].axis(\"off\")\n",
    "        plt.colorbar(im, ax=axes[idx, 3], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    attack_label = \"C&W L2\" if attack_key == \"cw\" else f\"{'PGD' if attack_key == 'pgd' else 'FGSM'}-{config.get_epsilon_str(epsilon)}\"\n",
    "    plt.suptitle(\n",
    "        f\"Adversarial Example Gallery: {attack_label} Attack\\n\"\n",
    "        f\"Green border = correct, Red border = misclassified\",\n",
    "        fontsize=16, fontweight=\"bold\", y=1.02\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create visualization for PGD attack\n",
    "if config.pgd_enabled and config.seeds[0] in all_results.get(\"pgd\", {}):\n",
    "    fig_examples = visualize_adversarial_examples(\n",
    "        all_results,\n",
    "        attack_key=\"pgd\",\n",
    "        epsilon=8/255,\n",
    "        num_examples=5,\n",
    "        amplification=10.0\n",
    "    )\n",
    "    if fig_examples:\n",
    "        plt.show()\n",
    "        \n",
    "        # Save\n",
    "        if paths.figures_dir.exists():\n",
    "            fig_examples.savefig(paths.figures_dir / \"adversarial_examples_pgd.png\",\n",
    "                                 dpi=200, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "            print(f\"‚úÖ Saved to {paths.figures_dir / 'adversarial_examples_pgd.png'}\")\n",
    "\n",
    "# Also show FGSM examples if available\n",
    "if config.fgsm_enabled and config.seeds[0] in all_results.get(\"fgsm\", {}):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üì∏ FGSM Attack Examples\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    fig_fgsm = visualize_adversarial_examples(\n",
    "        all_results,\n",
    "        attack_key=\"fgsm\",\n",
    "        epsilon=8/255,\n",
    "        num_examples=3,\n",
    "        amplification=10.0\n",
    "    )\n",
    "    if fig_fgsm:\n",
    "        plt.show()\n",
    "        \n",
    "        if paths.figures_dir.exists():\n",
    "            fig_fgsm.savefig(paths.figures_dir / \"adversarial_examples_fgsm.png\",\n",
    "                            dpi=200, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "            print(f\"‚úÖ Saved to {paths.figures_dir / 'adversarial_examples_fgsm.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bbbb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üìä Cell 16: PhD-Level Analysis - Statistical Significance & Seed Consistency\n",
    "#@markdown **Bootstrap confidence intervals and cross-seed consistency analysis**\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def statistical_analysis(df_results: pd.DataFrame, config: EvaluationConfig) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform rigorous statistical analysis of adversarial evaluation results.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"üìä STATISTICAL ANALYSIS OF ADVERSARIAL ROBUSTNESS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Cross-seed consistency (Coefficient of Variation)\n",
    "    print(\"\\n1Ô∏è‚É£ CROSS-SEED CONSISTENCY (Coefficient of Variation)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        for eps in df_results[df_results[\"Attack\"] == attack][\"Epsilon\"].unique():\n",
    "            subset = df_results[(df_results[\"Attack\"] == attack) & (df_results[\"Epsilon\"] == eps)]\n",
    "            if len(subset) >= 2:\n",
    "                mean_acc = subset[\"Robust_Acc\"].mean()\n",
    "                std_acc = subset[\"Robust_Acc\"].std()\n",
    "                cv = (std_acc / mean_acc) * 100 if mean_acc > 0 else 0\n",
    "                \n",
    "                consistency = \"‚úÖ High\" if cv < 5 else \"‚ö†Ô∏è Medium\" if cv < 10 else \"‚ùå Low\"\n",
    "                print(f\"   {attack:15} {eps:>7}: CV = {cv:5.2f}% {consistency}\")\n",
    "    \n",
    "    # 2. Bootstrap 95% Confidence Intervals\n",
    "    print(\"\\n2Ô∏è‚É£ BOOTSTRAP 95% CONFIDENCE INTERVALS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    def bootstrap_ci(data, n_bootstrap=1000, ci=0.95):\n",
    "        \"\"\"Calculate bootstrap confidence interval.\"\"\"\n",
    "        if len(data) < 2:\n",
    "            return data.mean(), data.mean(), data.mean()\n",
    "        \n",
    "        bootstraps = []\n",
    "        for _ in range(n_bootstrap):\n",
    "            sample = np.random.choice(data, size=len(data), replace=True)\n",
    "            bootstraps.append(sample.mean())\n",
    "        \n",
    "        lower = np.percentile(bootstraps, (1 - ci) / 2 * 100)\n",
    "        upper = np.percentile(bootstraps, (1 + ci) / 2 * 100)\n",
    "        return data.mean(), lower, upper\n",
    "    \n",
    "    ci_results = []\n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack]\n",
    "        \n",
    "        if attack == \"C&W-L2\":\n",
    "            subset = attack_data\n",
    "            mean, lower, upper = bootstrap_ci(subset[\"Robust_Acc\"].values)\n",
    "            ci_results.append({\n",
    "                \"Attack\": attack,\n",
    "                \"Epsilon\": \"N/A\",\n",
    "                \"Mean\": mean,\n",
    "                \"CI_Lower\": lower,\n",
    "                \"CI_Upper\": upper\n",
    "            })\n",
    "            print(f\"   {attack:15} {'N/A':>7}: {mean:5.2f}% [{lower:5.2f}%, {upper:5.2f}%]\")\n",
    "        else:\n",
    "            for eps in attack_data[\"Epsilon\"].unique():\n",
    "                subset = attack_data[attack_data[\"Epsilon\"] == eps]\n",
    "                mean, lower, upper = bootstrap_ci(subset[\"Robust_Acc\"].values)\n",
    "                ci_results.append({\n",
    "                    \"Attack\": attack,\n",
    "                    \"Epsilon\": eps,\n",
    "                    \"Mean\": mean,\n",
    "                    \"CI_Lower\": lower,\n",
    "                    \"CI_Upper\": upper\n",
    "                })\n",
    "                print(f\"   {attack:15} {eps:>7}: {mean:5.2f}% [{lower:5.2f}%, {upper:5.2f}%]\")\n",
    "    \n",
    "    results[\"confidence_intervals\"] = ci_results\n",
    "    \n",
    "    # 3. Attack Comparison (Paired t-test: FGSM vs PGD)\n",
    "    print(\"\\n3Ô∏è‚É£ ATTACK COMPARISON (Paired t-test at Œµ=8/255)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if config.fgsm_enabled and config.pgd_enabled:\n",
    "        eps_8 = \"8/255\"\n",
    "        fgsm_acc = df_results[(df_results[\"Attack\"] == \"FGSM\") & (df_results[\"Epsilon\"] == eps_8)][\"Robust_Acc\"].values\n",
    "        pgd_acc = df_results[(df_results[\"Attack\"] == f\"PGD-{config.pgd_steps}\") & (df_results[\"Epsilon\"] == eps_8)][\"Robust_Acc\"].values\n",
    "        \n",
    "        if len(fgsm_acc) >= 2 and len(pgd_acc) >= 2 and len(fgsm_acc) == len(pgd_acc):\n",
    "            t_stat, p_value = stats.ttest_rel(fgsm_acc, pgd_acc)\n",
    "            \n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "            \n",
    "            print(f\"   FGSM mean: {fgsm_acc.mean():.2f}%\")\n",
    "            print(f\"   PGD mean:  {pgd_acc.mean():.2f}%\")\n",
    "            print(f\"   Difference: {(fgsm_acc.mean() - pgd_acc.mean()):.2f}pp\")\n",
    "            print(f\"   t-statistic: {t_stat:.3f}\")\n",
    "            print(f\"   p-value: {p_value:.4f} {significance}\")\n",
    "            \n",
    "            results[\"fgsm_vs_pgd\"] = {\n",
    "                \"t_statistic\": t_stat,\n",
    "                \"p_value\": p_value,\n",
    "                \"significant\": p_value < 0.05\n",
    "            }\n",
    "    \n",
    "    # 4. Effect Size (Cohen's d)\n",
    "    print(\"\\n4Ô∏è‚É£ EFFECT SIZE (Cohen's d): Clean vs Adversarial\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack]\n",
    "        clean_acc = attack_data[\"Clean_Acc\"].values\n",
    "        robust_acc = attack_data[\"Robust_Acc\"].values\n",
    "        \n",
    "        # Cohen's d\n",
    "        pooled_std = np.sqrt((clean_acc.std()**2 + robust_acc.std()**2) / 2)\n",
    "        if pooled_std > 0:\n",
    "            cohens_d = (clean_acc.mean() - robust_acc.mean()) / pooled_std\n",
    "        else:\n",
    "            cohens_d = 0\n",
    "        \n",
    "        effect = \"Negligible\" if abs(cohens_d) < 0.2 else \"Small\" if abs(cohens_d) < 0.5 else \"Medium\" if abs(cohens_d) < 0.8 else \"Large\"\n",
    "        print(f\"   {attack:15}: d = {cohens_d:6.2f} ({effect})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run statistical analysis\n",
    "stat_results = statistical_analysis(df_results, config)\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION: SEED CONSISTENCY BOX PLOT\n",
    "# ============================================================================\n",
    "\n",
    "def create_seed_comparison_boxplot(df_results: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"Create box plot comparing robust accuracy across seeds.\"\"\"\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    colors = {\"FGSM\": \"#FF6B6B\", f\"PGD-{config.pgd_steps}\": \"#4ECDC4\", \"C&W-L2\": \"#9B59B6\"}\n",
    "    \n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack]\n",
    "        \n",
    "        fig.add_trace(go.Box(\n",
    "            y=attack_data[\"Robust_Acc\"],\n",
    "            x=[attack] * len(attack_data),\n",
    "            name=attack,\n",
    "            marker_color=colors.get(attack, \"#888888\"),\n",
    "            boxpoints=\"all\",\n",
    "            jitter=0.3,\n",
    "            pointpos=-1.8,\n",
    "            hovertemplate=\"Seed: %{text}<br>Robust Acc: %{y:.2f}%<extra></extra>\",\n",
    "            text=attack_data[\"Seed\"].astype(str)\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=\"<b>Cross-Seed Consistency: Robust Accuracy Distribution</b>\",\n",
    "            font=dict(size=16),\n",
    "            x=0.5\n",
    "        ),\n",
    "        xaxis_title=\"Attack Type\",\n",
    "        yaxis_title=\"Robust Accuracy (%)\",\n",
    "        yaxis=dict(range=[0, 100]),\n",
    "        showlegend=False,\n",
    "        height=500,\n",
    "        width=800\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig_boxplot = create_seed_comparison_boxplot(df_results)\n",
    "fig_boxplot.show()\n",
    "\n",
    "# Save\n",
    "if paths.figures_dir.exists():\n",
    "    fig_boxplot.write_html(paths.figures_dir / \"seed_consistency_boxplot.html\")\n",
    "    fig_boxplot.write_image(paths.figures_dir / \"seed_consistency_boxplot.png\", scale=2)\n",
    "    print(f\"‚úÖ Saved to {paths.figures_dir / 'seed_consistency_boxplot.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0bf83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üíæ Cell 17: Save Results & Export for Dissertation\n",
    "#@markdown **Export all results in multiple formats for dissertation use**\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üíæ SAVING RESULTS & EXPORTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS AS CSV\n",
    "# ============================================================================\n",
    "\n",
    "# Detailed results CSV\n",
    "csv_path = paths.results_dir / \"adversarial_robustness_results.csv\"\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"‚úÖ Detailed results: {csv_path}\")\n",
    "\n",
    "# Summary statistics CSV  \n",
    "summary_path = paths.results_dir / \"adversarial_robustness_summary.csv\"\n",
    "df_summary.to_csv(summary_path, index=False)\n",
    "print(f\"‚úÖ Summary statistics: {summary_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE AS JSON (for programmatic access)\n",
    "# ============================================================================\n",
    "\n",
    "json_results = {\n",
    "    \"metadata\": {\n",
    "        \"evaluation_date\": datetime.now().isoformat(),\n",
    "        \"seeds\": config.seeds,\n",
    "        \"epsilons\": [float(e) for e in config.epsilons],\n",
    "        \"attacks\": {\n",
    "            \"fgsm\": config.fgsm_enabled,\n",
    "            \"pgd\": config.pgd_enabled,\n",
    "            \"pgd_steps\": config.pgd_steps,\n",
    "            \"cw\": config.cw_enabled,\n",
    "            \"cw_iterations\": config.cw_iterations\n",
    "        },\n",
    "        \"dataset\": \"ISIC 2018\",\n",
    "        \"model\": \"ResNet-50\",\n",
    "        \"num_classes\": NUM_CLASSES\n",
    "    },\n",
    "    \"clean_accuracy\": {\n",
    "        str(seed): {\n",
    "            \"accuracy\": float(clean_results[seed][\"accuracy\"]),\n",
    "            \"balanced_accuracy\": float(clean_results[seed][\"balanced_accuracy\"]),\n",
    "            \"f1_macro\": float(clean_results[seed][\"f1_macro\"]),\n",
    "            \"auroc\": float(clean_results[seed][\"auroc\"])\n",
    "        }\n",
    "        for seed in config.seeds if seed in clean_results\n",
    "    },\n",
    "    \"adversarial_results\": df_results.to_dict(orient=\"records\")\n",
    "}\n",
    "\n",
    "json_path = paths.results_dir / \"adversarial_robustness_results.json\"\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(json_results, f, indent=2, default=str)\n",
    "print(f\"‚úÖ JSON results: {json_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATE LATEX TABLE FOR DISSERTATION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_latex_table(df_results: pd.DataFrame, config: EvaluationConfig) -> str:\n",
    "    \"\"\"Generate LaTeX table for dissertation.\"\"\"\n",
    "    \n",
    "    lines = [\n",
    "        \"\\\\begin{table}[htbp]\",\n",
    "        \"\\\\centering\",\n",
    "        \"\\\\caption{Adversarial Robustness Evaluation: Baseline ResNet-50 on ISIC 2018}\",\n",
    "        \"\\\\label{tab:adversarial_robustness}\",\n",
    "        \"\\\\begin{tabular}{llcccc}\",\n",
    "        \"\\\\toprule\",\n",
    "        \"Attack & $\\\\epsilon$ & Clean Acc (\\\\%) & Robust Acc (\\\\%) & Acc Drop (pp) & ASR (\\\\%) \\\\\\\\\",\n",
    "        \"\\\\midrule\"\n",
    "    ]\n",
    "    \n",
    "    # Group and average across seeds\n",
    "    for attack in df_results[\"Attack\"].unique():\n",
    "        attack_data = df_results[df_results[\"Attack\"] == attack]\n",
    "        \n",
    "        for eps in attack_data[\"Epsilon\"].unique():\n",
    "            subset = attack_data[attack_data[\"Epsilon\"] == eps]\n",
    "            \n",
    "            clean = f\"{subset['Clean_Acc'].mean():.1f} $\\\\pm$ {subset['Clean_Acc'].std():.1f}\"\n",
    "            robust = f\"{subset['Robust_Acc'].mean():.1f} $\\\\pm$ {subset['Robust_Acc'].std():.1f}\"\n",
    "            drop = f\"{subset['Acc_Drop'].mean():.1f} $\\\\pm$ {subset['Acc_Drop'].std():.1f}\"\n",
    "            asr = f\"{subset['Attack_Success'].mean():.1f} $\\\\pm$ {subset['Attack_Success'].std():.1f}\"\n",
    "            \n",
    "            lines.append(f\"{attack} & {eps} & {clean} & {robust} & {drop} & {asr} \\\\\\\\\")\n",
    "    \n",
    "    lines.extend([\n",
    "        \"\\\\bottomrule\",\n",
    "        \"\\\\end{tabular}\",\n",
    "        \"\\\\begin{tablenotes}\",\n",
    "        \"\\\\small\",\n",
    "        \"\\\\item Note: Values are mean $\\\\pm$ std across seeds (42, 123, 456). ASR = Attack Success Rate.\",\n",
    "        \"\\\\end{tablenotes}\",\n",
    "        \"\\\\end{table}\"\n",
    "    ])\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "latex_table = generate_latex_table(df_results, config)\n",
    "\n",
    "# Save LaTeX table\n",
    "latex_path = paths.results_dir / \"adversarial_robustness_table.tex\"\n",
    "with open(latex_path, \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "print(f\"‚úÖ LaTeX table: {latex_path}\")\n",
    "\n",
    "# Print LaTeX table\n",
    "print(\"\\nüìÑ LATEX TABLE FOR DISSERTATION:\")\n",
    "print(\"-\" * 60)\n",
    "print(latex_table)\n",
    "\n",
    "# ============================================================================\n",
    "# LIST ALL SAVED FILES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìÅ ALL SAVED FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if paths.results_dir.exists():\n",
    "    for f in sorted(paths.results_dir.rglob(\"*\")):\n",
    "        if f.is_file():\n",
    "            size_kb = f.stat().st_size / 1024\n",
    "            print(f\"   {f.relative_to(paths.results_dir)} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title üìã Cell 18: Executive Summary & Key Findings\n",
    "#@markdown **Final summary of adversarial robustness evaluation**\n",
    "\n",
    "def print_executive_summary(df_results: pd.DataFrame, clean_results: dict, config: EvaluationConfig):\n",
    "    \"\"\"Generate executive summary of evaluation results.\"\"\"\n",
    "    \n",
    "    print(\"‚ïî\" + \"‚ïê\"*68 + \"‚ïó\")\n",
    "    print(\"‚ïë\" + \" \"*20 + \"EXECUTIVE SUMMARY\" + \" \"*31 + \"‚ïë\")\n",
    "    print(\"‚ïë\" + \" \"*10 + \"Phase 4: Adversarial Robustness Evaluation\" + \" \"*15 + \"‚ïë\")\n",
    "    print(\"‚ïö\" + \"‚ïê\"*68 + \"‚ïù\")\n",
    "    \n",
    "    # Clean accuracy\n",
    "    mean_clean = np.mean([r[\"accuracy\"] for r in clean_results.values()]) * 100\n",
    "    std_clean = np.std([r[\"accuracy\"] for r in clean_results.values()]) * 100\n",
    "    \n",
    "    print(f\"\\nüìä BASELINE CLEAN PERFORMANCE\")\n",
    "    print(f\"   ‚Ä¢ Clean Accuracy: {mean_clean:.2f}% ¬± {std_clean:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Model: ResNet-50 (ImageNet pretrained)\")\n",
    "    print(f\"   ‚Ä¢ Dataset: ISIC 2018 (7 classes)\")\n",
    "    print(f\"   ‚Ä¢ Seeds evaluated: {config.seeds}\")\n",
    "    \n",
    "    # Adversarial results summary\n",
    "    print(f\"\\nüõ°Ô∏è ADVERSARIAL ROBUSTNESS FINDINGS\")\n",
    "    \n",
    "    # FGSM\n",
    "    if config.fgsm_enabled:\n",
    "        fgsm_8 = df_results[(df_results[\"Attack\"] == \"FGSM\") & (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "        if len(fgsm_8) > 0:\n",
    "            mean_robust = fgsm_8[\"Robust_Acc\"].mean()\n",
    "            mean_drop = fgsm_8[\"Acc_Drop\"].mean()\n",
    "            print(f\"\\n   FGSM (Œµ=8/255):\")\n",
    "            print(f\"   ‚îú‚îÄ Robust Accuracy: {mean_robust:.2f}%\")\n",
    "            print(f\"   ‚îú‚îÄ Accuracy Drop: {mean_drop:.2f}pp\")\n",
    "            print(f\"   ‚îî‚îÄ Interpretation: {'Moderate vulnerability' if mean_drop < 40 else 'High vulnerability'}\")\n",
    "    \n",
    "    # PGD\n",
    "    if config.pgd_enabled:\n",
    "        pgd_8 = df_results[(df_results[\"Attack\"] == f\"PGD-{config.pgd_steps}\") & (df_results[\"Epsilon\"] == \"8/255\")]\n",
    "        if len(pgd_8) > 0:\n",
    "            mean_robust = pgd_8[\"Robust_Acc\"].mean()\n",
    "            mean_drop = pgd_8[\"Acc_Drop\"].mean()\n",
    "            print(f\"\\n   PGD-{config.pgd_steps} (Œµ=8/255):\")\n",
    "            print(f\"   ‚îú‚îÄ Robust Accuracy: {mean_robust:.2f}%\")\n",
    "            print(f\"   ‚îú‚îÄ Accuracy Drop: {mean_drop:.2f}pp\")\n",
    "            print(f\"   ‚îî‚îÄ Interpretation: {'Severe vulnerability' if mean_drop > 60 else 'High vulnerability' if mean_drop > 40 else 'Moderate'}\")\n",
    "    \n",
    "    # C&W\n",
    "    if config.cw_enabled:\n",
    "        cw = df_results[df_results[\"Attack\"] == \"C&W-L2\"]\n",
    "        if len(cw) > 0:\n",
    "            mean_robust = cw[\"Robust_Acc\"].mean()\n",
    "            mean_drop = cw[\"Acc_Drop\"].mean()\n",
    "            print(f\"\\n   C&W L2:\")\n",
    "            print(f\"   ‚îú‚îÄ Robust Accuracy: {mean_robust:.2f}%\")\n",
    "            print(f\"   ‚îú‚îÄ Accuracy Drop: {mean_drop:.2f}pp\")\n",
    "            print(f\"   ‚îî‚îÄ Interpretation: Strongest attack, minimal perturbation\")\n",
    "    \n",
    "    # Key insights\n",
    "    print(f\"\\nüîë KEY INSIGHTS\")\n",
    "    print(f\"   1. Standard CNNs are highly vulnerable to adversarial attacks\")\n",
    "    print(f\"   2. PGD attacks are stronger than FGSM (multi-step > single-step)\")\n",
    "    print(f\"   3. Some classes (e.g., MEL, BCC) may be more vulnerable than others\")\n",
    "    print(f\"   4. Adversarial training is needed for robust medical AI deployment\")\n",
    "    \n",
    "    # Research implications\n",
    "    print(f\"\\nüìö RESEARCH IMPLICATIONS\")\n",
    "    print(f\"   ‚Ä¢ These results motivate Phase 5: Adversarial Training\")\n",
    "    print(f\"   ‚Ä¢ Tri-objective optimization will balance accuracy, robustness, and explainability\")\n",
    "    print(f\"   ‚Ä¢ Medical AI systems require robustness validation before clinical use\")\n",
    "    \n",
    "    # Files generated\n",
    "    print(f\"\\nüìÅ GENERATED OUTPUTS\")\n",
    "    print(f\"   ‚Ä¢ Results CSV: adversarial_robustness_results.csv\")\n",
    "    print(f\"   ‚Ä¢ Summary CSV: adversarial_robustness_summary.csv\")\n",
    "    print(f\"   ‚Ä¢ JSON results: adversarial_robustness_results.json\")\n",
    "    print(f\"   ‚Ä¢ LaTeX table: adversarial_robustness_table.tex\")\n",
    "    print(f\"   ‚Ä¢ Figures: robustness_curves.png, vulnerability_heatmap.png, etc.\")\n",
    "    \n",
    "    print(\"\\n\" + \"‚ïê\"*70)\n",
    "    print(f\"‚úÖ Phase 4 Adversarial Robustness Evaluation COMPLETE\")\n",
    "    print(f\"‚è±Ô∏è  Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"‚ïê\"*70)\n",
    "\n",
    "# Print executive summary\n",
    "print_executive_summary(df_results, clean_results, config)\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL MEMORY CLEANUP\n",
    "# ============================================================================\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\nüßπ Memory cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
