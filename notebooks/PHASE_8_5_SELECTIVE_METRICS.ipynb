{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76a8ade",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2202628",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 8.5: Environment Setup and Imports\n",
    "\n",
    "This cell configures the environment and imports all necessary modules\n",
    "for selective prediction evaluation metrics.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set project root\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Phase 8.5: Selective Metrics imports\n",
    "from src.selection import (\n",
    "    # Data classes\n",
    "    SelectiveMetrics,\n",
    "    RiskCoverageCurve,\n",
    "    \n",
    "    # Core metric functions\n",
    "    compute_coverage,\n",
    "    compute_selective_accuracy,\n",
    "    compute_selective_risk,\n",
    "    compute_risk_on_rejected,\n",
    "    compute_improvement,\n",
    "    compute_rejection_quality,\n",
    "    compute_rejection_precision_recall,\n",
    "    \n",
    "    # Risk-coverage\n",
    "    compute_risk_coverage_curve,\n",
    "    compute_aurc,\n",
    "    \n",
    "    # Calibration\n",
    "    compute_ece_post_selection,\n",
    "    \n",
    "    # Main entry point\n",
    "    compute_selective_metrics,\n",
    "    \n",
    "    # Comparison\n",
    "    compare_strategies,\n",
    "    \n",
    "    # Visualization\n",
    "    plot_risk_coverage_curve,\n",
    "    plot_accuracy_coverage_curve,\n",
    "    plot_strategy_comparison,\n",
    "    \n",
    "    # Utilities\n",
    "    find_threshold_for_coverage,\n",
    "    compute_metrics_at_coverage,\n",
    "    validate_hypothesis_h3a,\n",
    ")\n",
    "\n",
    "# Plot configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 8),\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'legend.fontsize': 11,\n",
    "})\n",
    "\n",
    "# Create results directory\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results' / 'phase_8_5'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 8.5: SELECTIVE PREDICTION EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Results Dir:  {RESULTS_DIR}\")\n",
    "print(f\"Device:       {DEVICE}\")\n",
    "print(f\"PyTorch:      {torch.__version__}\")\n",
    "print(f\"NumPy:        {np.__version__}\")\n",
    "print(f\"Timestamp:    {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626d939a",
   "metadata": {},
   "source": [
    "## Cell 2: Load Selective Prediction Data\n",
    "\n",
    "Load predictions, labels, confidence scores, and stability scores from Phase 8.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7411306",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 8.5: Load Selective Prediction Data\n",
    "\n",
    "Load the prediction data from previous phases or generate synthetic data\n",
    "for demonstration purposes.\n",
    "\"\"\"\n",
    "\n",
    "# Configuration\n",
    "USE_REAL_DATA = False  # Set to True when real data is available\n",
    "DATA_PATH = PROJECT_ROOT / 'results' / 'phase_8' / 'stability_scores.csv'\n",
    "\n",
    "if USE_REAL_DATA and DATA_PATH.exists():\n",
    "    print(\"Loading real data from Phase 8.4...\")\n",
    "    data = pd.read_csv(DATA_PATH)\n",
    "    predictions = data['prediction'].values\n",
    "    labels = data['label'].values\n",
    "    confidences = data['confidence'].values\n",
    "    stability = data['stability'].values\n",
    "    print(f\"Loaded {len(predictions)} samples\")\n",
    "else:\n",
    "    print(\"Generating synthetic data for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Parameters\n",
    "    N_SAMPLES = 1000\n",
    "    N_CLASSES = 3\n",
    "    ERROR_RATE = 0.15  # 15% error rate\n",
    "    \n",
    "    # Generate ground truth labels\n",
    "    labels = np.random.randint(0, N_CLASSES, size=N_SAMPLES)\n",
    "    \n",
    "    # Generate predictions (with controlled error rate)\n",
    "    predictions = labels.copy()\n",
    "    n_errors = int(N_SAMPLES * ERROR_RATE)\n",
    "    error_indices = np.random.choice(N_SAMPLES, size=n_errors, replace=False)\n",
    "    predictions[error_indices] = (labels[error_indices] + np.random.randint(1, N_CLASSES, size=n_errors)) % N_CLASSES\n",
    "    \n",
    "    # Generate confidence scores (errors have lower confidence)\n",
    "    confidences = np.random.uniform(0.75, 0.99, size=N_SAMPLES)\n",
    "    confidences[error_indices] = np.random.uniform(0.40, 0.70, size=n_errors)\n",
    "    \n",
    "    # Generate stability scores (errors have lower stability)\n",
    "    stability = np.random.uniform(0.70, 0.95, size=N_SAMPLES)\n",
    "    stability[error_indices] = np.random.uniform(0.30, 0.60, size=n_errors)\n",
    "    \n",
    "    print(f\"Generated {N_SAMPLES} synthetic samples\")\n",
    "\n",
    "# Compute derived quantities\n",
    "is_correct = predictions == labels\n",
    "overall_accuracy = np.mean(is_correct)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Samples:      {len(predictions):,}\")\n",
    "print(f\"Overall Accuracy:   {overall_accuracy:.2%}\")\n",
    "print(f\"Error Rate:         {1-overall_accuracy:.2%}\")\n",
    "print(f\"\\nConfidence Scores:\")\n",
    "print(f\"  Mean:   {confidences.mean():.3f}\")\n",
    "print(f\"  Std:    {confidences.std():.3f}\")\n",
    "print(f\"  Range:  [{confidences.min():.3f}, {confidences.max():.3f}]\")\n",
    "print(f\"\\nStability Scores:\")\n",
    "print(f\"  Mean:   {stability.mean():.3f}\")\n",
    "print(f\"  Std:    {stability.std():.3f}\")\n",
    "print(f\"  Range:  [{stability.min():.3f}, {stability.max():.3f}]\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525d4da",
   "metadata": {},
   "source": [
    "## Cell 3: Compute Core Selective Metrics\n",
    "\n",
    "Compute all core metrics for a given selection threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83158616",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 8.5: Compute Core Selective Metrics\n",
    "\n",
    "Compute comprehensive selective prediction metrics using combined\n",
    "confidence + stability scoring at 90% coverage target.\n",
    "\"\"\"\n",
    "\n",
    "# Configuration\n",
    "TARGET_COVERAGE = 0.90\n",
    "CONFIDENCE_WEIGHT = 0.5\n",
    "STABILITY_WEIGHT = 0.5\n",
    "\n",
    "# Compute combined scores\n",
    "combined_scores = CONFIDENCE_WEIGHT * confidences + STABILITY_WEIGHT * stability\n",
    "\n",
    "# Find threshold for target coverage\n",
    "threshold = find_threshold_for_coverage(combined_scores, TARGET_COVERAGE)\n",
    "is_accepted = combined_scores >= threshold\n",
    "\n",
    "print(f\"Target Coverage: {TARGET_COVERAGE:.0%}\")\n",
    "print(f\"Score Threshold: {threshold:.4f}\")\n",
    "print(f\"Actual Coverage: {np.mean(is_accepted):.2%}\")\n",
    "\n",
    "# Compute comprehensive metrics with bootstrap CIs\n",
    "print(\"\\nComputing metrics with bootstrap confidence intervals...\")\n",
    "metrics = compute_selective_metrics(\n",
    "    predictions=predictions,\n",
    "    labels=labels,\n",
    "    is_accepted=is_accepted,\n",
    "    confidences=confidences,\n",
    "    scores=combined_scores,\n",
    "    compute_ci=True,\n",
    "    n_bootstrap=1000,\n",
    "    confidence_level=0.95,\n",
    "    metadata={\n",
    "        \"target_coverage\": TARGET_COVERAGE,\n",
    "        \"threshold\": threshold,\n",
    "        \"confidence_weight\": CONFIDENCE_WEIGHT,\n",
    "        \"stability_weight\": STABILITY_WEIGHT,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "print(metrics.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d954fd",
   "metadata": {},
   "source": [
    "## Cell 4: Hypothesis H3a Validation\n",
    "\n",
    "Validate hypothesis H3a: ‚â•4pp improvement at 90% coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ce0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 8.5: Hypothesis H3a Validation\n",
    "\n",
    "Validate the primary hypothesis:\n",
    "H3a: Selective prediction achieves ‚â•4pp improvement at 90% coverage\n",
    "\"\"\"\n",
    "\n",
    "# Validate H3a\n",
    "h3a_result = validate_hypothesis_h3a(\n",
    "    metrics,\n",
    "    target_improvement=0.04,  # 4 percentage points\n",
    "    target_coverage=0.90\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*60)\n",
    "print(\"HYPOTHESIS H3a VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nHypothesis: {h3a_result['description']}\")\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"  Coverage:           {h3a_result['coverage']:.1%}\")\n",
    "print(f\"  Selective Accuracy: {metrics.selective_accuracy:.2%}\")\n",
    "print(f\"  Overall Accuracy:   {metrics.overall_accuracy:.2%}\")\n",
    "print(f\"  Improvement:        {h3a_result['improvement_pp']:+.2f}pp\")\n",
    "print(f\"  Target:             ‚â•{h3a_result['target_improvement']*100:.0f}pp\")\n",
    "print(f\"  Margin:             {h3a_result['margin']*100:+.2f}pp\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "if h3a_result['passed']:\n",
    "    print(f\"\\n‚úÖ HYPOTHESIS H3a: PASSED\")\n",
    "    print(f\"   Selective prediction achieves {h3a_result['improvement_pp']:.2f}pp improvement\")\n",
    "    print(f\"   at {h3a_result['coverage']:.1%} coverage (exceeds target by {h3a_result['margin']*100:.2f}pp)\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå HYPOTHESIS H3a: NOT PASSED\")\n",
    "    print(f\"   Improvement of {h3a_result['improvement_pp']:.2f}pp is below target of 4pp\")\n",
    "\n",
    "# Confidence interval for improvement\n",
    "if 'improvement' in metrics.confidence_intervals:\n",
    "    ci_low, ci_high = metrics.confidence_intervals['improvement']\n",
    "    print(f\"\\n95% CI for Improvement: [{ci_low*100:.2f}pp, {ci_high*100:.2f}pp]\")\n",
    "    if ci_low >= 0.04:\n",
    "        print(\"   ‚úÖ Entire CI above target threshold - strong evidence!\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc15626",
   "metadata": {},
   "source": [
    "## Cell 5: Strategy Comparison (H3b Validation)\n",
    "\n",
    "Compare different gating strategies to validate H3b:\n",
    "Combined gating outperforms single-signal approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06696b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 8.5: Strategy Comparison for H3b Validation\n",
    "\n",
    "Compare four gating strategies:\n",
    "1. Confidence-only: Accept if confidence ‚â• threshold\n",
    "2. Stability-only: Accept if stability ‚â• threshold\n",
    "3. Combined (AND): Accept if both thresholds met\n",
    "4. Combined Score: Accept based on weighted average score\n",
    "\"\"\"\n",
    "\n",
    "# Configuration for comparison\n",
    "CONFIDENCE_THRESHOLD = 0.85\n",
    "STABILITY_THRESHOLD = 0.75\n",
    "\n",
    "# Compare strategies\n",
    "strategy_results = compare_strategies(\n",
    "    predictions=predictions,\n",
    "    labels=labels,\n",
    "    confidence_scores=confidences,\n",
    "    stability_scores=stability,\n",
    "    confidence_threshold=CONFIDENCE_THRESHOLD,\n",
    "    stability_threshold=STABILITY_THRESHOLD,\n",
    "    target_coverage=TARGET_COVERAGE\n",
    ")\n",
    "\n",
    "# Display comparison table\n",
    "print(\"=\"*80)\n",
    "print(\"STRATEGY COMPARISON (H3b VALIDATION)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nThresholds: œÑ_conf={CONFIDENCE_THRESHOLD}, œÑ_stab={STABILITY_THRESHOLD}\")\n",
    "print(f\"Target Coverage: {TARGET_COVERAGE:.0%}\\n\")\n",
    "\n",
    "print(f\"{'Strategy':<20} {'Coverage':>10} {'Sel.Acc':>10} {'Improvement':>12} {'AURC':>10} {'E-AURC':>10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for name, m in strategy_results.items():\n",
    "    print(f\"{name:<20} {m.coverage:>10.1%} {m.selective_accuracy:>10.1%} {m.improvement*100:>+11.2f}pp {m.aurc:>10.4f} {m.e_aurc:>10.4f}\")\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "# H3b Validation: Does combined outperform single-signal?\n",
    "conf_only = strategy_results['confidence_only']\n",
    "stab_only = strategy_results['stability_only']\n",
    "combined_score = strategy_results['combined_score']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"H3b VALIDATION: Combined vs Single-Signal\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare at similar coverage levels\n",
    "print(f\"\\nAt ~{TARGET_COVERAGE:.0%} coverage (combined_score strategy):\")\n",
    "print(f\"  Combined Score Improvement:   {combined_score.improvement*100:+.2f}pp\")\n",
    "print(f\"  Combined Score AURC:          {combined_score.aurc:.4f}\")\n",
    "\n",
    "# Check if combined is better\n",
    "if combined_score.aurc <= min(conf_only.aurc, stab_only.aurc):\n",
    "    print(f\"\\n‚úÖ HYPOTHESIS H3b: SUPPORTED\")\n",
    "    print(f\"   Combined scoring achieves lower AURC than single-signal approaches\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è HYPOTHESIS H3b: MIXED RESULTS\")\n",
    "    print(f\"   Further investigation needed\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d5203",
   "metadata": {},
   "source": [
    "## Cell 6: Risk-Coverage Curves\n",
    "\n",
    "Generate publication-ready risk-coverage curves for all strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36423792",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 8.5: Risk-Coverage Curves\n",
    "\n",
    "Generate risk-coverage curves showing the trade-off between\n",
    "coverage (fraction of samples accepted) and risk (error rate).\n",
    "\n",
    "Lower curves indicate better selective prediction performance.\n",
    "\"\"\"\n",
    "\n",
    "# Compute risk-coverage curves for each strategy\n",
    "curves = {}\n",
    "\n",
    "# Confidence-only\n",
    "curves['Confidence'] = compute_risk_coverage_curve(\n",
    "    predictions, labels, confidences\n",
    ")\n",
    "\n",
    "# Stability-only\n",
    "curves['Stability'] = compute_risk_coverage_curve(\n",
    "    predictions, labels, stability\n",
    ")\n",
    "\n",
    "# Combined score\n",
    "curves['Combined'] = compute_risk_coverage_curve(\n",
    "    predictions, labels, combined_scores\n",
    ")\n",
    "\n",
    "# Print AURC comparison\n",
    "print(\"=\"*60)\n",
    "print(\"RISK-COVERAGE CURVE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Strategy':<15} {'AURC':>10} {'E-AURC':>10} {'Better than Conf?':>20}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "conf_aurc = curves['Confidence'].aurc\n",
    "for name, curve in curves.items():\n",
    "    better = \"‚úÖ Yes\" if curve.aurc < conf_aurc else (\"- Same\" if curve.aurc == conf_aurc else \"‚ùå No\")\n",
    "    if name == 'Confidence':\n",
    "        better = \"- Baseline\"\n",
    "    print(f\"{name:<15} {curve.aurc:>10.4f} {curve.e_aurc:>10.4f} {better:>20}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot risk-coverage curves\n",
    "fig = plot_risk_coverage_curve(\n",
    "    curves,\n",
    "    title=\"Risk-Coverage Curves: Strategy Comparison\",\n",
    "    save_path=RESULTS_DIR / 'risk_coverage_curves.png',\n",
    "    show_optimal=True,\n",
    "    figsize=(12, 8)\n",
    ")\n",
    "plt.show()\n",
    "print(f\"\\n‚úÖ Saved: {RESULTS_DIR / 'risk_coverage_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d930c08c",
   "metadata": {},
   "source": [
    "## Cell 7: Accuracy-Coverage Curves\n",
    "\n",
    "Generate accuracy-coverage curves showing selective accuracy vs coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 8.5: Accuracy-Coverage Curves\n",
    "\n",
    "Generate accuracy-coverage curves showing how selective accuracy\n",
    "improves as we reduce coverage (accept fewer samples).\n",
    "\"\"\"\n",
    "\n",
    "# Plot accuracy-coverage curves\n",
    "fig = plot_accuracy_coverage_curve(\n",
    "    curves,\n",
    "    title=\"Accuracy-Coverage Curves: Strategy Comparison\",\n",
    "    save_path=RESULTS_DIR / 'accuracy_coverage_curves.png',\n",
    "    figsize=(12, 8)\n",
    ")\n",
    "plt.show()\n",
    "print(f\"\\n‚úÖ Saved: {RESULTS_DIR / 'accuracy_coverage_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1478d64",
   "metadata": {},
   "source": [
    "## Cell 8: Strategy Comparison Visualization\n",
    "\n",
    "Generate bar plots comparing metrics across strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb86c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 8.5: Strategy Comparison Visualization\n",
    "\n",
    "Generate bar plots comparing coverage, selective accuracy,\n",
    "and improvement across gating strategies.\n",
    "\"\"\"\n",
    "\n",
    "# Plot strategy comparison\n",
    "fig = plot_strategy_comparison(\n",
    "    strategy_results,\n",
    "    title=\"Selective Prediction Strategy Comparison\",\n",
    "    save_path=RESULTS_DIR / 'strategy_comparison.png',\n",
    "    figsize=(14, 6)\n",
    ")\n",
    "plt.show()\n",
    "print(f\"\\n‚úÖ Saved: {RESULTS_DIR / 'strategy_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76944b5a",
   "metadata": {},
   "source": [
    "## Cell 9: Coverage Sweep Analysis\n",
    "\n",
    "Analyze metrics across different coverage levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd69eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 8.5: Coverage Sweep Analysis\n",
    "\n",
    "Analyze how selective accuracy and improvement vary\n",
    "across different coverage levels (50% to 100%).\n",
    "\"\"\"\n",
    "\n",
    "# Coverage levels to analyze\n",
    "coverage_levels = [0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 1.00]\n",
    "\n",
    "# Compute metrics at each coverage level\n",
    "sweep_results = []\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COVERAGE SWEEP ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Coverage':>10} {'Sel.Acc':>10} {'Improvement':>12} {'Rej.Precision':>15} {'AURC':>10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for target_cov in coverage_levels:\n",
    "    m = compute_metrics_at_coverage(\n",
    "        predictions, labels, combined_scores,\n",
    "        target_coverage=target_cov,\n",
    "        confidences=confidences\n",
    "    )\n",
    "    sweep_results.append({\n",
    "        'coverage': m.coverage,\n",
    "        'selective_accuracy': m.selective_accuracy,\n",
    "        'improvement': m.improvement,\n",
    "        'rejection_precision': m.rejection_precision,\n",
    "        'aurc': m.aurc\n",
    "    })\n",
    "    \n",
    "    prec_str = f\"{m.rejection_precision:.1%}\" if not np.isnan(m.rejection_precision) else \"N/A\"\n",
    "    print(f\"{m.coverage:>10.1%} {m.selective_accuracy:>10.2%} {m.improvement*100:>+11.2f}pp {prec_str:>15} {m.aurc:>10.4f}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Find coverage where improvement ‚â• 4pp\n",
    "for r in sweep_results:\n",
    "    if r['improvement'] >= 0.04:\n",
    "        print(f\"\\n‚úÖ H3a satisfied at {r['coverage']:.0%} coverage: {r['improvement']*100:.2f}pp improvement\")\n",
    "        break\n",
    "\n",
    "# Plot coverage sweep\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "coverages = [r['coverage'] for r in sweep_results]\n",
    "sel_accs = [r['selective_accuracy'] for r in sweep_results]\n",
    "improvements = [r['improvement'] * 100 for r in sweep_results]\n",
    "aurcs = [r['aurc'] for r in sweep_results]\n",
    "\n",
    "# Plot 1: Selective Accuracy vs Coverage\n",
    "axes[0].plot(coverages, sel_accs, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].axhline(y=overall_accuracy, color='r', linestyle='--', label=f'Overall Acc ({overall_accuracy:.1%})')\n",
    "axes[0].set_xlabel('Coverage')\n",
    "axes[0].set_ylabel('Selective Accuracy')\n",
    "axes[0].set_title('Selective Accuracy vs Coverage')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Improvement vs Coverage\n",
    "axes[1].plot(coverages, improvements, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].axhline(y=4, color='r', linestyle='--', label='H3a Target (4pp)')\n",
    "axes[1].axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "axes[1].set_xlabel('Coverage')\n",
    "axes[1].set_ylabel('Improvement (pp)')\n",
    "axes[1].set_title('Accuracy Improvement vs Coverage')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: AURC vs Coverage\n",
    "axes[2].plot(coverages, aurcs, 'mo-', linewidth=2, markersize=8)\n",
    "axes[2].set_xlabel('Coverage')\n",
    "axes[2].set_ylabel('AURC')\n",
    "axes[2].set_title('AURC vs Coverage')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'coverage_sweep.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\n‚úÖ Saved: {RESULTS_DIR / 'coverage_sweep.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24f25b",
   "metadata": {},
   "source": [
    "## Cell 10: Rejection Analysis\n",
    "\n",
    "Analyze the quality of rejection decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4bfc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 8.5: Rejection Analysis\n",
    "\n",
    "Analyze the quality of rejection decisions:\n",
    "- What fraction of rejected samples are actually errors?\n",
    "- What fraction of errors are successfully rejected?\n",
    "\"\"\"\n",
    "\n",
    "# Rejection analysis\n",
    "precision, recall = compute_rejection_precision_recall(predictions, labels, is_accepted)\n",
    "rejection_quality = compute_rejection_quality(metrics.risk_on_rejected, metrics.selective_risk)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REJECTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Sample Breakdown:\")\n",
    "print(f\"   Total Samples:       {metrics.n_total:,}\")\n",
    "print(f\"   Accepted:            {metrics.n_accepted:,} ({metrics.coverage:.1%})\")\n",
    "print(f\"   Rejected:            {metrics.n_rejected:,} ({1-metrics.coverage:.1%})\")\n",
    "\n",
    "print(f\"\\nüéØ Rejection Quality:\")\n",
    "print(f\"   Correct Rejected (Type I Error):     {metrics.n_correct_rejected:,}\")\n",
    "print(f\"   Incorrect Rejected (Good Rejection): {metrics.n_incorrect_rejected:,}\")\n",
    "print(f\"   Correct Accepted:                    {metrics.n_correct_accepted:,}\")\n",
    "print(f\"   Incorrect Accepted (Missed):         {metrics.n_incorrect_accepted:,}\")\n",
    "\n",
    "print(f\"\\nüìà Rejection Metrics:\")\n",
    "print(f\"   Rejection Precision: {precision:.1%}\")\n",
    "print(f\"      (Fraction of rejected that are errors)\")\n",
    "print(f\"   Rejection Recall:    {recall:.1%}\")\n",
    "print(f\"      (Fraction of errors that are rejected)\")\n",
    "\n",
    "if not np.isnan(rejection_quality):\n",
    "    print(f\"   Rejection Quality:   {rejection_quality:.2f}x\")\n",
    "    print(f\"      (Rejected samples are {rejection_quality:.1f}x more likely to be errors)\")\n",
    "\n",
    "# Visualize rejection quality\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix style plot\n",
    "confusion_data = np.array([\n",
    "    [metrics.n_correct_accepted, metrics.n_incorrect_accepted],\n",
    "    [metrics.n_correct_rejected, metrics.n_incorrect_rejected]\n",
    "])\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_data, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=['Correct', 'Incorrect'],\n",
    "    yticklabels=['Accepted', 'Rejected'],\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_xlabel('Prediction Correctness')\n",
    "axes[0].set_ylabel('Acceptance Decision')\n",
    "axes[0].set_title('Rejection Decision Matrix')\n",
    "\n",
    "# Risk comparison\n",
    "risks = [metrics.selective_risk, metrics.risk_on_rejected]\n",
    "labels_risk = ['Accepted (Selective Risk)', 'Rejected (Risk on Rejected)']\n",
    "colors = ['green', 'red']\n",
    "\n",
    "bars = axes[1].bar(labels_risk, risks, color=colors, alpha=0.7)\n",
    "axes[1].set_ylabel('Error Rate')\n",
    "axes[1].set_title('Error Rate Comparison: Accepted vs Rejected')\n",
    "axes[1].set_ylim(0, max(risks) * 1.2)\n",
    "\n",
    "for bar, risk in zip(bars, risks):\n",
    "    if not np.isnan(risk):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                    f'{risk:.1%}', ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'rejection_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\n‚úÖ Saved: {RESULTS_DIR / 'rejection_analysis.png'}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e00b4b",
   "metadata": {},
   "source": [
    "## Cell 11: Calibration Post-Selection\n",
    "\n",
    "Analyze calibration of accepted predictions using ECE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18762e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 8.5: Calibration Post-Selection\n",
    "\n",
    "Analyze Expected Calibration Error (ECE) on accepted samples\n",
    "to verify confidence reliability after selective prediction.\n",
    "\"\"\"\n",
    "\n",
    "# Compute ECE for different subsets\n",
    "ece_overall = compute_ece_post_selection(\n",
    "    predictions, labels, confidences, np.ones(len(predictions), dtype=bool)\n",
    ")\n",
    "ece_accepted = compute_ece_post_selection(\n",
    "    predictions, labels, confidences, is_accepted\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CALIBRATION ANALYSIS (POST-SELECTION)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìê Expected Calibration Error (ECE):\")\n",
    "print(f\"   Overall (all samples):  {ece_overall:.4f}\")\n",
    "print(f\"   Post-Selection:         {ece_accepted:.4f}\")\n",
    "\n",
    "if ece_accepted < ece_overall:\n",
    "    improvement = (ece_overall - ece_accepted) / ece_overall * 100\n",
    "    print(f\"\\n‚úÖ Calibration improved by {improvement:.1f}% after selection\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Calibration did not improve (may need recalibration)\")\n",
    "\n",
    "# Reliability diagram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (subset_name, mask) in enumerate([('All Samples', np.ones(len(predictions), dtype=bool)), \n",
    "                                            ('Accepted Only', is_accepted)]):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get subset data\n",
    "    sub_preds = predictions[mask]\n",
    "    sub_labels = labels[mask]\n",
    "    sub_confs = confidences[mask]\n",
    "    \n",
    "    # Bin confidences\n",
    "    n_bins = 10\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_accs = []\n",
    "    bin_confs = []\n",
    "    bin_counts = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        in_bin = (sub_confs > bin_boundaries[i]) & (sub_confs <= bin_boundaries[i+1])\n",
    "        if np.sum(in_bin) > 0:\n",
    "            bin_accs.append(np.mean(sub_preds[in_bin] == sub_labels[in_bin]))\n",
    "            bin_confs.append(np.mean(sub_confs[in_bin]))\n",
    "            bin_counts.append(np.sum(in_bin))\n",
    "        else:\n",
    "            bin_accs.append(0)\n",
    "            bin_confs.append((bin_boundaries[i] + bin_boundaries[i+1]) / 2)\n",
    "            bin_counts.append(0)\n",
    "    \n",
    "    # Plot reliability diagram\n",
    "    ax.bar(range(n_bins), bin_accs, width=0.8, alpha=0.5, label='Accuracy')\n",
    "    ax.plot([0, n_bins-1], [0, 1], 'r--', label='Perfect Calibration')\n",
    "    ax.set_xlabel('Confidence Bin')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Reliability Diagram: {subset_name}')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'calibration_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\n‚úÖ Saved: {RESULTS_DIR / 'calibration_analysis.png'}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b1c069",
   "metadata": {},
   "source": [
    "## Cell 12: Export Results\n",
    "\n",
    "Export all metrics and results for dissertation reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5911ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 8.5: Export Results\n",
    "\n",
    "Export all computed metrics and validation results for\n",
    "dissertation Chapter 7 and publication.\n",
    "\"\"\"\n",
    "\n",
    "# Export main metrics to JSON\n",
    "metrics_path = RESULTS_DIR / 'selective_metrics.json'\n",
    "metrics.to_json(metrics_path)\n",
    "print(f\"‚úÖ Saved: {metrics_path}\")\n",
    "\n",
    "# Export H3a validation results\n",
    "h3a_path = RESULTS_DIR / 'h3a_validation.json'\n",
    "with open(h3a_path, 'w') as f:\n",
    "    json.dump(h3a_result, f, indent=2, default=str)\n",
    "print(f\"‚úÖ Saved: {h3a_path}\")\n",
    "\n",
    "# Export strategy comparison\n",
    "strategy_data = {\n",
    "    name: m.to_dict() for name, m in strategy_results.items()\n",
    "}\n",
    "strategy_path = RESULTS_DIR / 'strategy_comparison.json'\n",
    "with open(strategy_path, 'w') as f:\n",
    "    json.dump(strategy_data, f, indent=2)\n",
    "print(f\"‚úÖ Saved: {strategy_path}\")\n",
    "\n",
    "# Export coverage sweep\n",
    "sweep_df = pd.DataFrame(sweep_results)\n",
    "sweep_path = RESULTS_DIR / 'coverage_sweep.csv'\n",
    "sweep_df.to_csv(sweep_path, index=False)\n",
    "print(f\"‚úÖ Saved: {sweep_path}\")\n",
    "\n",
    "# Export risk-coverage curve data\n",
    "rc_data = {name: curve.to_dict() for name, curve in curves.items()}\n",
    "rc_path = RESULTS_DIR / 'risk_coverage_curves.json'\n",
    "with open(rc_path, 'w') as f:\n",
    "    json.dump(rc_data, f, indent=2)\n",
    "print(f\"‚úÖ Saved: {rc_path}\")\n",
    "\n",
    "# Summary table for dissertation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DISSERTATION SUMMARY TABLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "| Metric | Value | Target | Status |\n",
    "|--------|-------|--------|--------|\n",
    "| Coverage | {metrics.coverage:.1%} | ‚â•90% | {'‚úÖ' if metrics.coverage >= 0.90 else '‚ùå'} |\n",
    "| Selective Accuracy | {metrics.selective_accuracy:.2%} | - | - |\n",
    "| Overall Accuracy | {metrics.overall_accuracy:.2%} | - | - |\n",
    "| Improvement | {metrics.improvement*100:+.2f}pp | ‚â•4pp | {'‚úÖ' if metrics.improvement >= 0.04 else '‚ùå'} |\n",
    "| AURC | {metrics.aurc:.4f} | Lower | - |\n",
    "| E-AURC | {metrics.e_aurc:.4f} | ‚âà0 | {'‚úÖ' if metrics.e_aurc < 0.01 else '‚ùå'} |\n",
    "| Rejection Precision | {metrics.rejection_precision:.1%} | High | {'‚úÖ' if metrics.rejection_precision >= 0.8 else '‚ö†Ô∏è'} |\n",
    "| ECE Post-Selection | {metrics.ece_post_selection:.4f} | Low | {'‚úÖ' if metrics.ece_post_selection < 0.1 else '‚ö†Ô∏è'} |\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d89f184",
   "metadata": {},
   "source": [
    "## Cell 13: Final Summary\n",
    "\n",
    "Complete summary of Phase 8.5 evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 8.5: Final Summary\n",
    "\n",
    "Complete summary of all Phase 8.5 selective metrics evaluation.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 8.5: SELECTIVE PREDICTION EVALUATION METRICS - COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä CORE METRICS\n",
    "   Coverage:            {metrics.coverage:.1%}\n",
    "   Selective Accuracy:  {metrics.selective_accuracy:.2%}\n",
    "   Overall Accuracy:    {metrics.overall_accuracy:.2%}\n",
    "   Improvement:         {metrics.improvement*100:+.2f}pp\n",
    "\n",
    "üìà HYPOTHESIS VALIDATION\n",
    "   H3a (‚â•4pp at 90%):   {'‚úÖ PASSED' if h3a_result['passed'] else '‚ùå NOT PASSED'}\n",
    "   H3b (Combined > Single): {'‚úÖ SUPPORTED' if combined_score.aurc <= min(conf_only.aurc, stab_only.aurc) else '‚ö†Ô∏è MIXED'}\n",
    "\n",
    "üéØ REJECTION QUALITY\n",
    "   Precision:           {metrics.rejection_precision:.1%}\n",
    "   Recall:              {metrics.rejection_recall:.1%}\n",
    "   AURC:                {metrics.aurc:.4f}\n",
    "   E-AURC:              {metrics.e_aurc:.4f}\n",
    "\n",
    "üìê CALIBRATION\n",
    "   ECE Post-Selection:  {metrics.ece_post_selection:.4f}\n",
    "\n",
    "üìÅ OUTPUTS SAVED\n",
    "   {RESULTS_DIR / 'selective_metrics.json'}\n",
    "   {RESULTS_DIR / 'h3a_validation.json'}\n",
    "   {RESULTS_DIR / 'strategy_comparison.json'}\n",
    "   {RESULTS_DIR / 'coverage_sweep.csv'}\n",
    "   {RESULTS_DIR / 'risk_coverage_curves.json'}\n",
    "   {RESULTS_DIR / 'risk_coverage_curves.png'}\n",
    "   {RESULTS_DIR / 'accuracy_coverage_curves.png'}\n",
    "   {RESULTS_DIR / 'strategy_comparison.png'}\n",
    "   {RESULTS_DIR / 'coverage_sweep.png'}\n",
    "   {RESULTS_DIR / 'rejection_analysis.png'}\n",
    "   {RESULTS_DIR / 'calibration_analysis.png'}\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ PHASE 8.5 COMPLETE - Ready for Dissertation Chapter 7\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
