{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ff9043",
   "metadata": {},
   "source": [
    "# ADVERSARIAL EXAMPLES VISUALIZATION - PHASE 4.3/4.4\n",
    "===================================================\n",
    "\n",
    "### Publication-quality visualization and analysis of adversarial examples on medical imaging data (ISIC 2018).\n",
    "\n",
    "\n",
    "## Objectives:\n",
    "-----------\n",
    "1. Visualize clean vs adversarial images (FGSM, PGD, C&W)\n",
    "2. Quantify perturbation characteristics (L∞, L2, SSIM)\n",
    "3. Analyze model predictions and confidence changes\n",
    "4. Class-wise vulnerability analysis\n",
    "5. Transferability visualization (ResNet vs EfficientNet)\n",
    "\n",
    "## Expected Outputs:\n",
    "-----------------\n",
    "- Figure 1: Clean vs Adversarial Grid (6×4 layout)\n",
    "- Figure 2: Perturbation Heatmaps (amplified ×10)\n",
    "- Figure 3: Prediction Confidence Changes\n",
    "- Figure 4: Class-wise Attack Success Rates\n",
    "- Figure 5: Transferability Patterns\n",
    "\n",
    "### Author: Viraj Pankaj Jain\n",
    "### Date: November 24, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82630334",
   "metadata": {},
   "source": [
    "## SECTION 1: Setup and Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85918c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# Styling for publication-quality plots\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Project imports\n",
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.attacks.fgsm import FGSM, FGSMConfig\n",
    "from src.attacks.pgd import PGD, PGDConfig\n",
    "from src.models.build import build_model\n",
    "from src.datasets.isic import ISICDataset\n",
    "from src.datasets.transforms import get_test_transforms\n",
    "\n",
    "print(\"✅ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6dc044",
   "metadata": {},
   "source": [
    "## SECTION 2: Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765af4ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load ResNet-50 baseline (seed 42)\n",
    "resnet_checkpoint = PROJECT_ROOT / \"checkpoints\" / \"baseline\" / \"seed_42\" / \"best.pt\"\n",
    "resnet_model = build_model(\"resnet50\", num_classes=7, pretrained=False)\n",
    "resnet_model.load_state_dict(torch.load(resnet_checkpoint)[\"model_state_dict\"])\n",
    "resnet_model.to(device)\n",
    "resnet_model.eval()\n",
    "print(\"✅ ResNet-50 loaded\")\n",
    "\n",
    "# Load test data\n",
    "data_root = PROJECT_ROOT / \"data\" / \"processed\" / \"isic2018\"\n",
    "test_dataset = ISICDataset(\n",
    "    root=str(data_root),\n",
    "    split=\"test\",\n",
    "    csv_path=str(data_root / \"metadata_processed.csv\"),\n",
    "    transforms=get_test_transforms(\"isic\", 224),\n",
    ")\n",
    "\n",
    "# Get class names\n",
    "class_names = [\n",
    "    \"Melanoma\", \"Melanocytic nevus\", \"Basal cell carcinoma\",\n",
    "    \"Actinic keratosis\", \"Benign keratosis\", \"Dermatofibroma\", \"Vascular lesion\"\n",
    "]\n",
    "\n",
    "print(f\"✅ Dataset loaded: {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e79c43",
   "metadata": {},
   "source": [
    "## SECTION 4: Visualization - Clean vs Adversarial Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157eb98",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"Denormalize ImageNet-normalized tensor.\"\"\"\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return (tensor * std + mean).clamp(0, 1)\n",
    "\n",
    "fig, axes = plt.subplots(7, 3, figsize=(12, 20))\n",
    "fig.suptitle(\"Clean vs Adversarial Examples (ISIC 2018)\", fontsize=16, fontweight='bold')\n",
    "\n",
    "for cls_idx, (cls, advs) in enumerate(all_adversarials.items()):\n",
    "    # Clean\n",
    "    clean_img = denormalize(advs[\"clean\"].squeeze(0).cpu())\n",
    "    axes[cls_idx, 0].imshow(clean_img.permute(1, 2, 0))\n",
    "    axes[cls_idx, 0].set_title(f\"{class_names[cls]}\\n(Clean)\")\n",
    "    axes[cls_idx, 0].axis('off')\n",
    "    \n",
    "    # FGSM\n",
    "    fgsm_img = denormalize(advs[\"fgsm\"].squeeze(0).cpu())\n",
    "    axes[cls_idx, 1].imshow(fgsm_img.permute(1, 2, 0))\n",
    "    axes[cls_idx, 1].set_title(\"FGSM (ε=8/255)\")\n",
    "    axes[cls_idx, 1].axis('off')\n",
    "    \n",
    "    # PGD\n",
    "    pgd_img = denormalize(advs[\"pgd\"].squeeze(0).cpu())\n",
    "    axes[cls_idx, 2].imshow(pgd_img.permute(1, 2, 0))\n",
    "    axes[cls_idx, 2].set_title(\"PGD (ε=8/255, 10 steps)\")\n",
    "    axes[cls_idx, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_ROOT / \"results\" / \"figures\" / \"adversarial_grid.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Figure 1 saved: adversarial_grid.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed8e7a",
   "metadata": {},
   "source": [
    "## SECTION 5: Perturbation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05888ed0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def compute_perturbation_metrics(clean, adv):\n",
    "    \"\"\"Compute perturbation statistics.\"\"\"\n",
    "    pert = (adv - clean).abs()\n",
    "    \n",
    "    metrics = {\n",
    "        \"l_inf\": pert.max().item(),\n",
    "        \"l_2\": pert.norm(p=2).item() / np.sqrt(pert.numel()),\n",
    "        \"l_1\": pert.norm(p=1).item() / pert.numel(),\n",
    "        \"mean\": pert.mean().item(),\n",
    "        \"std\": pert.std().item(),\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Compute for all attacks\n",
    "perturbation_stats = {attack: [] for attack in [\"fgsm\", \"pgd\"]}\n",
    "\n",
    "for cls, advs in all_adversarials.items():\n",
    "    for attack_name in [\"fgsm\", \"pgd\"]:\n",
    "        metrics = compute_perturbation_metrics(advs[\"clean\"], advs[attack_name])\n",
    "        perturbation_stats[attack_name].append(metrics)\n",
    "\n",
    "# Visualize perturbation statistics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle(\"Perturbation Magnitude Analysis\", fontsize=14, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = [\"l_inf\", \"l_2\", \"mean\"]\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    data = {\n",
    "        \"FGSM\": [s[metric] for s in perturbation_stats[\"fgsm\"]],\n",
    "        \"PGD\": [s[metric] for s in perturbation_stats[\"pgd\"]],\n",
    "    }\n",
    "    \n",
    "    axes[idx].bar(data.keys(), [np.mean(v) for v in data.values()])\n",
    "    axes[idx].set_ylabel(f\"{metric.upper()} Norm\")\n",
    "    axes[idx].set_title(f\"{metric.upper()} Perturbation\")\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_ROOT / \"results\" / \"figures\" / \"perturbation_analysis.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Figure 2 saved: perturbation_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeeec95",
   "metadata": {},
   "source": [
    "## SECTION 6: Prediction Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10364b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_prediction_confidence(model, image, true_label):\n",
    "    \"\"\"Get model prediction and confidence.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(image)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        pred_label = logits.argmax(dim=1).item()\n",
    "        pred_conf = probs[0, pred_label].item()\n",
    "        true_conf = probs[0, true_label].item()\n",
    "    \n",
    "    return {\n",
    "        \"predicted_label\": pred_label,\n",
    "        \"predicted_confidence\": pred_conf,\n",
    "        \"true_label_confidence\": true_conf,\n",
    "        \"correct\": pred_label == true_label,\n",
    "    }\n",
    "\n",
    "# Analyze confidence for all samples\n",
    "confidence_analysis = {}\n",
    "for cls, advs in all_adversarials.items():\n",
    "    confidence_analysis[cls] = {}\n",
    "    \n",
    "    for attack_name, adv_img in advs.items():\n",
    "        conf = get_prediction_confidence(resnet_model, adv_img, cls)\n",
    "        confidence_analysis[cls][attack_name] = conf\n",
    "\n",
    "# Visualize confidence changes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle(\"Model Confidence Analysis\", fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 1: Confidence on true class\n",
    "for attack_name in [\"clean\", \"fgsm\", \"pgd\"]:\n",
    "    confs = [confidence_analysis[cls][attack_name][\"true_label_confidence\"] \n",
    "             for cls in range(7)]\n",
    "    axes[0].plot(class_names, confs, marker='o', label=attack_name.upper())\n",
    "\n",
    "axes[0].set_ylabel(\"Confidence on True Class\")\n",
    "axes[0].set_xlabel(\"Class\")\n",
    "axes[0].set_title(\"Confidence Drop Under Attack\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Attack success rate\n",
    "success_rates = {}\n",
    "for attack_name in [\"fgsm\", \"pgd\"]:\n",
    "    successes = [1 - int(confidence_analysis[cls][attack_name][\"correct\"]) \n",
    "                 for cls in range(7)]\n",
    "    success_rates[attack_name] = np.mean(successes) * 100\n",
    "\n",
    "axes[1].bar(success_rates.keys(), success_rates.values())\n",
    "axes[1].set_ylabel(\"Attack Success Rate (%)\")\n",
    "axes[1].set_title(\"Overall Attack Success\")\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_ROOT / \"results\" / \"figures\" / \"confidence_analysis.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Figure 3 saved: confidence_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b41cc5",
   "metadata": {},
   "source": [
    "## SECTION 7: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867a10a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADVERSARIAL EXAMPLES ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nPerturbation Statistics:\")\n",
    "for attack_name in [\"fgsm\", \"pgd\"]:\n",
    "    stats = perturbation_stats[attack_name]\n",
    "    l_inf_mean = np.mean([s[\"l_inf\"] for s in stats])\n",
    "    l_2_mean = np.mean([s[\"l_2\"] for s in stats])\n",
    "    print(f\"\\n{attack_name.upper()}:\")\n",
    "    print(f\"  L∞ norm: {l_inf_mean:.4f}\")\n",
    "    print(f\"  L2 norm: {l_2_mean:.4f}\")\n",
    "\n",
    "print(\"\\nAttack Success Rates:\")\n",
    "for attack_name in [\"fgsm\", \"pgd\"]:\n",
    "    successes = [1 - int(confidence_analysis[cls][attack_name][\"correct\"]) \n",
    "                 for cls in range(7)]\n",
    "    asr = np.mean(successes) * 100\n",
    "    print(f\"  {attack_name.upper()}: {asr:.1f}%\")\n",
    "\n",
    "print(\"\\nConfidence Changes:\")\n",
    "for attack_name in [\"clean\", \"fgsm\", \"pgd\"]:\n",
    "    confs = [confidence_analysis[cls][attack_name][\"true_label_confidence\"] \n",
    "             for cls in range(7)]\n",
    "    mean_conf = np.mean(confs) * 100\n",
    "    print(f\"  {attack_name.upper()}: {mean_conf:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Analysis complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
