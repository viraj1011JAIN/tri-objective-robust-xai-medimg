{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db4ca0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 3: BASELINE TRAINING SETUP\n",
      "======================================================================\n",
      "‚úÖ Local environment detected\n",
      "üìÅ Project root: c:\\Users\\Dissertation\\tri-objective-robust-xai-medimg\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 3: BASELINE TRAINING SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Mount Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Google Colab detected, Drive mounted\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚úÖ Local environment detected\")\n",
    "\n",
    "# Clone/update repository\n",
    "if IN_COLAB:\n",
    "    REPO_PATH = Path('/content/tri-objective-robust-xai-medimg')\n",
    "    if not REPO_PATH.exists():\n",
    "        !git clone https://github.com/viraj1011JAIN/tri-objective-robust-xai-medimg.git {REPO_PATH}\n",
    "        print(\"‚úÖ Repository cloned\")\n",
    "    else:\n",
    "        os.chdir(REPO_PATH)\n",
    "        !git pull origin main\n",
    "        print(\"‚úÖ Repository updated\")\n",
    "    \n",
    "    os.chdir(REPO_PATH)\n",
    "    sys.path.insert(0, str(REPO_PATH))\n",
    "    PROJECT_ROOT = REPO_PATH\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd().parent\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"üìÅ Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef21946f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: INSTALL DEPENDENCIES\n",
    "# ============================================================================\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q timm albumentations scikit-learn pandas matplotlib seaborn tqdm mlflow\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ea83b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "   Memory: 4.3 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: IMPORTS\n",
    "# ============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Albumentations for transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Project imports\n",
    "from src.datasets.isic import ISICDataset\n",
    "from src.models.build import build_model\n",
    "from src.utils.reproducibility import set_global_seed\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14823d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURATION\n",
      "======================================================================\n",
      "üìä Model: resnet50\n",
      "üìä Classes: 7\n",
      "üìä Epochs: 30\n",
      "üìä Batch size: 32\n",
      "üìä Seeds: [42, 123, 456]\n",
      "üìÅ Data: \\content\\drive\\MyDrive\\data\\data\\isic_2018\n",
      "üìÅ Checkpoints: \\content\\drive\\MyDrive\\checkpoints\\baseline\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: CONFIGURATION (OPTIMIZED FOR A100 40GB)\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFIGURATION (A100 OPTIMIZED)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Enable TF32 for faster matrix operations on A100\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True  # Auto-tune convolution algorithms\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths (Google Drive)\n",
    "    'data_root': Path('/content/drive/MyDrive/data/data/isic_2018'),\n",
    "    'checkpoint_dir': Path('/content/drive/MyDrive/checkpoints/baseline'),\n",
    "    'results_dir': Path('/content/drive/MyDrive/results/phase3'),\n",
    "    \n",
    "    # Model\n",
    "    'model_name': 'resnet50',\n",
    "    'num_classes': 7,\n",
    "    'pretrained': True,\n",
    "    \n",
    "    # Training (OPTIMIZED for A100 40GB)\n",
    "    'epochs': 30,\n",
    "    'batch_size': 128,          # Increased from 32 ‚Üí 128 for A100\n",
    "    'learning_rate': 3e-4,      # Scaled up with larger batch\n",
    "    'weight_decay': 1e-4,\n",
    "    'num_workers': 4,           # More workers for faster data loading\n",
    "    'pin_memory': True,\n",
    "    'use_amp': True,            # Mixed precision training (FP16)\n",
    "    \n",
    "    # Image\n",
    "    'image_size': 224,\n",
    "    \n",
    "    # Seeds for reproducibility\n",
    "    'seeds': [42, 123, 456],\n",
    "    \n",
    "    # Class names\n",
    "    'class_names': ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC'],\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "CONFIG['checkpoint_dir'].mkdir(parents=True, exist_ok=True)\n",
    "CONFIG['results_dir'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for seed in CONFIG['seeds']:\n",
    "    (CONFIG['checkpoint_dir'] / f'seed_{seed}').mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìä Model: {CONFIG['model_name']}\")\n",
    "print(f\"üìä Classes: {CONFIG['num_classes']}\")\n",
    "print(f\"üìä Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"üìä Batch size: {CONFIG['batch_size']} (optimized for A100)\")\n",
    "print(f\"üìä Mixed Precision (AMP): {CONFIG['use_amp']}\")\n",
    "print(f\"üìä Seeds: {CONFIG['seeds']}\")\n",
    "print(f\"üìÅ Data: {CONFIG['data_root']}\")\n",
    "print(f\"üìÅ Checkpoints: {CONFIG['checkpoint_dir']}\")\n",
    "\n",
    "# Show GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\\nüéÆ GPU Memory: {total_mem:.1f} GB\")\n",
    "    print(f\"üöÄ TF32 enabled: {torch.backends.cuda.matmul.allow_tf32}\")\n",
    "    print(f\"üöÄ cuDNN benchmark: {torch.backends.cudnn.benchmark}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01c44444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA PREPARATION\n",
      "======================================================================\n",
      "üìÑ Loading metadata: \\content\\drive\\MyDrive\\data\\data\\isic_2018\\metadata.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\content\\\\drive\\\\MyDrive\\\\data\\\\data\\\\isic_2018\\\\metadata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m metadata_path = CONFIG[\u001b[33m'\u001b[39m\u001b[33mdata_root\u001b[39m\u001b[33m'\u001b[39m] / \u001b[33m'\u001b[39m\u001b[33mmetadata.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìÑ Loading metadata: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Total samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Convert backslashes to forward slashes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dissertation\\tri-objective-robust-xai-medimg\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dissertation\\tri-objective-robust-xai-medimg\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dissertation\\tri-objective-robust-xai-medimg\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dissertation\\tri-objective-robust-xai-medimg\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dissertation\\tri-objective-robust-xai-medimg\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '\\\\content\\\\drive\\\\MyDrive\\\\data\\\\data\\\\isic_2018\\\\metadata.csv'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: DATA PREPARATION\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Fix metadata paths (Windows backslashes ‚Üí forward slashes)\n",
    "metadata_path = CONFIG['data_root'] / 'metadata.csv'\n",
    "print(f\"üìÑ Loading metadata: {metadata_path}\")\n",
    "\n",
    "df = pd.read_csv(metadata_path)\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "\n",
    "# Convert backslashes to forward slashes\n",
    "if 'image_path' in df.columns:\n",
    "    df['image_path'] = df['image_path'].str.replace('\\\\', '/', regex=False)\n",
    "    print(\"   ‚úÖ Fixed path separators\")\n",
    "\n",
    "# Save fixed metadata\n",
    "fixed_path = CONFIG['data_root'] / 'metadata_fixed.csv'\n",
    "df.to_csv(fixed_path, index=False)\n",
    "print(f\"   ‚úÖ Saved to: {fixed_path}\")\n",
    "\n",
    "# Show split distribution\n",
    "print(f\"\\nüìä Split Distribution:\")\n",
    "print(df['split'].value_counts())\n",
    "\n",
    "# Show class distribution\n",
    "print(f\"\\nüìä Class Distribution:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: CREATE TRANSFORMS & DATASETS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"CREATING DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Training transforms (with augmentation)\n",
    "train_transforms = A.Compose([\n",
    "    A.Resize(CONFIG['image_size'], CONFIG['image_size']),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation)\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(CONFIG['image_size'], CONFIG['image_size']),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ISICDataset(\n",
    "    root=str(CONFIG['data_root']),\n",
    "    split='train',\n",
    "    transforms=train_transforms,\n",
    "    csv_path=str(fixed_path),\n",
    "    image_column='image_path',\n",
    "    label_column='label'\n",
    ")\n",
    "\n",
    "val_dataset = ISICDataset(\n",
    "    root=str(CONFIG['data_root']),\n",
    "    split='val',\n",
    "    transforms=val_transforms,\n",
    "    csv_path=str(fixed_path),\n",
    "    image_column='image_path',\n",
    "    label_column='label'\n",
    ")\n",
    "\n",
    "test_dataset = ISICDataset(\n",
    "    root=str(CONFIG['data_root']),\n",
    "    split='test',\n",
    "    transforms=val_transforms,\n",
    "    csv_path=str(fixed_path),\n",
    "    image_column='image_path',\n",
    "    label_column='label'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train samples: {len(train_dataset)}\")\n",
    "print(f\"‚úÖ Val samples: {len(val_dataset)}\")\n",
    "print(f\"‚úÖ Test samples: {len(test_dataset)}\")\n",
    "print(f\"‚úÖ Classes: {train_dataset.class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: TRAINING FUNCTIONS (WITH MIXED PRECISION)\n",
    "# ============================================================================\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score, f1_score\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, scaler=None, use_amp=True):\n",
    "    \"\"\"Train for one epoch with optional mixed precision.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training', leave=False)\n",
    "    for batch in pbar:\n",
    "        # Handle (images, labels, meta) format\n",
    "        if len(batch) == 3:\n",
    "            images, labels, _ = batch\n",
    "        else:\n",
    "            images, labels = batch\n",
    "            \n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)  # Faster than zero_grad()\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with autocast(enabled=use_amp):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Mixed precision backward pass\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds) * 100\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, use_amp=True):\n",
    "    \"\"\"Evaluate model with optional mixed precision.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating', leave=False):\n",
    "            if len(batch) == 3:\n",
    "                images, labels, _ = batch\n",
    "            else:\n",
    "                images, labels = batch\n",
    "                \n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            with autocast(enabled=use_amp):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            probs = torch.softmax(outputs.float(), dim=1)  # Convert to float32 for softmax\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': epoch_loss,\n",
    "        'accuracy': accuracy_score(all_labels, all_preds) * 100,\n",
    "        'balanced_accuracy': balanced_accuracy_score(all_labels, all_preds) * 100,\n",
    "        'f1_macro': f1_score(all_labels, all_preds, average='macro') * 100,\n",
    "    }\n",
    "    \n",
    "    # AUROC (one-vs-rest)\n",
    "    try:\n",
    "        metrics['auroc'] = roc_auc_score(all_labels, all_probs, multi_class='ovr') * 100\n",
    "    except:\n",
    "        metrics['auroc'] = 0.0\n",
    "    \n",
    "    return metrics, all_probs, all_labels, all_preds\n",
    "\n",
    "print(\"‚úÖ Training functions defined (with Mixed Precision support)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b85c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: MAIN TRAINING LOOP (OPTIMIZED FOR A100)\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"BASELINE TRAINING - ALL SEEDS (A100 OPTIMIZED)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_seed_results = {}\n",
    "training_history = {}\n",
    "\n",
    "# Initialize GradScaler for mixed precision\n",
    "scaler = GradScaler() if CONFIG['use_amp'] else None\n",
    "print(f\"üöÄ Mixed Precision (AMP): {'Enabled' if CONFIG['use_amp'] else 'Disabled'}\")\n",
    "\n",
    "for seed_idx, seed in enumerate(CONFIG['seeds']):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SEED {seed} ({seed_idx+1}/{len(CONFIG['seeds'])})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    set_global_seed(seed)\n",
    "    \n",
    "    # Clear GPU cache before each seed\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Create data loaders (optimized)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True, \n",
    "        num_workers=CONFIG['num_workers'], \n",
    "        pin_memory=CONFIG['pin_memory'],\n",
    "        persistent_workers=True,  # Keep workers alive between epochs\n",
    "        prefetch_factor=2         # Prefetch batches\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=CONFIG['batch_size'] * 2,  # Larger batch for eval (no gradients)\n",
    "        shuffle=False, \n",
    "        num_workers=CONFIG['num_workers'], \n",
    "        pin_memory=CONFIG['pin_memory'],\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=CONFIG['batch_size'] * 2,\n",
    "        shuffle=False, \n",
    "        num_workers=CONFIG['num_workers'], \n",
    "        pin_memory=CONFIG['pin_memory']\n",
    "    )\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model(\n",
    "        architecture=CONFIG['model_name'],\n",
    "        num_classes=CONFIG['num_classes'],\n",
    "        pretrained=CONFIG['pretrained']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Compile model for faster execution (PyTorch 2.0+)\n",
    "    if hasattr(torch, 'compile'):\n",
    "        try:\n",
    "            model = torch.compile(model, mode='reduce-overhead')\n",
    "            print(\"   üöÄ Model compiled with torch.compile()\")\n",
    "        except:\n",
    "            print(\"   ‚ö†Ô∏è torch.compile() not available, using eager mode\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=CONFIG['epochs'], eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Reset scaler for each seed\n",
    "    if CONFIG['use_amp']:\n",
    "        scaler = GradScaler()\n",
    "    \n",
    "    # Training history\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'val_auroc': []}\n",
    "    best_val_auroc = 0.0\n",
    "    \n",
    "    print(f\"\\nüìä Training for {CONFIG['epochs']} epochs...\")\n",
    "    print(f\"   Batch size: {CONFIG['batch_size']} | LR: {CONFIG['learning_rate']}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        # Train with mixed precision\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, \n",
    "            scaler=scaler, use_amp=CONFIG['use_amp']\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics, _, _, _ = evaluate(\n",
    "            model, val_loader, criterion, device, \n",
    "            use_amp=CONFIG['use_amp']\n",
    "        )\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_acc'].append(val_metrics['accuracy'])\n",
    "        history['val_auroc'].append(val_metrics['auroc'])\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['auroc'] > best_val_auroc:\n",
    "            best_val_auroc = val_metrics['auroc']\n",
    "            checkpoint_path = CONFIG['checkpoint_dir'] / f'seed_{seed}' / 'best.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_auroc': best_val_auroc,\n",
    "                'config': {k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()},\n",
    "            }, checkpoint_path)\n",
    "        \n",
    "        # Print progress every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            gpu_mem = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "            print(f\"  Epoch {epoch+1:2d}/{CONFIG['epochs']} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.1f}% | \"\n",
    "                  f\"Val Acc: {val_metrics['accuracy']:.1f}% | Val AUROC: {val_metrics['auroc']:.1f}% | \"\n",
    "                  f\"GPU: {gpu_mem:.1f}GB\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    samples_per_sec = len(train_dataset) * CONFIG['epochs'] / elapsed\n",
    "    print(f\"\\n‚è±Ô∏è  Training time: {elapsed/60:.1f} minutes ({samples_per_sec:.0f} samples/sec)\")\n",
    "    \n",
    "    # Final test evaluation\n",
    "    print(f\"\\nüìä Final Test Evaluation...\")\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    test_metrics, test_probs, test_labels, test_preds = evaluate(\n",
    "        model, test_loader, criterion, device, use_amp=CONFIG['use_amp']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ SEED {seed} RESULTS:\")\n",
    "    print(f\"   Test Accuracy: {test_metrics['accuracy']:.2f}%\")\n",
    "    print(f\"   Test Balanced Acc: {test_metrics['balanced_accuracy']:.2f}%\")\n",
    "    print(f\"   Test AUROC: {test_metrics['auroc']:.2f}%\")\n",
    "    print(f\"   Test F1 (macro): {test_metrics['f1_macro']:.2f}%\")\n",
    "    print(f\"   Checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    # Store results\n",
    "    all_seed_results[seed] = test_metrics\n",
    "    training_history[seed] = history\n",
    "    \n",
    "    # Free memory\n",
    "    del model, optimizer, scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ALL SEEDS COMPLETE\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b926c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: AGGREGATE RESULTS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"AGGREGATED RESULTS (MEAN ¬± STD)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Aggregate across seeds\n",
    "metrics_list = ['accuracy', 'balanced_accuracy', 'auroc', 'f1_macro']\n",
    "\n",
    "print(\"\\nüìä Test Set Performance:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "aggregated = {}\n",
    "for metric in metrics_list:\n",
    "    values = [all_seed_results[seed][metric] for seed in CONFIG['seeds']]\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values)\n",
    "    aggregated[metric] = {'mean': mean_val, 'std': std_val}\n",
    "    print(f\"   {metric:20s}: {mean_val:.2f}% ¬± {std_val:.2f}%\")\n",
    "\n",
    "print(\"\\nüìä Per-Seed Results:\")\n",
    "print(\"-\" * 50)\n",
    "for seed in CONFIG['seeds']:\n",
    "    r = all_seed_results[seed]\n",
    "    print(f\"   Seed {seed}: Acc={r['accuracy']:.1f}%, AUROC={r['auroc']:.1f}%\")\n",
    "\n",
    "# Save results\n",
    "results_file = CONFIG['results_dir'] / 'baseline_results.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump({\n",
    "        'per_seed': {str(k): v for k, v in all_seed_results.items()},\n",
    "        'aggregated': aggregated,\n",
    "        'config': {k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items()}\n",
    "    }, f, indent=2)\n",
    "print(f\"\\n‚úÖ Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: VISUALIZATION - TRAINING CURVES\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING VISUALIZATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "ax = axes[0, 0]\n",
    "for i, seed in enumerate(CONFIG['seeds']):\n",
    "    ax.plot(training_history[seed]['train_loss'], label=f'Seed {seed}', color=colors[i], linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Training Loss', fontsize=12)\n",
    "ax.set_title('Training Loss Curves', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Validation Loss\n",
    "ax = axes[0, 1]\n",
    "for i, seed in enumerate(CONFIG['seeds']):\n",
    "    ax.plot(training_history[seed]['val_loss'], label=f'Seed {seed}', color=colors[i], linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax.set_title('Validation Loss Curves', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Validation Accuracy\n",
    "ax = axes[1, 0]\n",
    "for i, seed in enumerate(CONFIG['seeds']):\n",
    "    ax.plot(training_history[seed]['val_acc'], label=f'Seed {seed}', color=colors[i], linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Validation Accuracy Curves', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Validation AUROC\n",
    "ax = axes[1, 1]\n",
    "for i, seed in enumerate(CONFIG['seeds']):\n",
    "    ax.plot(training_history[seed]['val_auroc'], label=f'Seed {seed}', color=colors[i], linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Validation AUROC (%)', fontsize=12)\n",
    "ax.set_title('Validation AUROC Curves', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ResNet-50 Baseline Training on ISIC 2018\\n(3 Seeds for Statistical Robustness)', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "fig_path = CONFIG['results_dir'] / 'training_curves.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {fig_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ec3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: VISUALIZATION - FINAL RESULTS BAR CHART\n",
    "# ============================================================================\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "metrics = ['Accuracy', 'Balanced Acc', 'AUROC', 'F1 (macro)']\n",
    "metric_keys = ['accuracy', 'balanced_accuracy', 'auroc', 'f1_macro']\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "for i, seed in enumerate(CONFIG['seeds']):\n",
    "    values = [all_seed_results[seed][k] for k in metric_keys]\n",
    "    bars = ax.bar(x + i*width, values, width, label=f'Seed {seed}', color=colors[i], alpha=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.annotate(f'{val:.1f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                   xytext=(0, 3), textcoords='offset points', ha='center', fontsize=9)\n",
    "\n",
    "ax.set_ylabel('Score (%)', fontsize=12)\n",
    "ax.set_title('Baseline Model Performance by Seed\\nISIC 2018 Test Set', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(metrics, fontsize=11)\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = CONFIG['results_dir'] / 'seed_comparison.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úÖ Saved: {fig_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32281f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 3 COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä BASELINE MODEL PERFORMANCE (Mean ¬± Std):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   Accuracy:      {aggregated['accuracy']['mean']:.2f}% ¬± {aggregated['accuracy']['std']:.2f}%\")\n",
    "print(f\"   Balanced Acc:  {aggregated['balanced_accuracy']['mean']:.2f}% ¬± {aggregated['balanced_accuracy']['std']:.2f}%\")\n",
    "print(f\"   AUROC:         {aggregated['auroc']['mean']:.2f}% ¬± {aggregated['auroc']['std']:.2f}%\")\n",
    "print(f\"   F1 (macro):    {aggregated['f1_macro']['mean']:.2f}% ¬± {aggregated['f1_macro']['std']:.2f}%\")\n",
    "\n",
    "print(\"\\nüìÅ SAVED CHECKPOINTS:\")\n",
    "print(\"-\" * 50)\n",
    "for seed in CONFIG['seeds']:\n",
    "    ckpt_path = CONFIG['checkpoint_dir'] / f'seed_{seed}' / 'best.pt'\n",
    "    if ckpt_path.exists():\n",
    "        size_mb = ckpt_path.stat().st_size / (1024*1024)\n",
    "        print(f\"   ‚úÖ seed_{seed}/best.pt ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå seed_{seed}/best.pt - NOT FOUND\")\n",
    "\n",
    "print(\"\\nüéØ NEXT STEPS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"   1. Run Phase 4 notebook for adversarial robustness evaluation\")\n",
    "print(\"   2. Use these checkpoints as baseline comparison\")\n",
    "print(\"   3. Proceed to Phase 5 tri-objective robust training\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ PHASE 3 BASELINE TRAINING COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
