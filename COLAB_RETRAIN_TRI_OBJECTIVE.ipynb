{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff510e9d",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for data and checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Clone latest repository with fix\n",
    "!rm -rf tri-objective-robust-xai-medimg\n",
    "!git clone https://github.com/viraj1011JAIN/tri-objective-robust-xai-medimg.git\n",
    "%cd tri-objective-robust-xai-medimg\n",
    "\n",
    "# Pull latest changes (with the fix)\n",
    "!git pull origin main\n",
    "\n",
    "print(\"✅ Repository cloned with fix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    mem_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"Memory: {mem_gb:.1f} GB\")\n",
    "    if mem_gb < 16:\n",
    "        print(\"⚠️ WARNING: GPU memory is low. Consider using A100.\")\n",
    "else:\n",
    "    raise RuntimeError(\"❌ No GPU! Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt --quiet\n",
    "!pip install pillow==10.1.0 --quiet\n",
    "print(\"✅ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71732b6",
   "metadata": {},
   "source": [
    "## Step 2: Verify Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feabb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the PGD normalization fix\n",
    "from src.losses.tri_objective import TriObjectiveConfig, TRADESLoss\n",
    "\n",
    "config = TriObjectiveConfig()\n",
    "print(f\"✅ TriObjectiveConfig loaded\")\n",
    "print(f\"   clip_min: {config.clip_min}\")\n",
    "print(f\"   clip_max: {config.clip_max}\")\n",
    "\n",
    "if config.clip_min == 0.0 and config.clip_max == 1.0:\n",
    "    raise ValueError(\"❌ FIX NOT APPLIED! clip_min/max should be [-2.12, 2.64]\")\n",
    "else:\n",
    "    print(\"✅ PGD normalization fix verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5868d67c",
   "metadata": {},
   "source": [
    "## Step 3: Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac363c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ISIC data exists on Google Drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Expected data locations\n",
    "DATA_PATHS = [\n",
    "    \"/content/drive/MyDrive/data/processed/isic2018\",\n",
    "    \"/content/drive/MyDrive/ISIC2018\",\n",
    "    \"/content/data/isic2018\",\n",
    "]\n",
    "\n",
    "DATA_ROOT = None\n",
    "for path in DATA_PATHS:\n",
    "    if os.path.exists(path):\n",
    "        DATA_ROOT = path\n",
    "        break\n",
    "\n",
    "if DATA_ROOT:\n",
    "    print(f\"✅ Found ISIC data at: {DATA_ROOT}\")\n",
    "    # List contents\n",
    "    contents = os.listdir(DATA_ROOT)\n",
    "    print(f\"   Contents: {contents[:5]}...\" if len(contents) > 5 else f\"   Contents: {contents}\")\n",
    "else:\n",
    "    print(\"❌ ISIC data not found. Please upload to Google Drive.\")\n",
    "    print(\"Expected locations:\")\n",
    "    for p in DATA_PATHS:\n",
    "        print(f\"  - {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b589e26",
   "metadata": {},
   "source": [
    "## Step 4: Train Tri-Objective Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    \"data_root\": DATA_ROOT,\n",
    "    \"checkpoint_dir\": \"/content/drive/MyDrive/checkpoints/tri_objective_fixed\",\n",
    "    \"max_epochs\": 60,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"lambda_rob\": 0.3,\n",
    "    \"lambda_expl\": 0.0,  # Disable explanation loss (no CAVs)\n",
    "    \"trades_beta\": 6.0,\n",
    "    \"pgd_epsilon\": 8/255,\n",
    "    \"pgd_steps\": 7,\n",
    "    \"seeds\": [42, 123, 456],\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for k, v in TRAINING_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Project imports\n",
    "from src.datasets.isic import ISICDataset\n",
    "from src.datasets.transforms import get_isic_transforms\n",
    "from src.models.build import build_model\n",
    "from src.training.tri_objective_trainer import TriObjectiveTrainer, TriObjectiveConfig\n",
    "from src.utils.reproducibility import set_global_seed\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def train_tri_objective_seed(seed: int, config: dict):\n",
    "    \"\"\"Train tri-objective model for a single seed.\"\"\"\n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"Training Tri-Objective Model - Seed {seed}\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    \n",
    "    # Set seed\n",
    "    set_global_seed(seed)\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    logger.info(\"Loading datasets...\")\n",
    "    train_transforms = get_isic_transforms('train', image_size=224)\n",
    "    val_transforms = get_isic_transforms('val', image_size=224)\n",
    "    \n",
    "    train_dataset = ISICDataset(\n",
    "        root=config['data_root'],\n",
    "        split='train',\n",
    "        transforms=train_transforms\n",
    "    )\n",
    "    val_dataset = ISICDataset(\n",
    "        root=config['data_root'],\n",
    "        split='val',\n",
    "        transforms=val_transforms\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Train samples: {len(train_dataset)}\")\n",
    "    logger.info(f\"Val samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Build model\n",
    "    logger.info(\"Building ResNet-50 model...\")\n",
    "    model = build_model(\n",
    "        name='resnet50',\n",
    "        num_classes=7,\n",
    "        pretrained=True\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=config['max_epochs'],\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Trainer config\n",
    "    trainer_config = TriObjectiveConfig(\n",
    "        max_epochs=config['max_epochs'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay'],\n",
    "        early_stopping_patience=15,\n",
    "        lambda_rob=config['lambda_rob'],\n",
    "        lambda_expl=config['lambda_expl'],\n",
    "        trades_beta=config['trades_beta'],\n",
    "        pgd_epsilon=config['pgd_epsilon'],\n",
    "        pgd_num_steps=config['pgd_steps'],\n",
    "        generate_heatmaps=False,\n",
    "        batch_size=config['batch_size'],\n",
    "        device=str(device),\n",
    "    )\n",
    "    \n",
    "    # Checkpoint directory for this seed\n",
    "    checkpoint_dir = Path(config['checkpoint_dir']) / f\"seed_{seed}\"\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = TriObjectiveTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        config=trainer_config,\n",
    "        val_loader=val_loader,\n",
    "        scheduler=scheduler,\n",
    "        device=str(device),\n",
    "    )\n",
    "    trainer.checkpoint_dir = checkpoint_dir\n",
    "    \n",
    "    # Train\n",
    "    logger.info(f\"Starting training for {config['max_epochs']} epochs...\")\n",
    "    start_time = datetime.now()\n",
    "    history = trainer.fit()\n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    logger.info(f\"Training completed in: {end_time - start_time}\")\n",
    "    logger.info(f\"Best validation loss: {trainer.best_metric:.4f}\")\n",
    "    logger.info(f\"Checkpoint saved to: {checkpoint_dir}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"✅ Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fdef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all seeds\n",
    "all_histories = {}\n",
    "\n",
    "for seed in TRAINING_CONFIG['seeds']:\n",
    "    try:\n",
    "        history = train_tri_objective_seed(seed, TRAINING_CONFIG)\n",
    "        all_histories[seed] = history\n",
    "        print(f\"\\n✅ Seed {seed} completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Seed {seed} failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Successful seeds: {list(all_histories.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c10581a",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eae3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation of trained models\n",
    "from src.attacks.pgd import pgd_attack\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_model(checkpoint_path, test_loader, device):\n",
    "    \"\"\"Evaluate model on clean and adversarial examples.\"\"\"\n",
    "    # Load model\n",
    "    model = build_model('resnet50', num_classes=7, pretrained=False)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct_clean = 0\n",
    "    correct_robust = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Clean accuracy\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct_clean += (preds == labels).sum().item()\n",
    "        \n",
    "        # Robust accuracy (PGD-20)\n",
    "        images_adv = pgd_attack(\n",
    "            model, images, labels,\n",
    "            epsilon=8/255,\n",
    "            num_steps=20,\n",
    "            step_size=2/255,\n",
    "            clip_min=-2.12,  # ImageNet normalized\n",
    "            clip_max=2.64\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            outputs_adv = model(images_adv)\n",
    "            _, preds_adv = outputs_adv.max(1)\n",
    "            correct_robust += (preds_adv == labels).sum().item()\n",
    "        \n",
    "        total += labels.size(0)\n",
    "    \n",
    "    clean_acc = correct_clean / total * 100\n",
    "    robust_acc = correct_robust / total * 100\n",
    "    \n",
    "    return clean_acc, robust_acc\n",
    "\n",
    "print(\"✅ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45445d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all trained models\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device('cuda')\n",
    "checkpoint_dir = Path(TRAINING_CONFIG['checkpoint_dir'])\n",
    "\n",
    "# Create test loader\n",
    "test_transforms = get_isic_transforms('test', image_size=224)\n",
    "test_dataset = ISICDataset(\n",
    "    root=TRAINING_CONFIG['data_root'],\n",
    "    split='test',\n",
    "    transforms=test_transforms\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for seed in TRAINING_CONFIG['seeds']:\n",
    "    ckpt_path = checkpoint_dir / f\"seed_{seed}\" / \"best.pt\"\n",
    "    if ckpt_path.exists():\n",
    "        clean_acc, robust_acc = evaluate_model(ckpt_path, test_loader, device)\n",
    "        results.append((seed, clean_acc, robust_acc))\n",
    "        print(f\"Seed {seed}: Clean={clean_acc:.1f}%, Robust={robust_acc:.1f}%\")\n",
    "    else:\n",
    "        print(f\"Seed {seed}: Checkpoint not found at {ckpt_path}\")\n",
    "\n",
    "if results:\n",
    "    avg_clean = sum(r[1] for r in results) / len(results)\n",
    "    avg_robust = sum(r[2] for r in results) / len(results)\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\"Average: Clean={avg_clean:.1f}%, Robust={avg_robust:.1f}%\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Check if meeting targets\n",
    "    if avg_clean >= 75 and avg_robust >= 35:\n",
    "        print(\"✅ Model meets RQ1 targets!\")\n",
    "    else:\n",
    "        print(\"⚠️ Model below targets. May need hyperparameter tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf41e08",
   "metadata": {},
   "source": [
    "## Step 6: Copy Checkpoints to Standard Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy checkpoints to standard project location for evaluation pipeline\n",
    "import shutil\n",
    "\n",
    "src_dir = Path(TRAINING_CONFIG['checkpoint_dir'])\n",
    "dst_dir = Path('/content/tri-objective-robust-xai-medimg/checkpoints/tri_objective')\n",
    "\n",
    "if src_dir.exists():\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for seed in TRAINING_CONFIG['seeds']:\n",
    "        src = src_dir / f\"seed_{seed}\" / \"best.pt\"\n",
    "        dst = dst_dir / f\"seed_{seed}\" / \"best.pt\"\n",
    "        if src.exists():\n",
    "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy(src, dst)\n",
    "            print(f\"Copied seed {seed} checkpoint\")\n",
    "    print(f\"\\n✅ Checkpoints copied to {dst_dir}\")\n",
    "else:\n",
    "    print(\"❌ Source checkpoint directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d8783",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "The tri-objective models have been trained with the PGD normalization fix.\n",
    "\n",
    "### Next Steps:\n",
    "1. Download checkpoints from Google Drive\n",
    "2. Copy to local project: `checkpoints/tri_objective/`\n",
    "3. Run full RQ1 evaluation locally"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
