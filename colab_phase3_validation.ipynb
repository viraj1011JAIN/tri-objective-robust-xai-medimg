{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7710846",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538efbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/viraj1011JAIN/tri-objective-robust-xai-medimg.git\n",
    "%cd tri-objective-robust-xai-medimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa82bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: No GPU detected. Training will be slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (this may take 2-3 minutes)\n",
    "!pip install -r requirements.txt --quiet\n",
    "!pip install pytest pytest-cov --quiet\n",
    "\n",
    "print(\"âœ“ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Pillow compatibility issue (scikit-image requires >=10.1)\n",
    "!pip uninstall -y pillow 2>&1 | grep -E \"(Successfully|Found)\" || true\n",
    "!pip install pillow==10.1.0 --no-warn-conflicts\n",
    "\n",
    "print(\"âœ“ Pillow compatibility fixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de571fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix pyarrow compatibility issue (required for mlflow and other packages)\n",
    "!pip uninstall -y pyarrow 2>&1 | grep -E \"(Successfully|Found)\" || true\n",
    "!pip install pyarrow==15.0.2 --quiet\n",
    "\n",
    "print(\"âœ“ PyArrow fixed\")\n",
    "print(\"ðŸ”„ Restarting runtime to load new pyarrow...\")\n",
    "\n",
    "# Auto-restart runtime\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c66769",
   "metadata": {},
   "source": [
    "**âš ï¸ IMPORTANT:** If you get a pyarrow error in the next cell, **restart the runtime** (`Runtime â†’ Restart runtime`) and then **continue from cell 7 (skip cells 1-6)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c137c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify core imports\n",
    "import sys\n",
    "sys.path.insert(0, '/content/tri-objective-robust-xai-medimg')\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import timm\n",
    "    import mlflow\n",
    "    import yaml\n",
    "    print(\"âœ“ All core packages imported successfully\")\n",
    "    print(f\"  - torch: {torch.__version__}\")\n",
    "    print(f\"  - torchvision: {torchvision.__version__}\")\n",
    "    print(f\"  - timm: {timm.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"âœ— Import error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808a6f3",
   "metadata": {},
   "source": [
    "## Step 2: Verify Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check critical files exist\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Ensure we're in the correct directory\n",
    "if not os.path.exists('src'):\n",
    "    os.chdir('/content/tri-objective-robust-xai-medimg')\n",
    "    print(\"ðŸ“ Changed to project directory\")\n",
    "\n",
    "def check_file(path, description):\n",
    "    exists = Path(path).exists()\n",
    "    symbol = \"âœ“\" if exists else \"âœ—\"\n",
    "    print(f\"{symbol} {description}: {path}\")\n",
    "    return exists\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 3 FILE STRUCTURE CHECK\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print()\n",
    "\n",
    "checks = [\n",
    "    (\"src/models/__init__.py\", \"Models package\"),\n",
    "    (\"src/models/build.py\", \"Model factory\"),\n",
    "    (\"src/models/base_model.py\", \"Base model class\"),\n",
    "    (\"src/models/resnet.py\", \"ResNet50\"),\n",
    "    (\"src/models/efficientnet.py\", \"EfficientNet\"),\n",
    "    (\"src/losses/task_loss.py\", \"Task losses\"),\n",
    "    (\"src/datasets/__init__.py\", \"Datasets package\"),\n",
    "    (\"src/training/baseline_trainer.py\", \"Baseline trainer\"),\n",
    "    (\"tests/unit/test_models.py\", \"Model tests\"),\n",
    "]\n",
    "\n",
    "all_exist = all(check_file(path, desc) for path, desc in checks)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "if all_exist:\n",
    "    print(\"âœ“ All critical files present\")\n",
    "else:\n",
    "    print(\"âœ— Some files missing - check repository\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9282bf5",
   "metadata": {},
   "source": [
    "## Step 3: Test Model Registry & Build System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf510c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model building functions\n",
    "from src.models import build_model\n",
    "from src.models.build import MODEL_REGISTRY\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL REGISTRY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List available models\n",
    "print(\"\\nAvailable models in registry:\")\n",
    "for model_name in MODEL_REGISTRY.keys():\n",
    "    print(f\"  - {model_name}\")\n",
    "\n",
    "# Test building each model\n",
    "print(\"\\nTesting model instantiation:\")\n",
    "test_models = ['resnet50', 'efficientnet_b0']\n",
    "results = {}\n",
    "\n",
    "for model_name in test_models:\n",
    "    try:\n",
    "        # Build model\n",
    "        model = build_model(model_name, num_classes=7)\n",
    "        print(f\"\\nâœ“ {model_name}: Successfully built\")\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Test forward pass\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(2, 3, 224, 224)\n",
    "            output = model(dummy_input)\n",
    "            print(f\"  Input shape: {tuple(dummy_input.shape)}\")\n",
    "            print(f\"  Output shape: {tuple(output.shape)}\")\n",
    "            \n",
    "            # Verify output shape\n",
    "            assert output.shape == (2, 7), f\"Expected (2, 7), got {output.shape}\"\n",
    "            assert torch.isfinite(output).all(), \"Output contains NaN or Inf\"\n",
    "            print(f\"  âœ“ Forward pass successful\")\n",
    "        \n",
    "        results[model_name] = \"PASS\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâœ— {model_name}: FAILED\")\n",
    "        print(f\"  Error: {str(e)}\")\n",
    "        results[model_name] = \"FAIL\"\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL BUILD SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for model_name, status in results.items():\n",
    "    symbol = \"âœ“\" if status == \"PASS\" else \"âœ—\"\n",
    "    print(f\"{symbol} {model_name}: {status}\")\n",
    "\n",
    "all_passed = all(status == \"PASS\" for status in results.values())\n",
    "if all_passed:\n",
    "    print(\"\\nâœ“âœ“âœ“ All models working correctly âœ“âœ“âœ“\")\n",
    "else:\n",
    "    print(\"\\nâœ—âœ—âœ— Some models failed âœ—âœ—âœ—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c8898",
   "metadata": {},
   "source": [
    "## Step 4: Test Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss function implementations\n",
    "from src.losses.task_loss import FocalLoss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOSS FUNCTION TESTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test data (predictions need requires_grad=True)\n",
    "batch_size = 8\n",
    "num_classes = 7\n",
    "predictions = torch.randn(batch_size, num_classes, requires_grad=True)\n",
    "targets = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "losses_to_test = [\n",
    "    (\"CrossEntropyLoss\", nn.CrossEntropyLoss()),\n",
    "    (\"FocalLoss\", FocalLoss(num_classes=num_classes, alpha=1.0, gamma=2.0)),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for loss_name, loss_fn in losses_to_test:\n",
    "    try:\n",
    "        # Compute loss\n",
    "        loss = loss_fn(predictions, targets)\n",
    "        \n",
    "        print(f\"\\nâœ“ {loss_name}\")\n",
    "        print(f\"  Loss value: {loss.item():.4f}\")\n",
    "        \n",
    "        # Verify properties\n",
    "        assert loss.item() > 0, \"Loss should be positive\"\n",
    "        assert torch.isfinite(loss), \"Loss should be finite\"\n",
    "        assert loss.requires_grad, \"Loss should require gradients\"\n",
    "        \n",
    "        # Test backward pass\n",
    "        loss.backward()\n",
    "        print(f\"  âœ“ Backward pass successful\")\n",
    "        \n",
    "        results[loss_name] = \"PASS\"\n",
    "        \n",
    "        # Reset gradients for next test\n",
    "        predictions.grad = None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâœ— {loss_name}: FAILED\")\n",
    "        print(f\"  Error: {str(e)}\")\n",
    "        results[loss_name] = \"FAIL\"\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOSS FUNCTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for loss_name, status in results.items():\n",
    "    symbol = \"âœ“\" if status == \"PASS\" else \"âœ—\"\n",
    "    print(f\"{symbol} {loss_name}: {status}\")\n",
    "\n",
    "if all(status == \"PASS\" for status in results.values()):\n",
    "    print(\"\\nâœ“âœ“âœ“ All loss functions working âœ“âœ“âœ“\")\n",
    "else:\n",
    "    print(\"\\nâœ—âœ—âœ— Some loss functions failed âœ—âœ—âœ—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc4364",
   "metadata": {},
   "source": [
    "## Step 5: Test Dataset Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0df8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset loading infrastructure\n",
    "from src.datasets.transforms import get_train_transforms, get_test_transforms\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET INFRASTRUCTURE TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Test transform functions\n",
    "    print(\"\\nTesting transform functions...\")\n",
    "    train_transforms = get_train_transforms(image_size=224)\n",
    "    test_transforms = get_test_transforms(image_size=224)\n",
    "    \n",
    "    print(\"âœ“ Train transforms created\")\n",
    "    print(f\"  Type: {type(train_transforms)}\")\n",
    "    print(\"âœ“ Test transforms created\")\n",
    "    print(f\"  Type: {type(test_transforms)}\")\n",
    "    \n",
    "    # Test transform on dummy image\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create dummy image as numpy array (required for Albumentations)\n",
    "    dummy_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Apply transform using Albumentations syntax (named argument)\n",
    "    transformed = train_transforms(image=dummy_image)\n",
    "    transformed_tensor = transformed['image']\n",
    "    \n",
    "    print(f\"\\nâœ“ Transform applied to dummy image\")\n",
    "    print(f\"  Output shape: {tuple(transformed_tensor.shape)}\")\n",
    "    print(f\"  Output dtype: {transformed_tensor.dtype}\")\n",
    "    print(f\"  Value range: [{transformed_tensor.min():.3f}, {transformed_tensor.max():.3f}]\")\n",
    "    \n",
    "    assert transformed_tensor.shape == (3, 224, 224), f\"Expected (3, 224, 224), got {transformed_tensor.shape}\"\n",
    "    \n",
    "    print(\"\\nâœ“âœ“âœ“ Dataset infrastructure working âœ“âœ“âœ“\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Dataset infrastructure test FAILED\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6faf9a7",
   "metadata": {},
   "source": [
    "## Step 6: Run Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662ad441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pytest suite for Phase 3 components\n",
    "!pytest tests/unit/test_models.py -v --tb=short --color=yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a66831d",
   "metadata": {},
   "source": [
    "## Step 7: Training Infrastructure Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02957859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training infrastructure imports\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING INFRASTRUCTURE CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "components = []\n",
    "\n",
    "try:\n",
    "    from src.training.baseline_trainer import BaselineTrainer\n",
    "    print(\"âœ“ BaselineTrainer imported\")\n",
    "    components.append((\"BaselineTrainer\", \"PASS\"))\n",
    "except Exception as e:\n",
    "    print(f\"âœ— BaselineTrainer import failed: {e}\")\n",
    "    components.append((\"BaselineTrainer\", \"FAIL\"))\n",
    "\n",
    "try:\n",
    "    from src.training.base_trainer import BaseTrainer\n",
    "    print(\"âœ“ BaseTrainer imported\")\n",
    "    components.append((\"BaseTrainer\", \"PASS\"))\n",
    "except Exception as e:\n",
    "    print(f\"âœ— BaseTrainer import failed: {e}\")\n",
    "    components.append((\"BaseTrainer\", \"FAIL\"))\n",
    "\n",
    "try:\n",
    "    import torch.optim as optim\n",
    "    print(\"âœ“ Optimizers available\")\n",
    "    components.append((\"Optimizers\", \"PASS\"))\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Optimizers import failed: {e}\")\n",
    "    components.append((\"Optimizers\", \"FAIL\"))\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING INFRASTRUCTURE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for component, status in components:\n",
    "    symbol = \"âœ“\" if status == \"PASS\" else \"âœ—\"\n",
    "    print(f\"{symbol} {component}: {status}\")\n",
    "\n",
    "if all(status == \"PASS\" for _, status in components):\n",
    "    print(\"\\nâœ“âœ“âœ“ Training infrastructure ready âœ“âœ“âœ“\")\n",
    "else:\n",
    "    print(\"\\nâœ—âœ—âœ— Some components missing âœ—âœ—âœ—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40d3da",
   "metadata": {},
   "source": [
    "## Step 8: Complete Phase 3 Status Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b0e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive Phase 3 status report\n",
    "from pathlib import Path\n",
    "\n",
    "def check_component(path, name):\n",
    "    exists = Path(path).exists()\n",
    "    return (name, \"âœ“\" if exists else \"âœ—\", exists)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 3: MODEL ARCHITECTURE & BASELINE - STATUS REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sections = {\n",
    "    \"3.1 Model Architecture\": [\n",
    "        (\"src/models/base_model.py\", \"Base model abstract class\"),\n",
    "        (\"src/models/resnet.py\", \"ResNet50Classifier\"),\n",
    "        (\"src/models/efficientnet.py\", \"EfficientNetB0Classifier\"),\n",
    "        (\"src/models/build.py\", \"Model factory\"),\n",
    "        (\"src/models/model_registry.py\", \"Model registry\"),\n",
    "    ],\n",
    "    \"3.2 Loss Functions\": [\n",
    "        (\"src/losses/task_loss.py\", \"Task loss implementations\"),\n",
    "        (\"src/losses/calibration_loss.py\", \"Calibration losses\"),\n",
    "    ],\n",
    "    \"3.3 Training Infrastructure\": [\n",
    "        (\"src/training/base_trainer.py\", \"Base trainer\"),\n",
    "        (\"src/training/baseline_trainer.py\", \"Baseline trainer\"),\n",
    "        (\"scripts/training/train_baseline.py\", \"Training script\"),\n",
    "    ],\n",
    "    \"3.4-3.6 Experiment Configs\": [\n",
    "        (\"configs/experiments/baseline.yaml\", \"Baseline config\"),\n",
    "        (\"configs/datasets/isic.yaml\", \"ISIC dataset config\"),\n",
    "    ],\n",
    "    \"3.7 Fairness & Evaluation\": [\n",
    "        (\"src/evaluation/fairness.py\", \"Fairness metrics\"),\n",
    "    ],\n",
    "    \"3.8 Testing\": [\n",
    "        (\"tests/unit/test_models.py\", \"Model unit tests\"),\n",
    "        (\"tests/unit/test_losses.py\", \"Loss function tests\"),\n",
    "    ],\n",
    "}\n",
    "\n",
    "section_results = {}\n",
    "\n",
    "for section_name, items in sections.items():\n",
    "    print(f\"\\n{section_name}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    section_passed = 0\n",
    "    section_total = len(items)\n",
    "    \n",
    "    for path, desc in items:\n",
    "        name, symbol, exists = check_component(path, desc)\n",
    "        print(f\"  {symbol} {desc}\")\n",
    "        if exists:\n",
    "            section_passed += 1\n",
    "    \n",
    "    section_results[section_name] = (section_passed, section_total)\n",
    "\n",
    "# Overall summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OVERALL PHASE 3 SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total_passed = sum(p for p, _ in section_results.values())\n",
    "total_items = sum(t for _, t in section_results.values())\n",
    "\n",
    "for section, (passed, total) in section_results.items():\n",
    "    percentage = (passed / total * 100) if total > 0 else 0\n",
    "    status = \"âœ“\" if passed == total else \"âš \" if passed > 0 else \"âœ—\"\n",
    "    print(f\"{status} {section}: {passed}/{total} ({percentage:.0f}%)\")\n",
    "\n",
    "overall_percentage = (total_passed / total_items * 100) if total_items > 0 else 0\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"PHASE 3 COMPLETION: {total_passed}/{total_items} ({overall_percentage:.1f}%)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if overall_percentage >= 90:\n",
    "    print(\"\\nâœ“âœ“âœ“ PHASE 3 READY FOR TRAINING âœ“âœ“âœ“\")\n",
    "elif overall_percentage >= 70:\n",
    "    print(\"\\nâš âš âš  PHASE 3 MOSTLY COMPLETE - SOME COMPONENTS MISSING âš âš âš \")\n",
    "else:\n",
    "    print(\"\\nâœ—âœ—âœ— PHASE 3 INCOMPLETE - MAJOR COMPONENTS MISSING âœ—âœ—âœ—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68978e54",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If all tests pass:\n",
    "\n",
    "1. **Mount Google Drive** (for persistent storage):\n",
    "   ```python\n",
    "   from google.colab import drive\n",
    "   drive.mount('/content/drive')\n",
    "   ```\n",
    "\n",
    "2. **Download datasets** to Colab or Drive\n",
    "\n",
    "3. **Run baseline training**:\n",
    "   ```bash\n",
    "   !python scripts/training/train_baseline.py --config configs/experiments/baseline.yaml\n",
    "   ```\n",
    "\n",
    "4. **Monitor with MLflow** or TensorBoard\n",
    "\n",
    "If tests fail, check:\n",
    "- Repository clone was successful\n",
    "- All dependencies installed correctly\n",
    "- GPU runtime is enabled\n",
    "- Python path includes project root"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
